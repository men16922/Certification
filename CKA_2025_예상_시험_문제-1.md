# 2025 CKA ì‹œí—˜ ì˜ˆìƒ ë¬¸ì œ

## ëª©ì°¨
1. [HPA & ë¦¬ì†ŒìŠ¤ ê´€ë¦¬](#1-hpa--ë¦¬ì†ŒìŠ¤-ê´€ë¦¬)
2. [Pod & Deployment ìƒì„±](#2-pod--deployment-ìƒì„±)
3. [Multi-Container Pod & Sidecar](#3-multi-container-pod--sidecar)
4. [Network Policy & ë³´ì•ˆ](#4-network-policy--ë³´ì•ˆ)
5. [ETCD ë°±ì—… & ë³µì›](#5-etcd-ë°±ì—…--ë³µì›)
6. [í´ëŸ¬ìŠ¤í„° ì—…ê·¸ë ˆì´ë“œ](#6-í´ëŸ¬ìŠ¤í„°-ì—…ê·¸ë ˆì´ë“œ)
7. [RBAC & ì¸ì¦](#7-rbac--ì¸ì¦)
8. [Storage & PV/PVC](#8-storage--pvpvc)
9. [Service & Ingress](#9-service--ingress)
10. [Troubleshooting](#10-troubleshooting)
11. [ConfigMap & Secret](#11-configmap--secret)
12. [JSONPATH & ê³ ê¸‰ ì¿¼ë¦¬](#12-jsonpath--ê³ ê¸‰-ì¿¼ë¦¬)
13. [Gateway API & HTTPRoute](#13-gateway-api--httproute)
14. [CNI & ë„¤íŠ¸ì›Œí¬ ì„¤ì •](#14-cni--ë„¤íŠ¸ì›Œí¬-ì„¤ì •)
15. [ArgoCD & GitOps](#15-argocd--gitops)

---

## 1. HPA & ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

### ë¬¸ì œ 1-1: HPA ìƒì„± (ê°€ì¤‘ì¹˜: 8%)
**Namespace**: `production`

Create a Horizontal Pod Autoscaler named `web-app-hpa` for the existing deployment `web-app` in the `production` namespace.

Requirements:
- Target CPU utilization: 60%
- Minimum replicas: 2
- Maximum replicas: 8
- Downscale stabilization window: 60 seconds

```bash
kubectl create ns production
kubectl create deploy web-app --image=nginx --replicas=3 -n production

# HPA ìƒì„±
kubectl autoscale deployment web-app -n production --min=2 --max=8 --cpu-percent=60
kubectl edit hpa web-app -n production
```

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 8
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
```

### ë¬¸ì œ 1-2: WordPress ë¦¬ì†ŒìŠ¤ ìµœì í™” (ê°€ì¤‘ì¹˜: 9%)
**Namespace**: `wordpress`

A WordPress application has resource issues. Scale down the deployment to 0 replicas, update resource requests, then scale back to 3 replicas.

Requirements:
- Each pod should have CPU request: 200m, Memory request: 256Mi
- Apply same requests to both containers and init containers
- Ensure all 3 pods are running and ready

```bash
kubectl scale deploy wordpress --replicas=0 -n wordpress
kubectl patch deployment wordpress -n wordpress -p '{"spec": {"template": {"spec": {"containers": [{"name": "wordpress", "resources": {"requests": {"cpu": "200m", "memory": "256Mi"}}}]}}}}'
kubectl scale deploy wordpress --replicas=3 -n wordpress
kubectl get pods -n wordpress
```

---

## 2. Pod & Deployment ìƒì„±

### ë¬¸ì œ 2-1: ë³´ì•ˆ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” Pod (ê°€ì¤‘ì¹˜: 7%)
**Namespace**: `security`

Create a pod named `secure-pod` with the following specifications:
- Image: `busybox:1.35`
- Allow the pod to set system time (SYS_TIME capability)
- Command: sleep for 4800 seconds
- Labels: `tier=security`, `env=prod`

```bash
kubectl create ns security
kubectl run secure-pod -n security --image=busybox:1.35 --dry-run=client -o yaml > secure-pod.yaml
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
  namespace: security
  labels:
    tier: security
    env: prod
spec:
  containers:
  - name: secure-pod
    image: busybox:1.35
    command: ["sleep", "4800"]
    securityContext:
      capabilities:
        add: ["SYS_TIME"]
```

### ë¬¸ì œ 2-2: Deployment & NodePort ì„œë¹„ìŠ¤ (ê°€ì¤‘ì¹˜: 10%)
**Namespace**: `web-tier`

Reconfigure the existing deployment `frontend-app` to expose port 8080. Create a NodePort service named `frontend-svc` to expose the deployment.

Requirements:
- Container port: 8080
- Service port: 8080
- NodePort: 30080

```bash
kubectl create ns web-tier
kubectl create deploy frontend-app --image=nginx -n web-tier
kubectl edit deploy frontend-app -n web-tier  # Add containerPort: 8080

kubectl expose deploy frontend-app --name=frontend-svc --port=8080 --target-port=8080 -n web-tier
kubectl patch svc frontend-svc -n web-tier -p '{"spec": {"type": "NodePort", "ports": [{"port": 8080, "targetPort": 8080, "nodePort": 30080}]}}'
```

---

## 3. Multi-Container Pod & Sidecar

### ë¬¸ì œ 3-1: Sidecar ì»¨í…Œì´ë„ˆ ì¶”ê°€ (ê°€ì¤‘ì¹˜: 8%)
**Namespace**: `logging`

Update the existing deployment `app-server` by adding a sidecar container for log collection.

Requirements:
- Sidecar container name: `log-collector`
- Sidecar image: `busybox:stable`
- Sidecar command: `/bin/sh -c "tail -n+1 -f /var/log/app.log"`
- Shared volume at `/var/log` for both containers

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-server
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app-server
  template:
    metadata:
      labels:
        app: app-server
    spec:
      containers:
      - name: app
        image: nginx
        command: ['sh', '-c', 'while true; do echo "$(date): Application log" >> /var/log/app.log; sleep 5; done']
        volumeMounts:
        - name: log-volume
          mountPath: /var/log
      - name: log-collector
        image: busybox:stable
        command: ['/bin/sh', '-c', 'tail -n+1 -f /var/log/app.log']
        volumeMounts:
        - name: log-volume
          mountPath: /var/log
      volumes:
      - name: log-volume
        emptyDir: {}
```

### ë¬¸ì œ 3-2: Multi-Container Pod (ê°€ì¤‘ì¹˜: 6%)
**Namespace**: `microservices`

Create a pod named `service-mesh` with three containers:
1. `web-server`: nginx:1.21
2. `cache`: redis:6-alpine  
3. `database`: postgres:13-alpine (with POSTGRES_PASSWORD=secret)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: service-mesh
  namespace: microservices
spec:
  containers:
  - name: web-server
    image: nginx:1.21
  - name: cache
    image: redis:6-alpine
  - name: database
    image: postgres:13-alpine
    env:
    - name: POSTGRES_PASSWORD
      value: "secret"
```

---

## 4. Network Policy & ë³´ì•ˆ

### ë¬¸ì œ 4-1: ë³µí•© Network Policy (ê°€ì¤‘ì¹˜: 11%)
**Namespace**: `secure-app`

Create a NetworkPolicy named `app-security` with both ingress and egress rules:

Ingress:
- Allow traffic from IP range 172.16.0.0/16 except 172.16.1.0/24
- Allow from namespace with label `env=trusted`
- Allow from pods with label `role=frontend`
- Port: 8080

Egress:
- Allow traffic to IP range 10.0.0.0/24
- Port: 5432

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-security
  namespace: secure-app
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.16.0.0/16
        except:
        - 172.16.1.0/24
    - namespaceSelector:
        matchLabels:
          env: trusted
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5432
```

### ë¬¸ì œ 4-2: TLS ì„¤ì •ì´ ìˆëŠ” Network Policy (ê°€ì¤‘ì¹˜: 8%)
**Namespace**: `frontend`, `backend`

Create the least permissive NetworkPolicy to allow communication between frontend and backend deployments across namespaces.

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-to-backend
  namespace: backend
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: frontend
      podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080
```

---

## 5. ETCD ë°±ì—… & ë³µì›

### ë¬¸ì œ 5-1: ETCD ë°±ì—… (ê°€ì¤‘ì¹˜: 8%)
**Cluster**: `production-cluster`

Create a backup of the ETCD database and save it to `/opt/etcd-backup.db`.

Use the following certificates:
- CA: `/etc/kubernetes/pki/etcd/ca.crt`
- Cert: `/etc/kubernetes/pki/etcd/server.crt`
- Key: `/etc/kubernetes/pki/etcd/server.key`

```bash
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/etcd-backup.db

ETCDCTL_API=3 etcdctl snapshot status /opt/etcd-backup.db
```

### ë¬¸ì œ 5-2: ETCD ë³µì› (ê°€ì¤‘ì¹˜: 8%)
**Cluster**: `production-cluster`

Restore ETCD from the backup file `/opt/etcd-backup.db` to `/var/lib/etcd-restore`.

```bash
ETCDCTL_API=3 etcdctl --data-dir=/var/lib/etcd-restore snapshot restore /opt/etcd-backup.db

# Edit etcd.yaml to point to new data directory
sudo vi /etc/kubernetes/manifests/etcd.yaml
# Change: --data-dir=/var/lib/etcd-restore

# Verify etcd is running
kubectl get nodes
```

---

## 6. í´ëŸ¬ìŠ¤í„° ì—…ê·¸ë ˆì´ë“œ

### ë¬¸ì œ 6-1: Control Plane ì—…ê·¸ë ˆì´ë“œ (ê°€ì¤‘ì¹˜: 10%)
**Node**: `controlplane`

Upgrade the Kubernetes control plane from version 1.28.0 to 1.29.0.

Steps:
1. Drain the control plane node
2. Upgrade kubeadm, kubelet, kubectl
3. Apply the upgrade
4. Uncordon the node

```bash
# On control plane node
sudo apt update
sudo apt-cache madison kubeadm

sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.29.0-1.1' && \
sudo apt-mark hold kubeadm

sudo kubeadm upgrade plan
sudo kubeadm upgrade apply v1.29.0

kubectl drain controlplane --ignore-daemonsets

sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.29.0-1.1' kubectl='1.29.0-1.1' && \
sudo apt-mark hold kubelet kubectl

sudo systemctl daemon-reload
sudo systemctl restart kubelet

kubectl uncordon controlplane
```

---

## 7. RBAC & ì¸ì¦

### ë¬¸ì œ 7-1: CSR ë° ì‚¬ìš©ì ê¶Œí•œ (ê°€ì¤‘ì¹˜: 9%)
**User**: `developer`

Create a new user `developer` with the following permissions:
- Can create, list, get, update, delete pods in `development` namespace
- Use CSR for certificate approval

```bash
# Generate private key and CSR
openssl genrsa -out developer.key 2048
openssl req -new -key developer.key -out developer.csr -subj "/CN=developer"

# Create CSR resource
cat <<EOF | kubectl apply -f -
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: developer
spec:
  request: $(cat developer.csr | base64 | tr -d "\n")
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
EOF

# Approve CSR
kubectl certificate approve developer

# Create role and rolebinding
kubectl create role developer-role -n development --verb=create,list,get,update,delete --resource=pods
kubectl create rolebinding developer-binding -n development --role=developer-role --user=developer
```

### ë¬¸ì œ 7-2: PriorityClass (ê°€ì¤‘ì¹˜: 7%)
**Namespace**: `critical-apps`

Create a PriorityClass named `high-priority` with value 1000. Update the deployment `critical-app` to use this priority class.

```bash
kubectl create priorityclass high-priority --value=1000 --description="High priority class"
kubectl patch deployment critical-app -n critical-apps -p '{"spec":{"template":{"spec":{"priorityClassName":"high-priority"}}}}'
```

---

## 8. Storage & PV/PVC

### ë¬¸ì œ 8-1: StorageClass ë° PVC (ê°€ì¤‘ì¹˜: 8%)
**Namespace**: `data-storage`

Create a StorageClass named `fast-storage` and a PVC that uses it.

Requirements:
- StorageClass: `fast-storage` (set as default)
- PVC name: `app-data`
- Storage: 5Gi
- Access mode: ReadWriteOnce

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-data
  namespace: data-storage
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: fast-storage
```

### ë¬¸ì œ 8-2: ë°ì´í„° ë³µêµ¬ ë° ì¼ê´€ì„± (ê°€ì¤‘ì¹˜: 9%)
**Namespace**: `database`

A MariaDB deployment was accidentally deleted. Restore it using the existing PV.

Requirements:
- Create PVC named `mariadb-data` (250Mi)
- Use existing retained PV
- Deploy MariaDB with persistent storage

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mariadb-data
  namespace: database
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 250Mi
  storageClassName: standard
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mariadb
  namespace: database
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mariadb
  template:
    metadata:
      labels:
        app: mariadb
    spec:
      containers:
      - name: mariadb
        image: mariadb:latest
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "password"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: mariadb-data
```

---

## 9. Service & Ingress

### ë¬¸ì œ 9-1: Nginx Ingress (ê°€ì¤‘ì¹˜: 10%)
**Namespace**: `web-services`

Create an Nginx Ingress to expose the service `web-app-service` on `http://example.com/app`.

Requirements:
- Ingress name: `web-ingress`
- Host: `example.com`
- Path: `/app`
- Service port: 8080

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
  namespace: web-services
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: example.com
    http:
      paths:
      - path: /app
        pathType: Prefix
        backend:
          service:
            name: web-app-service
            port:
              number: 8080
```

### ë¬¸ì œ 9-2: DNS í™•ì¸ (ê°€ì¤‘ì¹˜: 6%)
**Namespace**: `default`

Create a pod named `dns-test` and verify DNS resolution for services.

Requirements:
- Test DNS resolution for service `kubernetes`
- Save results to `/tmp/dns-results.txt`

```bash
kubectl run dns-test --image=busybox:1.28 --restart=Never -it --rm -- nslookup kubernetes > /tmp/dns-results.txt
cat /tmp/dns-results.txt
```

---

## 10. Troubleshooting

### ë¬¸ì œ 10-1: Pod ë¬¸ì œ í•´ê²° (ê°€ì¤‘ì¹˜: 8%)
**Namespace**: `broken-apps`

A pod named `failing-app` is not starting. Identify and fix the issue.

Common issues to check:
- Image pull errors
- Resource constraints  
- Configuration errors
- Node taints/tolerations

```bash
kubectl describe pod failing-app -n broken-apps
kubectl get events -n broken-apps --sort-by='.lastTimestamp'
kubectl logs failing-app -n broken-apps

# Fix based on the identified issue
kubectl edit pod failing-app -n broken-apps
```

### ë¬¸ì œ 10-2: Node ë¬¸ì œ í•´ê²° (ê°€ì¤‘ì¹˜: 7%)
**Node**: `worker-node-01`

A worker node is in NotReady state. Investigate and fix the issue.

```bash
kubectl get nodes
kubectl describe node worker-node-01

# SSH to the node
ssh worker-node-01
sudo systemctl status kubelet
sudo systemctl status containerd
sudo journalctl -u kubelet -f

# Fix the issue (e.g., restart services)
sudo systemctl restart kubelet
sudo systemctl enable kubelet
```

---

## 11. ConfigMap & Secret

### ë¬¸ì œ 11-1: TLS ConfigMap (ê°€ì¤‘ì¹˜: 10%)
**Namespace**: `web-secure`

Configure nginx with TLS 1.3 only using ConfigMap and Secret.

Requirements:
- ConfigMap: `nginx-config` (nginx.conf with TLS 1.3 only)
- Secret: `nginx-tls` (TLS certificate and key)
- Deployment: `nginx-secure`

```bash
# Create TLS certificate
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/CN=example.com"
kubectl create secret tls nginx-tls --cert=tls.crt --key=tls.key -n web-secure

# Create ConfigMap
kubectl create configmap nginx-config -n web-secure --from-literal=nginx.conf="
events {}
http {
    server {
        listen 443 ssl;
        server_name example.com;
        ssl_certificate /etc/nginx/certs/tls.crt;
        ssl_certificate_key /etc/nginx/certs/tls.key;
        ssl_protocols TLSv1.3;
        location / {
            root /usr/share/nginx/html;
            index index.html;
        }
    }
}"
```

---

## 12. JSONPATH & ê³ ê¸‰ ì¿¼ë¦¬

### ë¬¸ì œ 12-1: Node ì •ë³´ ì¶”ì¶œ (ê°€ì¤‘ì¹˜: 7%)
**Output**: `/tmp/node-os-info.txt`

Extract OS images of all nodes using JSONPath and save to file.

```bash
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.nodeInfo.osImage}{"\n"}{end}' > /tmp/node-os-info.txt
```

### ë¬¸ì œ 12-2: Custom CRD ë‚˜ì—´ (ê°€ì¤‘ì¹˜: 7%)
**Output**: `/tmp/custom-crds.txt`

List all custom CRDs from cert-manager and save to file.

```bash
kubectl get crd | grep cert-manager > /tmp/custom-crds.txt
```

---

## 13. Gateway API & HTTPRoute

### ë¬¸ì œ 13-1: Gateway ë§ˆì´ê·¸ë ˆì´ì…˜ (ê°€ì¤‘ì¹˜: 12%)
**Namespace**: `gateway-demo`

Migrate from Ingress to Gateway API while maintaining HTTPS access.

Requirements:
- Gateway name: `web-gateway`
- Hostname: `api.example.com`
- HTTPRoute name: `web-route`
- Maintain existing TLS configuration

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: web-gateway
  namespace: gateway-demo
spec:
  gatewayClassName: nginx
  listeners:
  - name: https
    protocol: HTTPS
    port: 443
    hostname: api.example.com
    tls:
      mode: Terminate
      certificateRefs:
      - kind: Secret
        name: web-tls
---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: web-route
  namespace: gateway-demo
spec:
  parentRefs:
  - name: web-gateway
  hostnames:
  - "api.example.com"
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /
    backendRefs:
    - name: web-service
      port: 80
```

---

## 14. CNI & ë„¤íŠ¸ì›Œí¬ ì„¤ì •

### ë¬¸ì œ 14-1: CNI ì„¤ì¹˜ (ê°€ì¤‘ì¹˜: 10%)
**Requirement**: Network Policy support

Install a CNI that supports Network Policies. Choose between:
- Calico v3.28.2
- Flannel v0.26.1 (does not support Network Policy)

```bash
# Install Calico (supports Network Policy)
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.2/manifests/tigera-operator.yaml

# Verify installation
kubectl get pods -n calico-system
```

### ë¬¸ì œ 14-2: ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„° ì„¤ì • (ê°€ì¤‘ì¹˜: 10%)
**Node**: All nodes

Configure system parameters for Kubernetes:
- `net.bridge.bridge-nf-call-iptables = 1`
- `net.ipv6.conf.all.forwarding = 1`  
- `net.ipv4.ip_forward = 1`

```bash
# Create sysctl configuration
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.ipv6.conf.all.forwarding = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

# Apply settings
sudo sysctl --system

# Verify
sysctl net.ipv4.ip_forward
sysctl net.ipv6.conf.all.forwarding
sysctl net.bridge.bridge-nf-call-iptables
```

---

## 15. ArgoCD & GitOps

### ë¬¸ì œ 15-1: ArgoCD ì„¤ì¹˜ (ê°€ì¤‘ì¹˜: 7%)
**Namespace**: `argocd`

Install ArgoCD using Helm with specific configuration.

Requirements:
- Helm repository: `argo`
- Chart version: `7.7.3`
- Disable CRD installation
- Generate template to `~/argo-helm.yaml`

```bash
# Add Helm repository
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

# Generate template
helm template argocd argo/argo-cd --version 7.7.3 -n argocd --set crds.install=false > ~/argo-helm.yaml

# Apply if needed
kubectl apply -f ~/argo-helm.yaml
```

---

## ì‹œí—˜ íŒ & ìš”ë ¹

### ì‹œê°„ ê´€ë¦¬
- ì´ ì‹œí—˜ ì‹œê°„: 2ì‹œê°„ (120ë¶„)
- ë¬¸ì œë‹¹ í‰ê·  ì‹œê°„: 5-8ë¶„
- ì–´ë ¤ìš´ ë¬¸ì œëŠ” ë‚˜ì¤‘ì— ëŒì•„ì™€ì„œ í•´ê²°

### í•„ìˆ˜ ëª…ë ¹ì–´ ë‹¨ì¶•í‚¤
```bash
# kubectl ë‹¨ì¶•í‚¤ ì„¤ì •
alias k=kubectl
export do="--dry-run=client -o yaml"
export now="--force --grace-period 0"

# ìì£¼ ì‚¬ìš©í•˜ëŠ” ëª…ë ¹ì–´
k get po -A
k describe po <pod-name>
k logs <pod-name> -f
k edit <resource> <name>
k delete po <pod-name> $now
```

### Vim ì„¤ì •
```bash
# ~/.vimrc
set expandtab
set tabstop=2
set shiftwidth=2
set number
```

### ë¬¸ì œ í•´ê²° ìˆœì„œ
1. ë¬¸ì œ ìš”êµ¬ì‚¬í•­ ì •í™•íˆ íŒŒì•…
2. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸ ë° ìƒì„±
3. ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ ìƒì„± (--dry-run í™œìš©)
4. YAML íŒŒì¼ ìˆ˜ì •
5. ì ìš© ë° ê²€ì¦
6. ë¡œê·¸ ë° ìƒíƒœ í™•ì¸

### ì£¼ìš” ì°¸ê³  ë¬¸ì„œ
- https://kubernetes.io/docs/
- https://kubernetes.io/docs/reference/kubectl/cheatsheet/
- https://kubernetes.io/docs/concepts/

---

**Good Luck with your CKA Exam! ğŸš€**