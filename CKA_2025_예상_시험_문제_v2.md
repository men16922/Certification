# CKA 2025 예상 시험 문제 - Version 2

## 📊 시험 결과 분석 기반 집중 대비
**약점 영역 집중 공략:**
- 🔧 **Troubleshooting** (25%)
- 🌐 **Services and Networking** (20%) 
- ⚙️ **Cluster Architecture, Installation and Configuration** (25%)
- 📦 **Workloads & Scheduling** (15%)
- 💾 **Storage** (10%)
- 🔐 **Security** (5%)

---

## 🔧 Troubleshooting (25% - 5문제)

### 문제 1: 클러스터 노드 문제 해결 (가중치: 5%)
**Context**: `kubectl config use-context cluster-admin`

A worker node `worker-node-02` is in `NotReady` state. The kubelet service appears to be failing.

**Tasks:**
1. Identify the root cause of the kubelet failure
2. Fix the issue and bring the node to `Ready` state
3. Ensure the fix persists after reboot

```bash
# 현재 상태 확인
kubectl get nodes
kubectl describe node worker-node-02

# 노드에 SSH 접속
ssh worker-node-02

# 서비스 상태 확인
sudo systemctl status kubelet
sudo systemctl status containerd

# 로그 확인
sudo journalctl -u kubelet -f --no-pager
sudo journalctl -u containerd -f --no-pager

# 설정 파일 확인
sudo cat /var/lib/kubelet/config.yaml
sudo cat /etc/kubernetes/kubelet.conf

# 일반적인 해결책
sudo systemctl restart kubelet
sudo systemctl enable kubelet

# 디스크 공간 확인
df -h
sudo du -sh /var/lib/kubelet/*

# 네트워크 확인
ping 8.8.8.8
nslookup kubernetes.default.svc.cluster.local
```

### 문제 2: Pod 시작 실패 문제 해결 (가중치: 5%)
**Namespace**: `production`

Multiple pods in the `production` namespace are failing to start with `ImagePullBackOff` and `CrashLoopBackOff` errors.

**Tasks:**
1. Identify all failing pods and their error reasons
2. Fix the image pull issues
3. Resolve configuration problems causing crashes
4. Verify all pods are running successfully

```bash
# 실패한 Pod 확인
kubectl get pods -n production
kubectl get events -n production --sort-by='.lastTimestamp'

# 각 Pod 상세 정보 확인
kubectl describe pod <failing-pod> -n production
kubectl logs <failing-pod> -n production --previous

# 일반적인 문제들
# 1. 잘못된 이미지 이름
kubectl edit deployment <deployment-name> -n production

# 2. 리소스 부족
kubectl describe nodes
kubectl top nodes
kubectl top pods -n production

# 3. ConfigMap/Secret 누락
kubectl get configmap -n production
kubectl get secret -n production

# 4. 권한 문제
kubectl get serviceaccount -n production
kubectl describe pod <pod-name> -n production | grep -i "service account"
```

### 문제 3: 네트워크 연결 문제 해결 (가중치: 5%)
**Context**: `kubectl config use-context troubleshoot-cluster`

Pods in namespace `frontend` cannot communicate with pods in namespace `backend`. The application is reporting connection timeouts.

**Tasks:**
1. Test connectivity between namespaces
2. Identify network policy restrictions
3. Fix the connectivity issues
4. Verify end-to-end communication

```bash
# 연결 테스트
kubectl run test-pod -n frontend --image=busybox --rm -it -- /bin/sh
# Inside pod: nslookup backend-service.backend.svc.cluster.local
# Inside pod: wget -qO- http://backend-service.backend.svc.cluster.local:8080

# NetworkPolicy 확인
kubectl get networkpolicy -A
kubectl describe networkpolicy -n backend
kubectl describe networkpolicy -n frontend

# DNS 해결 확인
kubectl get svc -n backend
kubectl get endpoints -n backend

# CoreDNS 상태 확인
kubectl get pods -n kube-system | grep coredns
kubectl logs -n kube-system deployment/coredns
```

### 문제 4: ETCD 백업 실패 문제 해결 (가중치: 5%)
**Context**: `kubectl config use-context master-node`

The scheduled ETCD backup is failing. The backup script reports certificate errors.

**Tasks:**
1. Identify why the ETCD backup is failing
2. Fix the certificate issues
3. Successfully create a backup to `/opt/etcd-backup-$(date +%Y%m%d).db`
4. Verify the backup integrity

```bash
# 현재 ETCD 상태 확인
kubectl get pods -n kube-system | grep etcd

# 인증서 위치 확인
sudo find /etc/kubernetes -name "*.crt" -o -name "*.key" | grep etcd

# ETCD 백업 시도
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/etcd-backup-$(date +%Y%m%d).db

# 백업 검증
ETCDCTL_API=3 etcdctl snapshot status /opt/etcd-backup-$(date +%Y%m%d).db

# 인증서 만료 확인
openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text -noout | grep "Not After"
```

### 문제 5: 애플리케이션 성능 문제 해결 (가중치: 5%)
**Namespace**: `monitoring`

A web application is experiencing high response times and occasional timeouts. The application consists of a frontend deployment and a backend database.

**Tasks:**
1. Identify resource bottlenecks
2. Check for memory leaks or CPU spikes
3. Optimize resource allocation
4. Implement proper health checks

```bash
# 리소스 사용량 확인
kubectl top pods -n monitoring
kubectl top nodes

# Pod 상세 정보
kubectl describe pods -n monitoring
kubectl get hpa -n monitoring

# 로그 분석
kubectl logs -n monitoring deployment/frontend --tail=100
kubectl logs -n monitoring deployment/backend --tail=100

# 이벤트 확인
kubectl get events -n monitoring --sort-by='.lastTimestamp'

# 리소스 최적화
kubectl edit deployment frontend -n monitoring
kubectl edit deployment backend -n monitoring
```

---

## 🌐 Services and Networking (20% - 4문제)

### 문제 6: 복잡한 NetworkPolicy 구현 (가중치: 5%)
**Namespaces**: `web-tier`, `app-tier`, `db-tier`

Implement a three-tier application security model using NetworkPolicies.

**Requirements:**
- `web-tier` can only receive traffic from external sources on port 80
- `app-tier` can only receive traffic from `web-tier` on port 8080
- `db-tier` can only receive traffic from `app-tier` on port 5432
- No other communication should be allowed

```yaml
# Web-tier NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: web-tier-policy
  namespace: web-tier
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: app-tier
    ports:
    - protocol: TCP
      port: 8080
  - to: []  # DNS
    ports:
    - protocol: UDP
      port: 53

---
# App-tier NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-tier-policy
  namespace: app-tier
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: web-tier
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: db-tier
    ports:
    - protocol: TCP
      port: 5432
  - to: []  # DNS
    ports:
    - protocol: UDP
      port: 53

---
# DB-tier NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-tier-policy
  namespace: db-tier
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: app-tier
    ports:
    - protocol: TCP
      port: 5432
```

```bash
# 네임스페이스 라벨 설정
kubectl label namespace web-tier name=web-tier
kubectl label namespace app-tier name=app-tier
kubectl label namespace db-tier name=db-tier

# 테스트
kubectl run test-web -n web-tier --image=nginx
kubectl run test-app -n app-tier --image=nginx
kubectl run test-db -n db-tier --image=postgres --env="POSTGRES_PASSWORD=secret"
```

### 문제 7: Ingress와 TLS 설정 (가중치: 5%)
**Namespace**: `web-services`

Configure an Ingress controller to handle multiple domains with TLS termination.

**Requirements:**
- `api.example.com` → `api-service:8080`
- `web.example.com` → `web-service:80`
- Both domains must use TLS with provided certificates
- Redirect HTTP to HTTPS

```bash
# TLS 인증서 생성
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout api-tls.key -out api-tls.crt \
  -subj "/CN=api.example.com"

openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout web-tls.key -out web-tls.crt \
  -subj "/CN=web.example.com"

# Secret 생성
kubectl create secret tls api-tls-secret \
  --cert=api-tls.crt --key=api-tls.key -n web-services

kubectl create secret tls web-tls-secret \
  --cert=web-tls.crt --key=web-tls.key -n web-services
```

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-domain-ingress
  namespace: web-services
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - api.example.com
    secretName: api-tls-secret
  - hosts:
    - web.example.com
    secretName: web-tls-secret
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080
  - host: web.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

### 문제 8: Service Mesh와 Load Balancing (가중치: 5%)
**Namespace**: `microservices`

Configure advanced load balancing for a microservices architecture.

**Requirements:**
- Create a service with session affinity
- Implement weighted routing between service versions
- Configure health checks and readiness probes

```yaml
# Service with Session Affinity
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: microservices
spec:
  selector:
    app: user-service
  ports:
  - port: 8080
    targetPort: 8080
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600

---
# Deployment with Health Checks
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service-v1
  namespace: microservices
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user-service
      version: v1
  template:
    metadata:
      labels:
        app: user-service
        version: v1
    spec:
      containers:
      - name: user-service
        image: nginx:1.21
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi

---
# Weighted Service for Canary Deployment
apiVersion: v1
kind: Service
metadata:
  name: user-service-v2
  namespace: microservices
spec:
  selector:
    app: user-service
    version: v2
  ports:
  - port: 8080
    targetPort: 8080
```

### 문제 9: DNS 및 Service Discovery (가중치: 5%)
**Context**: `kubectl config use-context dns-cluster`

Configure custom DNS resolution and service discovery for a complex application.

**Tasks:**
1. Create a custom DNS entry for external service
2. Configure pod-specific DNS settings
3. Implement service discovery between namespaces
4. Test and verify DNS resolution

```yaml
# Custom DNS ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-dns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        hosts {
           192.168.1.100 external-api.company.com
           fallthrough
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }

---
# Pod with Custom DNS
apiVersion: v1
kind: Pod
metadata:
  name: dns-test-pod
  namespace: default
spec:
  containers:
  - name: test
    image: busybox:1.28
    command: ['sleep', '3600']
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
    - 8.8.8.8
    - 1.1.1.1
    searches:
    - default.svc.cluster.local
    - svc.cluster.local
    - cluster.local
    options:
    - name: ndots
      value: "2"
    - name: edns0
```

```bash
# DNS 테스트
kubectl exec dns-test-pod -- nslookup kubernetes.default.svc.cluster.local
kubectl exec dns-test-pod -- nslookup external-api.company.com

# Service Discovery 테스트
kubectl run test-client --image=busybox:1.28 --rm -it -- /bin/sh
# Inside pod:
# nslookup service-name.namespace.svc.cluster.local
# wget -qO- http://service-name.namespace.svc.cluster.local:8080
```

---

## ⚙️ Cluster Architecture, Installation and Configuration (25% - 5문제)

### 문제 10: 클러스터 업그레이드 (가중치: 5%)
**Context**: `kubectl config use-context upgrade-cluster`

Upgrade a Kubernetes cluster from version 1.28.0 to 1.29.0.

**Tasks:**
1. Upgrade the control plane node
2. Upgrade all worker nodes
3. Verify cluster functionality after upgrade
4. Ensure all system pods are running

```bash
# Control Plane 업그레이드
ssh control-plane-node

# kubeadm 업그레이드
sudo apt update
sudo apt-cache madison kubeadm

sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.29.0-1.1' && \
sudo apt-mark hold kubeadm

# 업그레이드 계획 확인
sudo kubeadm upgrade plan

# 업그레이드 적용
sudo kubeadm upgrade apply v1.29.0

# 노드 드레인
kubectl drain control-plane-node --ignore-daemonsets

# kubelet, kubectl 업그레이드
sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.29.0-1.1' kubectl='1.29.0-1.1' && \
sudo apt-mark hold kubelet kubectl

sudo systemctl daemon-reload
sudo systemctl restart kubelet

# 노드 언코든
kubectl uncordon control-plane-node

# Worker Node 업그레이드
for node in worker-node-01 worker-node-02; do
  ssh $node
  
  sudo apt-mark unhold kubeadm && \
  sudo apt-get update && sudo apt-get install -y kubeadm='1.29.0-1.1' && \
  sudo apt-mark hold kubeadm
  
  sudo kubeadm upgrade node
  
  exit
  
  kubectl drain $node --ignore-daemonsets --force
  
  ssh $node
  
  sudo apt-mark unhold kubelet kubectl && \
  sudo apt-get update && sudo apt-get install -y kubelet='1.29.0-1.1' kubectl='1.29.0-1.1' && \
  sudo apt-mark hold kubelet kubectl
  
  sudo systemctl daemon-reload
  sudo systemctl restart kubelet
  
  exit
  
  kubectl uncordon $node
done

# 업그레이드 검증
kubectl get nodes
kubectl get pods -A
kubectl version
```

### 문제 11: ETCD 클러스터 관리 (가중치: 5%)
**Context**: `kubectl config use-context etcd-cluster`

Manage ETCD cluster operations including backup, restore, and member management.

**Tasks:**
1. Create a complete ETCD backup
2. Add a new ETCD member to the cluster
3. Restore from backup to a new data directory
4. Verify cluster health and data integrity

```bash
# ETCD 백업
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/etcd-backup-$(date +%Y%m%d-%H%M%S).db

# 백업 검증
ETCDCTL_API=3 etcdctl snapshot status /opt/etcd-backup-$(date +%Y%m%d-%H%M%S).db

# ETCD 멤버 확인
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list

# 새 멤버 추가 (예시)
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member add etcd-new --peer-urls=https://10.0.0.4:2380

# 복원 (새 데이터 디렉토리로)
ETCDCTL_API=3 etcdctl --data-dir=/var/lib/etcd-restore \
  snapshot restore /opt/etcd-backup-$(date +%Y%m%d-%H%M%S).db

# etcd.yaml 수정
sudo vi /etc/kubernetes/manifests/etcd.yaml
# --data-dir=/var/lib/etcd-restore 로 변경

# 클러스터 상태 확인
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint health

# 데이터 무결성 확인
kubectl get nodes
kubectl get pods -A
```

### 문제 12: 고가용성 클러스터 구성 (가중치: 5%)
**Context**: `kubectl config use-context ha-cluster`

Configure a highly available Kubernetes cluster with multiple control plane nodes.

**Tasks:**
1. Set up a load balancer for API server
2. Join additional control plane nodes
3. Configure worker nodes to use the load balancer
4. Verify cluster resilience

```bash
# Load Balancer 설정 (HAProxy 예시)
# /etc/haproxy/haproxy.cfg
cat <<EOF | sudo tee -a /etc/haproxy/haproxy.cfg
frontend kubernetes-frontend
    bind *:6443
    mode tcp
    option tcplog
    default_backend kubernetes-backend

backend kubernetes-backend
    mode tcp
    option tcp-check
    balance roundrobin
    server master1 10.0.0.10:6443 check
    server master2 10.0.0.11:6443 check
    server master3 10.0.0.12:6443 check
EOF

sudo systemctl restart haproxy
sudo systemctl enable haproxy

# 첫 번째 Control Plane 초기화
sudo kubeadm init --control-plane-endpoint="loadbalancer.example.com:6443" \
  --upload-certs --pod-network-cidr=10.244.0.0/16

# 추가 Control Plane 노드 조인
sudo kubeadm join loadbalancer.example.com:6443 \
  --token <token> \
  --discovery-token-ca-cert-hash sha256:<hash> \
  --control-plane --certificate-key <certificate-key>

# Worker 노드 조인
sudo kubeadm join loadbalancer.example.com:6443 \
  --token <token> \
  --discovery-token-ca-cert-hash sha256:<hash>

# 클러스터 상태 확인
kubectl get nodes
kubectl get pods -n kube-system
kubectl cluster-info
```

### 문제 13: 인증서 관리 및 갱신 (가중치: 5%)
**Context**: `kubectl config use-context cert-cluster`

Manage Kubernetes cluster certificates including renewal and custom CA.

**Tasks:**
1. Check certificate expiration dates
2. Renew expiring certificates
3. Create a custom CA for additional services
4. Update kubeconfig with new certificates

```bash
# 인증서 만료일 확인
sudo kubeadm certs check-expiration

# 개별 인증서 확인
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep "Not After"
openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text -noout | grep "Not After"

# 모든 인증서 갱신
sudo kubeadm certs renew all

# 특정 인증서만 갱신
sudo kubeadm certs renew apiserver
sudo kubeadm certs renew etcd-server

# 새 kubeconfig 생성
sudo kubeadm init phase kubeconfig admin

# 사용자 정의 CA 생성
openssl genrsa -out custom-ca.key 4096
openssl req -new -x509 -key custom-ca.key -sha256 -subj "/C=US/ST=CA/O=MyOrg/CN=MyCA" \
  -days 3650 -out custom-ca.crt

# 서비스 인증서 생성
openssl genrsa -out service.key 4096
openssl req -new -key service.key -out service.csr \
  -subj "/C=US/ST=CA/O=MyOrg/CN=my-service.default.svc.cluster.local"

openssl x509 -req -in service.csr -CA custom-ca.crt -CAkey custom-ca.key \
  -CAcreateserial -out service.crt -days 365 -sha256

# Secret으로 저장
kubectl create secret tls my-service-tls --cert=service.crt --key=service.key

# 클러스터 재시작 (필요시)
sudo systemctl restart kubelet
```

### 문제 14: 클러스터 네트워킹 구성 (가중치: 5%)
**Context**: `kubectl config use-context network-cluster`

Configure advanced cluster networking including CNI plugins and network policies.

**Tasks:**
1. Install and configure Calico CNI
2. Set up network policies for multi-tenancy
3. Configure pod-to-pod encryption
4. Implement network segmentation

```bash
# Calico 설치
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml

# Calico 설정
cat <<EOF | kubectl apply -f -
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  calicoNetwork:
    ipPools:
    - blockSize: 26
      cidr: 10.244.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()
EOF

# 네트워크 정책 설정
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
EOF

# Pod 간 암호화 설정 (Calico)
cat <<EOF | kubectl apply -f -
apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: default-app-policy
spec:
  selector: app == "secure-app"
  types:
  - Ingress
  - Egress
  ingress:
  - action: Allow
    protocol: TCP
    destination:
      ports: [8080]
  egress:
  - action: Allow
EOF

# 네트워크 세그멘테이션
kubectl label namespace frontend tier=frontend
kubectl label namespace backend tier=backend
kubectl label namespace database tier=database

# 검증
kubectl get pods -n calico-system
calicoctl get nodes
calicoctl get ippool
```

---

## 📦 Workloads & Scheduling (15% - 3문제)

### 문제 15: 고급 스케줄링 및 Affinity (가중치: 5%)
**Namespace**: `scheduling-test`

Implement complex pod scheduling requirements using node affinity, pod affinity, and taints/tolerations.

**Requirements:**
- Schedule pods only on nodes with SSD storage
- Ensure database pods are spread across different availability zones
- Co-locate web and cache pods on the same nodes
- Handle node maintenance with proper tolerations

```yaml
# Node Affinity for SSD requirement
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database
  namespace: scheduling-test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: storage-type
                operator: In
                values:
                - ssd
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - database
            topologyKey: topology.kubernetes.io/zone
      tolerations:
      - key: maintenance
        operator: Equal
        value: "true"
        effect: NoSchedule
        tolerationSeconds: 300
      containers:
      - name: database
        image: postgres:13
        env:
        - name: POSTGRES_PASSWORD
          value: "secret"

---
# Pod Affinity for web and cache co-location
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: scheduling-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cache
            topologyKey: kubernetes.io/hostname
      containers:
      - name: web
        image: nginx:1.21

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cache
  namespace: scheduling-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cache
  template:
    metadata:
      labels:
        app: cache
    spec:
      containers:
      - name: cache
        image: redis:6-alpine
```

```bash
# 노드 라벨링
kubectl label node worker-node-01 storage-type=ssd
kubectl label node worker-node-02 storage-type=hdd
kubectl label node worker-node-01 topology.kubernetes.io/zone=zone-a
kubectl label node worker-node-02 topology.kubernetes.io/zone=zone-b

# 노드 테인트 설정 (유지보수용)
kubectl taint node worker-node-01 maintenance=true:NoSchedule

# 검증
kubectl get pods -n scheduling-test -o wide
kubectl describe pod <pod-name> -n scheduling-test
```

### 문제 16: 리소스 관리 및 HPA/VPA (가중치: 5%)
**Namespace**: `resource-management`

Configure comprehensive resource management including HPA, VPA, and resource quotas.

**Tasks:**
1. Set up HPA with custom metrics
2. Configure VPA for automatic resource adjustment
3. Implement resource quotas and limits
4. Monitor and optimize resource usage

```yaml
# Resource Quota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: resource-quota
  namespace: resource-management
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
    persistentvolumeclaims: "4"
    pods: "10"

---
# Limit Range
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-range
  namespace: resource-management
spec:
  limits:
  - default:
      cpu: 500m
      memory: 512Mi
    defaultRequest:
      cpu: 100m
      memory: 128Mi
    type: Container

---
# Deployment with Resource Requests/Limits
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: resource-management
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: web
        image: nginx:1.21
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        ports:
        - containerPort: 80

---
# HPA with multiple metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
  namespace: resource-management
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60

---
# VPA
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
  namespace: resource-management
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: web
      maxAllowed:
        cpu: 1
        memory: 1Gi
      minAllowed:
        cpu: 50m
        memory: 64Mi
```

### 문제 17: Job 및 CronJob 관리 (가중치: 5%)
**Namespace**: `batch-processing`

Configure complex batch processing workflows using Jobs and CronJobs.

**Requirements:**
- Create a parallel job for data processing
- Set up a CronJob for daily backups
- Implement job failure handling and retries
- Configure job cleanup policies

```yaml
# Parallel Job
apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing-job
  namespace: batch-processing
spec:
  parallelism: 5
  completions: 20
  backoffLimit: 3
  activeDeadlineSeconds: 3600
  ttlSecondsAfterFinished: 86400  # 24 hours
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: processor
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          echo "Processing batch $HOSTNAME"
          # Simulate processing time
          sleep $((RANDOM % 60 + 30))
          echo "Batch $HOSTNAME completed"
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi

---
# CronJob for Daily Backup
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-backup
  namespace: batch-processing
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  timeZone: "UTC"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 300
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 1800  # 30 minutes
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: backup
            image: alpine:3.18
            command:
            - sh
            - -c
            - |
              echo "Starting backup at $(date)"
              # Simulate backup process
              sleep 60
              if [ $((RANDOM % 10)) -lt 8 ]; then
                echo "Backup completed successfully at $(date)"
                exit 0
              else
                echo "Backup failed at $(date)"
                exit 1
              fi
            env:
            - name: BACKUP_LOCATION
              value: "/backup/$(date +%Y%m%d)"
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc

---
# PVC for backup storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: batch-processing
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```

```bash
# Job 모니터링
kubectl get jobs -n batch-processing
kubectl describe job data-processing-job -n batch-processing
kubectl logs -l job-name=data-processing-job -n batch-processing

# CronJob 관리
kubectl get cronjobs -n batch-processing
kubectl describe cronjob daily-backup -n batch-processing

# 수동 Job 실행
kubectl create job manual-backup --from=cronjob/daily-backup -n batch-processing

# Job 정리
kubectl delete job data-processing-job -n batch-processing
```

---

## 💾 Storage (10% - 2문제)

### 문제 18: 동적 스토리지 프로비저닝 (가중치: 5%)
**Namespace**: `storage-demo`

Configure dynamic storage provisioning with multiple storage classes and volume expansion.

**Tasks:**
1. Create storage classes for different performance tiers
2. Implement volume expansion capabilities
3. Configure backup and snapshot policies
4. Test storage failover scenarios

```yaml
# Fast SSD Storage Class
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  encrypted: "true"
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete

---
# Standard HDD Storage Class
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard-hdd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: st1
  encrypted: "true"
allowVolumeExpansion: true
volumeBindingMode: Immediate
reclaimPolicy: Retain

---
# Database PVC with fast storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: database-pvc
  namespace: storage-demo
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 20Gi

---
# Backup PVC with standard storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: storage-demo
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: standard-hdd
  resources:
    requests:
      storage: 100Gi

---
# StatefulSet using dynamic storage
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
  namespace: storage-demo
spec:
  serviceName: database
  replicas: 3
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      containers:
      - name: postgres
        image: postgres:13
        env:
        - name: POSTGRES_PASSWORD
          value: "secret"
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1
            memory: 2Gi
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 10Gi
```

```bash
# 볼륨 확장 테스트
kubectl patch pvc database-pvc -n storage-demo -p '{"spec":{"resources":{"requests":{"storage":"30Gi"}}}}'

# 스토리지 상태 확인
kubectl get pv
kubectl get pvc -n storage-demo
kubectl describe pvc database-pvc -n storage-demo

# 스냅샷 생성 (CSI 드라이버 지원 시)
kubectl create -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: database-snapshot
  namespace: storage-demo
spec:
  volumeSnapshotClassName: csi-aws-vsc
  source:
    persistentVolumeClaimName: database-pvc
EOF
```

### 문제 19: 복잡한 볼륨 마운트 및 데이터 관리 (가중치: 5%)
**Namespace**: `data-management`

Configure complex volume mounting scenarios including shared storage, backup strategies, and data migration.

**Tasks:**
1. Set up shared storage between multiple pods
2. Implement automated backup to external storage
3. Configure data migration between storage classes
4. Test data persistence and recovery

```yaml
# Shared Storage PV (NFS example)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: shared-storage-pv
spec:
  capacity:
    storage: 50Gi
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: shared-nfs
  nfs:
    server: nfs-server.example.com
    path: /shared/data

---
# Shared Storage PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-storage-pvc
  namespace: data-management
spec:
  accessModes:
  - ReadWriteMany
  storageClassName: shared-nfs
  resources:
    requests:
      storage: 50Gi

---
# Multi-container pod with complex volume mounts
apiVersion: v1
kind: Pod
metadata:
  name: data-processor
  namespace: data-management
spec:
  containers:
  - name: processor
    image: alpine:3.18
    command: ['sleep', '3600']
    volumeMounts:
    - name: shared-data
      mountPath: /shared
    - name: config-data
      mountPath: /config
      readOnly: true
    - name: temp-data
      mountPath: /tmp/processing
    - name: backup-data
      mountPath: /backup
  - name: backup-agent
    image: alpine:3.18
    command:
    - sh
    - -c
    - |
      while true; do
        echo "Creating backup at $(date)"
        tar -czf /backup/backup-$(date +%Y%m%d-%H%M%S).tar.gz /shared
        sleep 3600
      done
    volumeMounts:
    - name: shared-data
      mountPath: /shared
      readOnly: true
    - name: backup-data
      mountPath: /backup
  volumes:
  - name: shared-data
    persistentVolumeClaim:
      claimName: shared-storage-pvc
  - name: config-data
    configMap:
      name: processor-config
  - name: temp-data
    emptyDir:
      sizeLimit: 1Gi
  - name: backup-data
    persistentVolumeClaim:
      claimName: backup-pvc

---
# ConfigMap for configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: processor-config
  namespace: data-management
data:
  config.yaml: |
    processing:
      batch_size: 1000
      timeout: 300
    backup:
      interval: 3600
      retention: 7

---
# Backup PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: data-management
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: standard-hdd
```

```bash
# 데이터 마이그레이션 Job
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: data-migration
  namespace: data-management
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: migrator
        image: alpine:3.18
        command:
        - sh
        - -c
        - |
          echo "Starting data migration"
          cp -r /source/* /destination/
          echo "Data migration completed"
        volumeMounts:
        - name: source-data
          mountPath: /source
        - name: destination-data
          mountPath: /destination
      volumes:
      - name: source-data
        persistentVolumeClaim:
          claimName: old-data-pvc
      - name: destination-data
        persistentVolumeClaim:
          claimName: new-data-pvc
EOF

# 볼륨 상태 모니터링
kubectl get pv,pvc -n data-management
kubectl describe pod data-processor -n data-management
kubectl exec -it data-processor -c processor -n data-management -- df -h
```

---

## 🔐 Security (5% - 1문제)

### 문제 20: 종합 보안 구성 (가중치: 5%)
**Context**: `kubectl config use-context security-cluster`

Implement comprehensive security measures including RBAC, Pod Security Standards, and network policies.

**Tasks:**
1. Create custom RBAC roles and bindings
2. Implement Pod Security Standards
3. Configure service accounts with minimal privileges
4. Set up admission controllers and policies

```yaml
# Custom ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-manager
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get", "list"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "create", "update", "patch"]

---
# ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pod-manager-sa
  namespace: secure-apps

---
# ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pod-manager-binding
subjects:
- kind: ServiceAccount
  name: pod-manager-sa
  namespace: secure-apps
roleRef:
  kind: ClusterRole
  name: pod-manager
  apiGroup: rbac.authorization.k8s.io

---
# Pod Security Standards
apiVersion: v1
kind: Namespace
metadata:
  name: secure-apps
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/audit-version: latest
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: latest

---
# Secure Pod
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
  namespace: secure-apps
spec:
  serviceAccountName: pod-manager-sa
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      runAsNonRoot: true
      runAsUser: 1000
      capabilities:
        drop:
        - ALL
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
    volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: var-cache
      mountPath: /var/cache/nginx
    - name: var-run
      mountPath: /var/run
  volumes:
  - name: tmp
    emptyDir: {}
  - name: var-cache
    emptyDir: {}
  - name: var-run
    emptyDir: {}

---
# Network Policy for secure communication
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: secure-app-netpol
  namespace: secure-apps
spec:
  podSelector:
    matchLabels:
      app: secure-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend
    - podSelector:
        matchLabels:
          role: client
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432
  - to: []  # DNS
    ports:
    - protocol: UDP
      port: 53
```

```bash
# RBAC 테스트
kubectl auth can-i create pods --as=system:serviceaccount:secure-apps:pod-manager-sa
kubectl auth can-i delete nodes --as=system:serviceaccount:secure-apps:pod-manager-sa

# Pod Security Standards 테스트
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: insecure-pod
  namespace: secure-apps
spec:
  containers:
  - name: app
    image: nginx
    securityContext:
      privileged: true  # This should be rejected
EOF

# 보안 스캔
kubectl get pods -n secure-apps -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.securityContext}{"\n"}{end}'
```

---

## 📝 시험 전략 및 팁

### 🎯 시간 관리 전략
- **총 시험 시간**: 2시간 (120분)
- **문제별 권장 시간**:
  - 5% 문제: 6분
  - 10% 문제: 12분
  - 15% 문제: 18분
  - 20% 문제: 24분
  - 25% 문제: 30분

### 🔧 필수 명령어 단축키
```bash
# 기본 설정
alias k=kubectl
export do="--dry-run=client -o yaml"
export now="--force --grace-period 0"

# 자주 사용하는 명령어
k get po -A -o wide
k describe po <pod-name>
k logs <pod-name> -f --previous
k edit <resource> <name>
k delete po <pod-name> $now
```

### 📚 문제 해결 순서
1. **문제 요구사항 정확히 파악**
2. **Context 및 Namespace 확인**
3. **기존 리소스 상태 점검**
4. **단계별 해결 및 검증**
5. **최종 동작 확인**

### 🚨 Troubleshooting 체크리스트
```bash
# Pod 문제
kubectl get pods -A
kubectl describe pod <pod-name>
kubectl logs <pod-name> --previous
kubectl get events --sort-by='.lastTimestamp'

# Node 문제
kubectl get nodes
kubectl describe node <node-name>
kubectl top nodes

# Network 문제
kubectl get svc,ep
kubectl get networkpolicy -A
kubectl exec -it <pod> -- nslookup <service>

# Storage 문제
kubectl get pv,pvc
kubectl describe pvc <pvc-name>
kubectl get storageclass
```

### 📖 주요 참고 문서
- **Kubernetes 공식 문서**: https://kubernetes.io/docs/
- **kubectl 치트시트**: https://kubernetes.io/docs/reference/kubectl/cheatsheet/
- **Troubleshooting**: https://kubernetes.io/docs/tasks/debug/
- **네트워킹**: https://kubernetes.io/docs/concepts/services-networking/
- **보안**: https://kubernetes.io/docs/concepts/security/

---

**이 문제집으로 약점 영역을 집중 공략하여 CKA 시험에 합격하세요! 🚀**

**Good Luck! 💪**