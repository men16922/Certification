# CKA 2025 ì˜ˆìƒ ì‹œí—˜ ë¬¸ì œ - Version 2

## ğŸ“Š ì‹œí—˜ ê²°ê³¼ ë¶„ì„ ê¸°ë°˜ ì§‘ì¤‘ ëŒ€ë¹„
**ì•½ì  ì˜ì—­ ì§‘ì¤‘ ê³µëµ:**
- ğŸ”§ **Troubleshooting** (25%)
- ğŸŒ **Services and Networking** (20%) 
- âš™ï¸ **Cluster Architecture, Installation and Configuration** (25%)
- ğŸ“¦ **Workloads & Scheduling** (15%)
- ğŸ’¾ **Storage** (10%)
- ğŸ” **Security** (5%)

---

## ğŸ”§ Troubleshooting (25% - 5ë¬¸ì œ)

### ë¬¸ì œ 1: í´ëŸ¬ìŠ¤í„° ë…¸ë“œ ë¬¸ì œ í•´ê²° (ê°€ì¤‘ì¹˜: 5%)
**Context**: `kubectl config use-context cluster-admin`

A worker node `worker-node-02` is in `NotReady` state. The kubelet service appears to be failing.

**Tasks:**
1. Identify the root cause of the kubelet failure
2. Fix the issue and bring the node to `Ready` state
3. Ensure the fix persists after reboot

```bash
# í˜„ì¬ ìƒíƒœ í™•ì¸
kubectl get nodes
kubectl describe node worker-node-02

# ë…¸ë“œì— SSH ì ‘ì†
ssh worker-node-02

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
sudo systemctl status kubelet
sudo systemctl status containerd

# ë¡œê·¸ í™•ì¸
sudo journalctl -u kubelet -f --no-pager
sudo journalctl -u containerd -f --no-pager

# ì„¤ì • íŒŒì¼ í™•ì¸
sudo cat /var/lib/kubelet/config.yaml
sudo cat /etc/kubernetes/kubelet.conf

# ì¼ë°˜ì ì¸ í•´ê²°ì±…
sudo systemctl restart kubelet
sudo systemctl enable kubelet

# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
df -h
sudo du -sh /var/lib/kubelet/*

# ë„¤íŠ¸ì›Œí¬ í™•ì¸
ping 8.8.8.8
nslookup kubernetes.default.svc.cluster.local
```

### ë¬¸ì œ 2: Pod ì‹œì‘ ì‹¤íŒ¨ ë¬¸ì œ í•´ê²° (ê°€ì¤‘ì¹˜: 5%)
**Namespace**: `production`

Multiple pods in the `production` namespace are failing to start with `ImagePullBackOff` and `CrashLoopBackOff` errors.

**Tasks:**
1. Identify all failing pods and their error reasons
2. Fix the image pull issues
3. Resolve configuration problems causing crashes
4. Verify all pods are running successfully

```bash
# ì‹¤íŒ¨í•œ Pod í™•ì¸
kubectl get pods -n production
kubectl get events -n production --sort-by='.lastTimestamp'

# ê° Pod ìƒì„¸ ì •ë³´ í™•ì¸
kubectl describe pod <failing-pod> -n production
kubectl logs <failing-pod> -n production --previous

# ì¼ë°˜ì ì¸ ë¬¸ì œë“¤
# 1. ì˜ëª»ëœ ì´ë¯¸ì§€ ì´ë¦„
kubectl edit deployment <deployment-name> -n production

# 2. ë¦¬ì†ŒìŠ¤ ë¶€ì¡±
kubectl describe nodes
kubectl top nodes
kubectl top pods -n production

# 3. ConfigMap/Secret ëˆ„ë½
kubectl get configmap -n production
kubectl get secret -n production

# 4. ê¶Œí•œ ë¬¸ì œ
kubectl get serviceaccount -n production
kubectl describe pod <pod-name> -n production | grep -i "service account"
```

### ë¬¸ì œ 3: ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ í•´ê²° (ê°€ì¤‘ì¹˜: 5%)
**Context**: `kubectl config use-context troubleshoot-cluster`

Pods in namespace `frontend` cannot communicate with pods in namespace `backend`. The application is reporting connection timeouts.

**Tasks:**
1. Test connectivity between namespaces
2. Identify network policy restrictions
3. Fix the connectivity issues
4. Verify end-to-end communication

```bash
# ì—°ê²° í…ŒìŠ¤íŠ¸
kubectl run test-pod -n frontend --image=busybox --rm -it -- /bin/sh
# Inside pod: nslookup backend-service.backend.svc.cluster.local
# Inside pod: wget -qO- http://backend-service.backend.svc.cluster.local:8080

# NetworkPolicy í™•ì¸
kubectl get networkpolicy -A
kubectl describe networkpolicy -n backend
kubectl describe networkpolicy -n frontend

# DNS í•´ê²° í™•ì¸
kubectl get svc -n backend
kubectl get endpoints -n backend

# CoreDNS ìƒíƒœ í™•ì¸
kubectl get pods -n kube-system | grep coredns
kubectl logs -n kube-system deployment/coredns
```

### ë¬¸ì œ 4: ETCD ë°±ì—… ì‹¤íŒ¨ ë¬¸ì œ í•´ê²° (ê°€ì¤‘ì¹˜: 5%)
**Context**: `kubectl config use-context master-node`

The scheduled ETCD backup is failing. The backup script reports certificate errors.

**Tasks:**
1. Identify why the ETCD backup is failing
2. Fix the certificate issues
3. Successfully create a backup to `/opt/etcd-backup-$(date +%Y%m%d).db`
4. Verify the backup integrity

```bash
# í˜„ì¬ ETCD ìƒíƒœ í™•ì¸
kubectl get pods -n kube-system | grep etcd

# ì¸ì¦ì„œ ìœ„ì¹˜ í™•ì¸
sudo find /etc/kubernetes -name "*.crt" -o -name "*.key" | grep etcd

# ETCD ë°±ì—… ì‹œë„
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/etcd-backup-$(date +%Y%m%d).db

# ë°±ì—… ê²€ì¦
ETCDCTL_API=3 etcdctl snapshot status /opt/etcd-backup-$(date +%Y%m%d).db

# ì¸ì¦ì„œ ë§Œë£Œ í™•ì¸
openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text -noout | grep "Not After"
```

### ë¬¸ì œ 5: ì• í”Œë¦¬ì¼€ì´ì…˜ ì„±ëŠ¥ ë¬¸ì œ í•´ê²° (ê°€ì¤‘ì¹˜: 5%)
**Namespace**: `monitoring`

A web application is experiencing high response times and occasional timeouts. The application consists of a frontend deployment and a backend database.

**Tasks:**
1. Identify resource bottlenecks
2. Check for memory leaks or CPU spikes
3. Optimize resource allocation
4. Implement proper health checks

```bash
# ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í™•ì¸
kubectl top pods -n monitoring
kubectl top nodes

# Pod ìƒì„¸ ì •ë³´
kubectl describe pods -n monitoring
kubectl get hpa -n monitoring

# ë¡œê·¸ ë¶„ì„
kubectl logs -n monitoring deployment/frontend --tail=100
kubectl logs -n monitoring deployment/backend --tail=100

# ì´ë²¤íŠ¸ í™•ì¸
kubectl get events -n monitoring --sort-by='.lastTimestamp'

# ë¦¬ì†ŒìŠ¤ ìµœì í™”
kubectl edit deployment frontend -n monitoring
kubectl edit deployment backend -n monitoring
```

---

## ğŸŒ Services and Networking (20% - 4ë¬¸ì œ)

### ë¬¸ì œ 6: ë³µì¡í•œ NetworkPolicy êµ¬í˜„ (ê°€ì¤‘ì¹˜: 5%)
**Namespaces**: `web-tier`, `app-tier`, `db-tier`

Implement a three-tier application security model using NetworkPolicies.

**Requirements:**
- `web-tier` can only receive traffic from external sources on port 80
- `app-tier` can only receive traffic from `web-tier` on port 8080
- `db-tier` can only receive traffic from `app-tier` on port 5432
- No other communication should be allowed

```yaml
# Web-tier NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: web-tier-policy
  namespace: web-tier
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: app-tier
    ports:
    - protocol: TCP
      port: 8080
  - to: []  # DNS
    ports:
    - protocol: UDP
      port: 53

---
# App-tier NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-tier-policy
  namespace: app-tier
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: web-tier
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: db-tier
    ports:
    - protocol: TCP
      port: 5432
  - to: []  # DNS
    ports:
    - protocol: UDP
      port: 53

---
# DB-tier NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-tier-policy
  namespace: db-tier
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: app-tier
    ports:
    - protocol: TCP
      port: 5432
```

```bash
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¼ë²¨ ì„¤ì •
kubectl label namespace web-tier name=web-tier
kubectl label namespace app-tier name=app-tier
kubectl label namespace db-tier name=db-tier

# í…ŒìŠ¤íŠ¸
kubectl run test-web -n web-tier --image=nginx
kubectl run test-app -n app-tier --image=nginx
kubectl run test-db -n db-tier --image=postgres --env="POSTGRES_PASSWORD=secret"
```

### ë¬¸ì œ 7: Ingressì™€ TLS ì„¤ì • (ê°€ì¤‘ì¹˜: 5%)
**Namespace**: `web-services`

Configure an Ingress controller to handle multiple domains with TLS termination.

**Requirements:**
- `api.example.com` â†’ `api-service:8080`
- `web.example.com` â†’ `web-service:80`
- Both domains must use TLS with provided certificates
- Redirect HTTP to HTTPS

```bash
# TLS ì¸ì¦ì„œ ìƒì„±
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout api-tls.key -out api-tls.crt \
  -subj "/CN=api.example.com"

openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout web-tls.key -out web-tls.crt \
  -subj "/CN=web.example.com"

# Secret ìƒì„±
kubectl create secret tls api-tls-secret \
  --cert=api-tls.crt --key=api-tls.key -n web-services

kubectl create secret tls web-tls-secret \
  --cert=web-tls.crt --key=web-tls.key -n web-services
```

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-domain-ingress
  namespace: web-services
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - api.example.com
    secretName: api-tls-secret
  - hosts:
    - web.example.com
    secretName: web-tls-secret
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8080
  - host: web.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

### ë¬¸ì œ 8: Service Meshì™€ Load Balancing (ê°€ì¤‘ì¹˜: 5%)
**Namespace**: `microservices`

Configure advanced load balancing for a microservices architecture.

**Requirements:**
- Create a service with session affinity
- Implement weighted routing between service versions
- Configure health checks and readiness probes

```yaml
# Service with Session Affinity
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: microservices
spec:
  selector:
    app: user-service
  ports:
  - port: 8080
    targetPort: 8080
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600

---
# Deployment with Health Checks
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service-v1
  namespace: microservices
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user-service
      version: v1
  template:
    metadata:
      labels:
        app: user-service
        version: v1
    spec:
      containers:
      - name: user-service
        image: nginx:1.21
        ports:
        - containerPort: 8080
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi

---
# Weighted Service for Canary Deployment
apiVersion: v1
kind: Service
metadata:
  name: user-service-v2
  namespace: microservices
spec:
  selector:
    app: user-service
    version: v2
  ports:
  - port: 8080
    targetPort: 8080
```

### ë¬¸ì œ 9: DNS ë° Service Discovery (ê°€ì¤‘ì¹˜: 5%)
**Context**: `kubectl config use-context dns-cluster`

Configure custom DNS resolution and service discovery for a complex application.

**Tasks:**
1. Create a custom DNS entry for external service
2. Configure pod-specific DNS settings
3. Implement service discovery between namespaces
4. Test and verify DNS resolution

```yaml
# Custom DNS ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-dns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        hosts {
           192.168.1.100 external-api.company.com
           fallthrough
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }

---
# Pod with Custom DNS
apiVersion: v1
kind: Pod
metadata:
  name: dns-test-pod
  namespace: default
spec:
  containers:
  - name: test
    image: busybox:1.28
    command: ['sleep', '3600']
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
    - 8.8.8.8
    - 1.1.1.1
    searches:
    - default.svc.cluster.local
    - svc.cluster.local
    - cluster.local
    options:
    - name: ndots
      value: "2"
    - name: edns0
```

```bash
# DNS í…ŒìŠ¤íŠ¸
kubectl exec dns-test-pod -- nslookup kubernetes.default.svc.cluster.local
kubectl exec dns-test-pod -- nslookup external-api.company.com

# Service Discovery í…ŒìŠ¤íŠ¸
kubectl run test-client --image=busybox:1.28 --rm -it -- /bin/sh
# Inside pod:
# nslookup service-name.namespace.svc.cluster.local
# wget -qO- http://service-name.namespace.svc.cluster.local:8080
```

---

## âš™ï¸ Cluster Architecture, Installation and Configuration (25% - 5ë¬¸ì œ)

### ë¬¸ì œ 10: í´ëŸ¬ìŠ¤í„° ì—…ê·¸ë ˆì´ë“œ (ê°€ì¤‘ì¹˜: 5%)
**Context**: `kubectl config use-context upgrade-cluster`

Upgrade a Kubernetes cluster from version 1.28.0 to 1.29.0.

**Tasks:**
1. Upgrade the control plane node
2. Upgrade all worker nodes
3. Verify cluster functionality after upgrade
4. Ensure all system pods are running

```bash
# Control Plane ì—…ê·¸ë ˆì´ë“œ
ssh control-plane-node

# kubeadm ì—…ê·¸ë ˆì´ë“œ
sudo apt update
sudo apt-cache madison kubeadm

sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.29.0-1.1' && \
sudo apt-mark hold kubeadm

# ì—…ê·¸ë ˆì´ë“œ ê³„íš í™•ì¸
sudo kubeadm upgrade plan

# ì—…ê·¸ë ˆì´ë“œ ì ìš©
sudo kubeadm upgrade apply v1.29.0

# ë…¸ë“œ ë“œë ˆì¸
kubectl drain control-plane-node --ignore-daemonsets

# kubelet, kubectl ì—…ê·¸ë ˆì´ë“œ
sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.29.0-1.1' kubectl='1.29.0-1.1' && \
sudo apt-mark hold kubelet kubectl

sudo systemctl daemon-reload
sudo systemctl restart kubelet

# ë…¸ë“œ ì–¸ì½”ë“ 
kubectl uncordon control-plane-node

# Worker Node ì—…ê·¸ë ˆì´ë“œ
for node in worker-node-01 worker-node-02; do
  ssh $node
  
  sudo apt-mark unhold kubeadm && \
  sudo apt-get update && sudo apt-get install -y kubeadm='1.29.0-1.1' && \
  sudo apt-mark hold kubeadm
  
  sudo kubeadm upgrade node
  
  exit
  
  kubectl drain $node --ignore-daemonsets --force
  
  ssh $node
  
  sudo apt-mark unhold kubelet kubectl && \
  sudo apt-get update && sudo apt-get install -y kubelet='1.29.0-1.1' kubectl='1.29.0-1.1' && \
  sudo apt-mark hold kubelet kubectl
  
  sudo systemctl daemon-reload
  sudo systemctl restart kubelet
  
  exit
  
  kubectl uncordon $node
done

# ì—…ê·¸ë ˆì´ë“œ ê²€ì¦
kubectl get nodes
kubectl get pods -A
kubectl version
```

### ë¬¸ì œ 11: ETCD í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ (ê°€ì¤‘ì¹˜: 5%)
**Context**: `kubectl config use-context etcd-cluster`

Manage ETCD cluster operations including backup, restore, and member management.

**Tasks:**
1. Create a complete ETCD backup
2. Add a new ETCD member to the cluster
3. Restore from backup to a new data directory
4. Verify cluster health and data integrity

```bash
# ETCD ë°±ì—…
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/etcd-backup-$(date +%Y%m%d-%H%M%S).db

# ë°±ì—… ê²€ì¦
ETCDCTL_API=3 etcdctl snapshot status /opt/etcd-backup-$(date +%Y%m%d-%H%M%S).db

# ETCD ë©¤ë²„ í™•ì¸
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list

# ìƒˆ ë©¤ë²„ ì¶”ê°€ (ì˜ˆì‹œ)
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member add etcd-new --peer-urls=https://10.0.0.4:2380

# ë³µì› (ìƒˆ ë°ì´í„° ë””ë ‰í† ë¦¬ë¡œ)
ETCDCTL_API=3 etcdctl --data-dir=/var/lib/etcd-restore \
  snapshot restore /opt/etcd-backup-$(date +%Y%m%d-%H%M%S).db

# etcd.yaml ìˆ˜ì •
sudo vi /etc/kubernetes/manifests/etcd.yaml
# --data-dir=/var/lib/etcd-restore ë¡œ ë³€ê²½

# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint health

# ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
kubectl get nodes
kubectl get pods -A
```

### ë¬¸ì œ 12: ê³ ê°€ìš©ì„± í´ëŸ¬ìŠ¤í„° êµ¬ì„± (ê°€ì¤‘ì¹˜: 5%)
**Context**: `kubectl config use-context ha-cluster`

Configure a highly available Kubernetes cluster with multiple control plane nodes.

**Tasks:**
1. Set up a load balancer for API server
2. Join additional control plane nodes
3. Configure worker nodes to use the load balancer
4. Verify cluster resilience

```bash
# Load Balancer ì„¤ì • (HAProxy ì˜ˆì‹œ)
# /etc/haproxy/haproxy.cfg
cat <<EOF | sudo tee -a /etc/haproxy/haproxy.cfg
frontend kubernetes-frontend
    bind *:6443
    mode tcp
    option tcplog
    default_backend kubernetes-backend

backend kubernetes-backend
    mode tcp
    option tcp-check
    balance roundrobin
    server master1 10.0.0.10:6443 check
    server master2 10.0.0.11:6443 check
    server master3 10.0.0.12:6443 check
EOF

sudo systemctl restart haproxy
sudo systemctl enable haproxy

# ì²« ë²ˆì§¸ Control Plane ì´ˆê¸°í™”
sudo kubeadm init --control-plane-endpoint="loadbalancer.example.com:6443" \
  --upload-certs --pod-network-cidr=10.244.0.0/16

# ì¶”ê°€ Control Plane ë…¸ë“œ ì¡°ì¸
sudo kubeadm join loadbalancer.example.com:6443 \
  --token <token> \
  --discovery-token-ca-cert-hash sha256:<hash> \
  --control-plane --certificate-key <certificate-key>

# Worker ë…¸ë“œ ì¡°ì¸
sudo kubeadm join loadbalancer.example.com:6443 \
  --token <token> \
  --discovery-token-ca-cert-hash sha256:<hash>

# í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
kubectl get nodes
kubectl get pods -n kube-system
kubectl cluster-info
```

### ë¬¸ì œ 13: ì¸ì¦ì„œ ê´€ë¦¬ ë° ê°±ì‹  (ê°€ì¤‘ì¹˜: 5%)
**Context**: `kubectl config use-context cert-cluster`

Manage Kubernetes cluster certificates including renewal and custom CA.

**Tasks:**
1. Check certificate expiration dates
2. Renew expiring certificates
3. Create a custom CA for additional services
4. Update kubeconfig with new certificates

```bash
# ì¸ì¦ì„œ ë§Œë£Œì¼ í™•ì¸
sudo kubeadm certs check-expiration

# ê°œë³„ ì¸ì¦ì„œ í™•ì¸
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep "Not After"
openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text -noout | grep "Not After"

# ëª¨ë“  ì¸ì¦ì„œ ê°±ì‹ 
sudo kubeadm certs renew all

# íŠ¹ì • ì¸ì¦ì„œë§Œ ê°±ì‹ 
sudo kubeadm certs renew apiserver
sudo kubeadm certs renew etcd-server

# ìƒˆ kubeconfig ìƒì„±
sudo kubeadm init phase kubeconfig admin

# ì‚¬ìš©ì ì •ì˜ CA ìƒì„±
openssl genrsa -out custom-ca.key 4096
openssl req -new -x509 -key custom-ca.key -sha256 -subj "/C=US/ST=CA/O=MyOrg/CN=MyCA" \
  -days 3650 -out custom-ca.crt

# ì„œë¹„ìŠ¤ ì¸ì¦ì„œ ìƒì„±
openssl genrsa -out service.key 4096
openssl req -new -key service.key -out service.csr \
  -subj "/C=US/ST=CA/O=MyOrg/CN=my-service.default.svc.cluster.local"

openssl x509 -req -in service.csr -CA custom-ca.crt -CAkey custom-ca.key \
  -CAcreateserial -out service.crt -days 365 -sha256

# Secretìœ¼ë¡œ ì €ì¥
kubectl create secret tls my-service-tls --cert=service.crt --key=service.key

# í´ëŸ¬ìŠ¤í„° ì¬ì‹œì‘ (í•„ìš”ì‹œ)
sudo systemctl restart kubelet
```

### ë¬¸ì œ 14: í´ëŸ¬ìŠ¤í„° ë„¤íŠ¸ì›Œí‚¹ êµ¬ì„± (ê°€ì¤‘ì¹˜: 5%)
**Context**: `kubectl config use-context network-cluster`

Configure advanced cluster networking including CNI plugins and network policies.

**Tasks:**
1. Install and configure Calico CNI
2. Set up network policies for multi-tenancy
3. Configure pod-to-pod encryption
4. Implement network segmentation

```bash
# Calico ì„¤ì¹˜
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml

# Calico ì„¤ì •
cat <<EOF | kubectl apply -f -
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  calicoNetwork:
    ipPools:
    - blockSize: 26
      cidr: 10.244.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()
EOF

# ë„¤íŠ¸ì›Œí¬ ì •ì±… ì„¤ì •
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
EOF

# Pod ê°„ ì•”í˜¸í™” ì„¤ì • (Calico)
cat <<EOF | kubectl apply -f -
apiVersion: projectcalico.org/v3
kind: GlobalNetworkPolicy
metadata:
  name: default-app-policy
spec:
  selector: app == "secure-app"
  types:
  - Ingress
  - Egress
  ingress:
  - action: Allow
    protocol: TCP
    destination:
      ports: [8080]
  egress:
  - action: Allow
EOF

# ë„¤íŠ¸ì›Œí¬ ì„¸ê·¸ë©˜í…Œì´ì…˜
kubectl label namespace frontend tier=frontend
kubectl label namespace backend tier=backend
kubectl label namespace database tier=database

# ê²€ì¦
kubectl get pods -n calico-system
calicoctl get nodes
calicoctl get ippool
```

---

## ğŸ“¦ Workloads & Scheduling (15% - 3ë¬¸ì œ)

### ë¬¸ì œ 15: ê³ ê¸‰ ìŠ¤ì¼€ì¤„ë§ ë° Affinity (ê°€ì¤‘ì¹˜: 5%)
**Namespace**: `scheduling-test`

Implement complex pod scheduling requirements using node affinity, pod affinity, and taints/tolerations.

**Requirements:**
- Schedule pods only on nodes with SSD storage
- Ensure database pods are spread across different availability zones
- Co-locate web and cache pods on the same nodes
- Handle node maintenance with proper tolerations

```yaml
# Node Affinity for SSD requirement
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database
  namespace: scheduling-test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: storage-type
                operator: In
                values:
                - ssd
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - database
            topologyKey: topology.kubernetes.io/zone
      tolerations:
      - key: maintenance
        operator: Equal
        value: "true"
        effect: NoSchedule
        tolerationSeconds: 300
      containers:
      - name: database
        image: postgres:13
        env:
        - name: POSTGRES_PASSWORD
          value: "secret"

---
# Pod Affinity for web and cache co-location
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: scheduling-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cache
            topologyKey: kubernetes.io/hostname
      containers:
      - name: web
        image: nginx:1.21

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cache
  namespace: scheduling-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cache
  template:
    metadata:
      labels:
        app: cache
    spec:
      containers:
      - name: cache
        image: redis:6-alpine
```

```bash
# ë…¸ë“œ ë¼ë²¨ë§
kubectl label node worker-node-01 storage-type=ssd
kubectl label node worker-node-02 storage-type=hdd
kubectl label node worker-node-01 topology.kubernetes.io/zone=zone-a
kubectl label node worker-node-02 topology.kubernetes.io/zone=zone-b

# ë…¸ë“œ í…Œì¸íŠ¸ ì„¤ì • (ìœ ì§€ë³´ìˆ˜ìš©)
kubectl taint node worker-node-01 maintenance=true:NoSchedule

# ê²€ì¦
kubectl get pods -n scheduling-test -o wide
kubectl describe pod <pod-name> -n scheduling-test
```

### ë¬¸ì œ 16: ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ë° HPA/VPA (ê°€ì¤‘ì¹˜: 5%)
**Namespace**: `resource-management`

Configure comprehensive resource management including HPA, VPA, and resource quotas.

**Tasks:**
1. Set up HPA with custom metrics
2. Configure VPA for automatic resource adjustment
3. Implement resource quotas and limits
4. Monitor and optimize resource usage

```yaml
# Resource Quota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: resource-quota
  namespace: resource-management
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
    persistentvolumeclaims: "4"
    pods: "10"

---
# Limit Range
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-range
  namespace: resource-management
spec:
  limits:
  - default:
      cpu: 500m
      memory: 512Mi
    defaultRequest:
      cpu: 100m
      memory: 128Mi
    type: Container

---
# Deployment with Resource Requests/Limits
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: resource-management
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: web
        image: nginx:1.21
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        ports:
        - containerPort: 80

---
# HPA with multiple metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
  namespace: resource-management
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60

---
# VPA
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-app-vpa
  namespace: resource-management
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: web
      maxAllowed:
        cpu: 1
        memory: 1Gi
      minAllowed:
        cpu: 50m
        memory: 64Mi
```

### ë¬¸ì œ 17: Job ë° CronJob ê´€ë¦¬ (ê°€ì¤‘ì¹˜: 5%)
**Namespace**: `batch-processing`

Configure complex batch processing workflows using Jobs and CronJobs.

**Requirements:**
- Create a parallel job for data processing
- Set up a CronJob for daily backups
- Implement job failure handling and retries
- Configure job cleanup policies

```yaml
# Parallel Job
apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing-job
  namespace: batch-processing
spec:
  parallelism: 5
  completions: 20
  backoffLimit: 3
  activeDeadlineSeconds: 3600
  ttlSecondsAfterFinished: 86400  # 24 hours
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: processor
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          echo "Processing batch $HOSTNAME"
          # Simulate processing time
          sleep $((RANDOM % 60 + 30))
          echo "Batch $HOSTNAME completed"
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi

---
# CronJob for Daily Backup
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-backup
  namespace: batch-processing
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  timeZone: "UTC"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 300
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 1800  # 30 minutes
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: backup
            image: alpine:3.18
            command:
            - sh
            - -c
            - |
              echo "Starting backup at $(date)"
              # Simulate backup process
              sleep 60
              if [ $((RANDOM % 10)) -lt 8 ]; then
                echo "Backup completed successfully at $(date)"
                exit 0
              else
                echo "Backup failed at $(date)"
                exit 1
              fi
            env:
            - name: BACKUP_LOCATION
              value: "/backup/$(date +%Y%m%d)"
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc

---
# PVC for backup storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: batch-processing
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```

```bash
# Job ëª¨ë‹ˆí„°ë§
kubectl get jobs -n batch-processing
kubectl describe job data-processing-job -n batch-processing
kubectl logs -l job-name=data-processing-job -n batch-processing

# CronJob ê´€ë¦¬
kubectl get cronjobs -n batch-processing
kubectl describe cronjob daily-backup -n batch-processing

# ìˆ˜ë™ Job ì‹¤í–‰
kubectl create job manual-backup --from=cronjob/daily-backup -n batch-processing

# Job ì •ë¦¬
kubectl delete job data-processing-job -n batch-processing
```

---

## ğŸ’¾ Storage (10% - 2ë¬¸ì œ)

### ë¬¸ì œ 18: ë™ì  ìŠ¤í† ë¦¬ì§€ í”„ë¡œë¹„ì €ë‹ (ê°€ì¤‘ì¹˜: 5%)
**Namespace**: `storage-demo`

Configure dynamic storage provisioning with multiple storage classes and volume expansion.

**Tasks:**
1. Create storage classes for different performance tiers
2. Implement volume expansion capabilities
3. Configure backup and snapshot policies
4. Test storage failover scenarios

```yaml
# Fast SSD Storage Class
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  encrypted: "true"
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete

---
# Standard HDD Storage Class
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard-hdd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: st1
  encrypted: "true"
allowVolumeExpansion: true
volumeBindingMode: Immediate
reclaimPolicy: Retain

---
# Database PVC with fast storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: database-pvc
  namespace: storage-demo
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 20Gi

---
# Backup PVC with standard storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: storage-demo
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: standard-hdd
  resources:
    requests:
      storage: 100Gi

---
# StatefulSet using dynamic storage
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
  namespace: storage-demo
spec:
  serviceName: database
  replicas: 3
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      containers:
      - name: postgres
        image: postgres:13
        env:
        - name: POSTGRES_PASSWORD
          value: "secret"
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1
            memory: 2Gi
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 10Gi
```

```bash
# ë³¼ë¥¨ í™•ì¥ í…ŒìŠ¤íŠ¸
kubectl patch pvc database-pvc -n storage-demo -p '{"spec":{"resources":{"requests":{"storage":"30Gi"}}}}'

# ìŠ¤í† ë¦¬ì§€ ìƒíƒœ í™•ì¸
kubectl get pv
kubectl get pvc -n storage-demo
kubectl describe pvc database-pvc -n storage-demo

# ìŠ¤ëƒ…ìƒ· ìƒì„± (CSI ë“œë¼ì´ë²„ ì§€ì› ì‹œ)
kubectl create -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: database-snapshot
  namespace: storage-demo
spec:
  volumeSnapshotClassName: csi-aws-vsc
  source:
    persistentVolumeClaimName: database-pvc
EOF
```

### ë¬¸ì œ 19: ë³µì¡í•œ ë³¼ë¥¨ ë§ˆìš´íŠ¸ ë° ë°ì´í„° ê´€ë¦¬ (ê°€ì¤‘ì¹˜: 5%)
**Namespace**: `data-management`

Configure complex volume mounting scenarios including shared storage, backup strategies, and data migration.

**Tasks:**
1. Set up shared storage between multiple pods
2. Implement automated backup to external storage
3. Configure data migration between storage classes
4. Test data persistence and recovery

```yaml
# Shared Storage PV (NFS example)
apiVersion: v1
kind: PersistentVolume
metadata:
  name: shared-storage-pv
spec:
  capacity:
    storage: 50Gi
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: shared-nfs
  nfs:
    server: nfs-server.example.com
    path: /shared/data

---
# Shared Storage PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-storage-pvc
  namespace: data-management
spec:
  accessModes:
  - ReadWriteMany
  storageClassName: shared-nfs
  resources:
    requests:
      storage: 50Gi

---
# Multi-container pod with complex volume mounts
apiVersion: v1
kind: Pod
metadata:
  name: data-processor
  namespace: data-management
spec:
  containers:
  - name: processor
    image: alpine:3.18
    command: ['sleep', '3600']
    volumeMounts:
    - name: shared-data
      mountPath: /shared
    - name: config-data
      mountPath: /config
      readOnly: true
    - name: temp-data
      mountPath: /tmp/processing
    - name: backup-data
      mountPath: /backup
  - name: backup-agent
    image: alpine:3.18
    command:
    - sh
    - -c
    - |
      while true; do
        echo "Creating backup at $(date)"
        tar -czf /backup/backup-$(date +%Y%m%d-%H%M%S).tar.gz /shared
        sleep 3600
      done
    volumeMounts:
    - name: shared-data
      mountPath: /shared
      readOnly: true
    - name: backup-data
      mountPath: /backup
  volumes:
  - name: shared-data
    persistentVolumeClaim:
      claimName: shared-storage-pvc
  - name: config-data
    configMap:
      name: processor-config
  - name: temp-data
    emptyDir:
      sizeLimit: 1Gi
  - name: backup-data
    persistentVolumeClaim:
      claimName: backup-pvc

---
# ConfigMap for configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: processor-config
  namespace: data-management
data:
  config.yaml: |
    processing:
      batch_size: 1000
      timeout: 300
    backup:
      interval: 3600
      retention: 7

---
# Backup PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: data-management
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: standard-hdd
```

```bash
# ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ Job
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: data-migration
  namespace: data-management
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: migrator
        image: alpine:3.18
        command:
        - sh
        - -c
        - |
          echo "Starting data migration"
          cp -r /source/* /destination/
          echo "Data migration completed"
        volumeMounts:
        - name: source-data
          mountPath: /source
        - name: destination-data
          mountPath: /destination
      volumes:
      - name: source-data
        persistentVolumeClaim:
          claimName: old-data-pvc
      - name: destination-data
        persistentVolumeClaim:
          claimName: new-data-pvc
EOF

# ë³¼ë¥¨ ìƒíƒœ ëª¨ë‹ˆí„°ë§
kubectl get pv,pvc -n data-management
kubectl describe pod data-processor -n data-management
kubectl exec -it data-processor -c processor -n data-management -- df -h
```

---

## ğŸ” Security (5% - 1ë¬¸ì œ)

### ë¬¸ì œ 20: ì¢…í•© ë³´ì•ˆ êµ¬ì„± (ê°€ì¤‘ì¹˜: 5%)
**Context**: `kubectl config use-context security-cluster`

Implement comprehensive security measures including RBAC, Pod Security Standards, and network policies.

**Tasks:**
1. Create custom RBAC roles and bindings
2. Implement Pod Security Standards
3. Configure service accounts with minimal privileges
4. Set up admission controllers and policies

```yaml
# Custom ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-manager
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get", "list"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "create", "update", "patch"]

---
# ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: pod-manager-sa
  namespace: secure-apps

---
# ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pod-manager-binding
subjects:
- kind: ServiceAccount
  name: pod-manager-sa
  namespace: secure-apps
roleRef:
  kind: ClusterRole
  name: pod-manager
  apiGroup: rbac.authorization.k8s.io

---
# Pod Security Standards
apiVersion: v1
kind: Namespace
metadata:
  name: secure-apps
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/enforce-version: latest
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/audit-version: latest
    pod-security.kubernetes.io/warn: restricted
    pod-security.kubernetes.io/warn-version: latest

---
# Secure Pod
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
  namespace: secure-apps
spec:
  serviceAccountName: pod-manager-sa
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: app
    image: nginx:1.21
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      runAsNonRoot: true
      runAsUser: 1000
      capabilities:
        drop:
        - ALL
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
    volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: var-cache
      mountPath: /var/cache/nginx
    - name: var-run
      mountPath: /var/run
  volumes:
  - name: tmp
    emptyDir: {}
  - name: var-cache
    emptyDir: {}
  - name: var-run
    emptyDir: {}

---
# Network Policy for secure communication
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: secure-app-netpol
  namespace: secure-apps
spec:
  podSelector:
    matchLabels:
      app: secure-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend
    - podSelector:
        matchLabels:
          role: client
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432
  - to: []  # DNS
    ports:
    - protocol: UDP
      port: 53
```

```bash
# RBAC í…ŒìŠ¤íŠ¸
kubectl auth can-i create pods --as=system:serviceaccount:secure-apps:pod-manager-sa
kubectl auth can-i delete nodes --as=system:serviceaccount:secure-apps:pod-manager-sa

# Pod Security Standards í…ŒìŠ¤íŠ¸
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: insecure-pod
  namespace: secure-apps
spec:
  containers:
  - name: app
    image: nginx
    securityContext:
      privileged: true  # This should be rejected
EOF

# ë³´ì•ˆ ìŠ¤ìº”
kubectl get pods -n secure-apps -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.securityContext}{"\n"}{end}'
```

---

## ğŸ“ ì‹œí—˜ ì „ëµ ë° íŒ

### ğŸ¯ ì‹œê°„ ê´€ë¦¬ ì „ëµ
- **ì´ ì‹œí—˜ ì‹œê°„**: 2ì‹œê°„ (120ë¶„)
- **ë¬¸ì œë³„ ê¶Œì¥ ì‹œê°„**:
  - 5% ë¬¸ì œ: 6ë¶„
  - 10% ë¬¸ì œ: 12ë¶„
  - 15% ë¬¸ì œ: 18ë¶„
  - 20% ë¬¸ì œ: 24ë¶„
  - 25% ë¬¸ì œ: 30ë¶„

### ğŸ”§ í•„ìˆ˜ ëª…ë ¹ì–´ ë‹¨ì¶•í‚¤
```bash
# ê¸°ë³¸ ì„¤ì •
alias k=kubectl
export do="--dry-run=client -o yaml"
export now="--force --grace-period 0"

# ìì£¼ ì‚¬ìš©í•˜ëŠ” ëª…ë ¹ì–´
k get po -A -o wide
k describe po <pod-name>
k logs <pod-name> -f --previous
k edit <resource> <name>
k delete po <pod-name> $now
```

### ğŸ“š ë¬¸ì œ í•´ê²° ìˆœì„œ
1. **ë¬¸ì œ ìš”êµ¬ì‚¬í•­ ì •í™•íˆ íŒŒì•…**
2. **Context ë° Namespace í™•ì¸**
3. **ê¸°ì¡´ ë¦¬ì†ŒìŠ¤ ìƒíƒœ ì ê²€**
4. **ë‹¨ê³„ë³„ í•´ê²° ë° ê²€ì¦**
5. **ìµœì¢… ë™ì‘ í™•ì¸**

### ğŸš¨ Troubleshooting ì²´í¬ë¦¬ìŠ¤íŠ¸
```bash
# Pod ë¬¸ì œ
kubectl get pods -A
kubectl describe pod <pod-name>
kubectl logs <pod-name> --previous
kubectl get events --sort-by='.lastTimestamp'

# Node ë¬¸ì œ
kubectl get nodes
kubectl describe node <node-name>
kubectl top nodes

# Network ë¬¸ì œ
kubectl get svc,ep
kubectl get networkpolicy -A
kubectl exec -it <pod> -- nslookup <service>

# Storage ë¬¸ì œ
kubectl get pv,pvc
kubectl describe pvc <pvc-name>
kubectl get storageclass
```

### ğŸ“– ì£¼ìš” ì°¸ê³  ë¬¸ì„œ
- **Kubernetes ê³µì‹ ë¬¸ì„œ**: https://kubernetes.io/docs/
- **kubectl ì¹˜íŠ¸ì‹œíŠ¸**: https://kubernetes.io/docs/reference/kubectl/cheatsheet/
- **Troubleshooting**: https://kubernetes.io/docs/tasks/debug/
- **ë„¤íŠ¸ì›Œí‚¹**: https://kubernetes.io/docs/concepts/services-networking/
- **ë³´ì•ˆ**: https://kubernetes.io/docs/concepts/security/

---

**ì´ ë¬¸ì œì§‘ìœ¼ë¡œ ì•½ì  ì˜ì—­ì„ ì§‘ì¤‘ ê³µëµí•˜ì—¬ CKA ì‹œí—˜ì— í•©ê²©í•˜ì„¸ìš”! ğŸš€**

**Good Luck! ğŸ’ª**