# 1. HPA 2025

Question No: 1

Create a Horizontal Pod Scaler (HPA) named apache-server in the auto-scale namespace. This HPA must target existing deployment called apache-server in the auto-scale namespace.

Set the HPA to aim for 50% CPU usage per pod. Configure it to have at least 1 pod and at max 4 pod.

Also set the downscale stabilization window to 30 seconds.

- [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [https://kubernetes.io/docs/reference/kubectl/generated/kubectl_autoscale/](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_autoscale/)

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° deployment ìƒì„±
kubectl create namespace auto-scale
kubectl create deployment apache-server --image=nginx:1.21 -n auto-scale

# ë¦¬ì†ŒìŠ¤ ìš”ì²­ ì„¤ì • (HPA ë™ì‘ì„ ìœ„í•´ í•„ìš”)
kubectl patch deployment apache-server -n auto-scale -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","resources":{"requests":{"cpu":"100m","memory":"128Mi"}}}]}}}}'

# Metrics Server ì„¤ì¹˜ (HPA ë™ì‘ì„ ìœ„í•´ í•„ìš”)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl patch deployment metrics-server -n kube-system --type='json' -p='[{"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"}]'


echo "HPA í…ŒìŠ¤íŠ¸ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# Declarative
vi hpa-2025.yaml
kubectl apply -f hpa-2025.yaml
kubectl get hpa -n auto-scale

# Imperative
kubectl autoscale deployment apache-server -n auto-scale --min=1 --max=4 --cpu-percent=50
kubectl edit hpa apache-server -n auto-scale
```

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: apache-server
  namespace: auto-scale
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: apache-server
  minReplicas: 1
  maxReplicas: 4
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 30
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```


# 2. Deploy & NodePort 2025
Question No: 2
Weightage: 10%

Reconfigure the existing deployment front-end in namespace spline-reticulator to expose port 80/tcp of the existing container nginx.

Create a new service named front-end-svc exposing the container port 80/tcp.

Configure the new service to also expose the individual pod via a NodePort.

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë° deployment ìƒì„±
kubectl create namespace spline-reticulator
kubectl create deployment front-end --image=nginx:1.21 -n spline-reticulator

echo "Deployment & NodePort í…ŒìŠ¤íŠ¸ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# Deployment í¸ì§‘í•˜ì—¬ containerPort ì¶”ê°€
kubectl patch deployment front-end -n spline-reticulator -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","ports":[{"containerPort":80,"protocol":"TCP"}]}]}}}}'

# ì„œë¹„ìŠ¤ ìƒì„±
kubectl expose deployment front-end --name=front-end-svc --port=80 --target-port=80 -n spline-reticulator

# NodePortë¡œ ë³€ê²½
kubectl patch service front-end-svc -n spline-reticulator -p '{"spec": {"type": "NodePort"}}'

# ê²°ê³¼ í™•ì¸
kubectl get service front-end-svc -n spline-reticulator
kubectl get endpoints front-end-svc -n spline-reticulator
```

```yaml
spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        **ports:
        - containerPort: 80
          protocol: TCP**
```

# 3. Patch Deploy, Add SideCar, Volume Mount 2025
Update the existing deployment synergy-leverager, adding a co-located container named sidecar using the busybox:stable image to the existing pod.

The new co-located container has to run the following command:

```bash
/bin/sh -c "tail -n+1 -f /var/log/synergy-leverager.log"
Use a volume mounted at /var/log to make the log file synergy-leverager.log available to the co-located container.
```

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
# ê¸°ë³¸ deployment ìƒì„±
kubectl create deployment synergy-leverager --image=nginx:1.21

echo "Sidecar í…ŒìŠ¤íŠ¸ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# ê¸°ì¡´ deployment ì‚­ì œ í›„ sidecarê°€ í¬í•¨ëœ ìƒˆ deployment ìƒì„±
kubectl delete deployment synergy-leverager

# YAML íŒŒì¼ ìƒì„±
kubectl create deployment synergy-leverager --image=nginx --dry-run=client -o yaml > synergy-leverager.yaml
vi synergy-leverager.yaml
kubectl apply -f synergy-leverager.yaml
kubectl rollout restart deployment synergy-leverager
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: synergy-leverager
  name: synergy-leverager
spec:
  replicas: 1
  selector:
    matchLabels:
      app: synergy-leverager
  template:
    metadata:
      labels:
        app: synergy-leverager
    spec:
      containers:
        - image: nginx
          name: nginx
          command: ['sh', '-c', 'while true; do echo "logging" >> /var/log/synergy-leverager.log; sleep 1; done']
          volumeMounts:
            - name: data
              mountPath: /var/log
        - name: sidecar
          image: busybox:stable
          command: ['/bin/sh', '-c', 'touch /var/log/synergy-leverager.log; tail -n+1 -f /var/log/synergy-leverager.log']
          volumeMounts:
            - name: data
              mountPath: /var/log
      volumes:
        - name: data
          emptyDir: {}
```

# 4. JSONPATH ì¿¼ë¦¬

# Q9

Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file
all-nodes-os-info.txt at root location.

Note: The osImage are under the nodeInfo section under status of each node.

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
# í…ŒìŠ¤íŠ¸ìš© Pod ìƒì„± (JSONPath ì—°ìŠµìš©)
kubectl run test-pod1 --image=nginx:1.21
kubectl run test-pod2 --image=busybox:1.35 -- sleep 3600
kubectl run test-pod3 --image=redis:6-alpine

echo "JSONPath í…ŒìŠ¤íŠ¸ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# ë…¸ë“œ OS ì •ë³´ ì¶”ì¶œ
kubectl get nodes -o=jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.nodeInfo.osImage}{"\n"}{end}' \
> /root/all-nodes-os-info.txt

# ê²°ê³¼ í™•ì¸
cat /root/all-nodes-os-info.txt

# ì¶”ê°€ JSONPath ì˜ˆì œë“¤
kubectl get pods -o jsonpath='{.items[*].status.podIP}' > /tmp/pod-ips.txt
kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.phase}{"\n"}{end}' > /tmp/pod-status.txt
```

# 5. PriorityClass 2025
Perform the following tasks:

Create a new PriorityClass named high-priority for user workloads with a value that is one less than the highest existing user-defined priority class value.

Patch the existing Deployment busybox-logger running in the priority namespace to use the high-priority priority class.
Ensure that the busybox-logger Deployment rolls out successfully with the new priority class set.

Note: It is expected that pods from other Deployments running in the priority namespace are evicted.

- [https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_priorityclass/](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_priorityclass/)
- [https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/)
- [https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority)

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace priority

# ê¸°ì¡´ ìš°ì„ ìˆœìœ„ í´ë˜ìŠ¤ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
kubectl create priorityclass existing-priority --value=1000 --description="Existing user priority class"

# í…ŒìŠ¤íŠ¸ìš© deployment ìƒì„±
kubectl create deployment busybox-logger --image=busybox:1.35 --namespace=priority --replicas=2
kubectl patch deployment busybox-logger -n priority -p '{"spec":{"template":{"spec":{"containers":[{"name":"busybox","command":["sh","-c","while true; do echo Logging...; sleep 5; done"]}]}}}}'

# ë‹¤ë¥¸ deploymentë„ ìƒì„± (eviction í…ŒìŠ¤íŠ¸ìš©)
kubectl create deployment other-app --image=nginx:1.21 --namespace=priority --replicas=1

echo "PriorityClass í…ŒìŠ¤íŠ¸ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# ê¸°ì¡´ ìš°ì„ ìˆœìœ„ í´ë˜ìŠ¤ í™•ì¸
kubectl get priorityclass

# ìƒˆ PriorityClass ìƒì„± (ê¸°ì¡´ ê°’ë³´ë‹¤ 1 ì‘ê²Œ)
kubectl create priorityclass high-priority --value=999 --description="High priority class for critical workloads"

# Deploymentì— ìš°ì„ ìˆœìœ„ í´ë˜ìŠ¤ ì ìš©
kubectl patch deployment busybox-logger -p '{"spec":{"template":{"spec": {"priorityClassName": "high-priority"}}}}' --namespace=priority

# ë¡¤ì•„ì›ƒ ìƒíƒœ í™•ì¸
kubectl rollout status deployment/busybox-logger -n priority

# ê²°ê³¼ í™•ì¸
kubectl get deployment -n priority busybox-logger -o yaml | grep -A 2 priorityClassName
kubectl get pods -n priority -o custom-columns=NAME:.metadata.name,PRIORITY:.spec.priorityClassName,STATUS:.status.phase
```

# 6. StorageClass 2025

Create a StorageClass named local-path and set it as the default StorageClass.

Requirements:
- Provisioner: rancher.io/local-path
- Volume Binding Mode: WaitForFirstConsumer
- Set as default StorageClass

- [https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/)

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
# ê¸°ì¡´ StorageClass í™•ì¸
kubectl get storageclass

echo "StorageClass í…ŒìŠ¤íŠ¸ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# ê¸°ì¡´ ê¸°ë³¸ StorageClass ì œê±° (ìˆë‹¤ë©´)
kubectl patch storageclass standard -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}' || true

# YAML íŒŒì¼ ìƒì„±
vi local-path.yaml
kubectl apply -f local-path.yaml

# StorageClass í™•ì¸
kubectl get storageclass local-path -o yaml
kubectl describe storageclass local-path
```

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
```

# 7. ArgoCD 2025

Weightage: 7%

Install ArgoCD in the cluster by performing the following tasks:
Add the official Argo CD Helm repository with the name argo.

Generate a template of the ArgoCD Helm Chart version 7.7.3 for the argocd namespace and save it to ~/argo-helm.yaml.
Configure the chart to not install CRDs.

Note - The Argo CD CRDs have already been pre-installed in the cluster.

- [https://helm.sh/docs](https://helm.sh/docs)

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
# ArgoCD CRD ì‚¬ì „ ì„¤ì¹˜ (ì‹œë®¬ë ˆì´ì…˜)
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/crds/application-crd.yaml
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/crds/applicationset-crd.yaml

# CRD ì„¤ì¹˜ í™•ì¸
kubectl get crd | grep argoproj

echo "ArgoCD ì„¤ì¹˜ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# Helm repository ì¶”ê°€
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

# Repository í™•ì¸
helm repo list | grep argo

# Chart ë²„ì „ í™•ì¸
helm search repo argo/argo-cd --versions | head -5

# í…œí”Œë¦¿ ìƒì„± (CRD ì„¤ì¹˜ ë¹„í™œì„±í™”)
helm template argocd argo/argo-cd --version 7.7.3 -n argocd \
  --set crds.install=false > ~/argo-helm.yaml

# ìƒì„±ëœ í…œí”Œë¦¿ í™•ì¸
ls -la ~/argo-helm.yaml
head -20 ~/argo-helm.yaml

# CRDê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
grep -i "CustomResourceDefinition" ~/argo-helm.yaml || echo "No CRDs found (expected)"
```

# 8. Deploy Recovery & Data Consistency
A MariaDB Deployment in the mariadb namespace has been deleted by mistake. Your task is to restore the Deployment ensuring data persistence. Follow the steps:

Create a PersistentVolumeClaim (PVC) named mariadb in the mariadb namespace with the following specifications:
Storage 250Mi

Note - You must use the existing retained PersistentVolume (PV). There is only one existing PersistentVolume.
Edit the MariaDB Deployment file located at ~/mariadb-deployment.yaml to use the PVC you created in the previous step.
Apply the updated Deployment file to the cluster.

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
kubectl create namespace mariadb

# ê¸°ì¡´ PV ì‹œë®¬ë ˆì´ì…˜ (retained PV ìƒì„±)
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mariadb-pv
spec:
  capacity:
    storage: 300Mi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: standard
  hostPath:
    path: /tmp/mariadb-data
EOF

# MariaDB Deployment íŒŒì¼ ìƒì„± (~/mariadb-deployment.yaml)
cat <<EOF > ~/mariadb-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mariadb
  namespace: mariadb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mariadb
  template:
    metadata:
      labels:
        app: mariadb
    spec:
      containers:
        - name: mariadb
          image: mariadb:latest
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: "password"
          ports:
            - containerPort: 3306
          # volumeMounts ì„¹ì…˜ì„ ì¶”ê°€í•´ì•¼ í•¨
EOF

echo "MariaDB ë³µêµ¬ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# PVC ìƒì„±
vi mariadb-pvc.yaml
kubectl apply -f mariadb-pvc.yaml

# PV, PVC ìƒíƒœ í™•ì¸
kubectl get pv,pvc -n mariadb

# MariaDB Deployment íŒŒì¼ í¸ì§‘ (PVC ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •)
vi ~/mariadb-deployment.yaml

# Deployment ì ìš©
kubectl apply -f ~/mariadb-deployment.yaml

# ë°°í¬ ìƒíƒœ í™•ì¸
kubectl get deployment -n mariadb
kubectl get pods -n mariadb
```

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mariadb-pv
spec:
  capacity:
    storage: 300Mi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: standard
  hostPath:
    path: /tmp/mariadb-data
```

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mariadb
  namespace: mariadb
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 250Mi
  storageClassName: standard
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mariadb
  namespace: mariadb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mariadb
  template:
    metadata:
      labels:
        app: mariadb
    spec:
      containers:
        - name: mariadb
          image: mariadb:latest
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: "password"
          ports:
            - containerPort: 3306
          volumeMounts:
            - name: mariadb-storage
              mountPath: /var/lib/mysql
      volumes:
        - name: mariadb-storage
          persistentVolumeClaim:
            claimName: mariadb

```

# 7. ì‚¬ìš©ì ë° CSR ìƒì„± - ì‚¬ìš©ìì—ê²Œ ê¶Œí•œ ë¶€ì—¬

Create a new user â€œajeetâ€. Grant him access to the cluster.
User â€œajeetâ€ should have permission to create, list, get, update and delete pods.
The private key exists at location:

/root/ajeet/.key and csr at /root/ajeet.csr

- [https://kubernetes.io/docs/tasks/tls/certificate-issue-client-csr/](https://kubernetes.io/docs/tasks/tls/certificate-issue-client-csr/)
- [https://kubernetes.io/ko/docs/reference/kubectl/cheatsheet/](https://kubernetes.io/ko/docs/reference/kubectl/cheatsheet/)

```bash
openssl genrsa -out ajeet.key 3072
openssl req -new -key ajeet.key -out ajeet.csr -subj "/CN=ajeet"
cat ajeet.csr | base64 | tr -d "\n"

vi csr.yaml
kubectl apply -f csr.yaml
kubectl get csr
kubectl certificate approve ajeet

kubectl create clusterrole ajeet-role --verb=create,list,get,update,delete --resource=pods
kubectl create clusterrolebinding ajeet-role-binding --clusterrole=ajeet-role  --user=ajeet

kubectl get csr ajeet -o jsonpath='{.status.certificate}'| base64 -d > ajeet.crt
kubectl config set-credentials ajeet --client-key=ajeet.key --client-certificate=ajeet.crt --embed-certs=true
kubectl config set-context ajeet --cluster=kubernetes --user=ajeet
kubectl --context ajeet auth whoami

kubectl config use-context ajeet
kubectl config view --minify
kubectl --context=ajeet auth can-i get pods
kubectl --context=ajeet get pods
```

```yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ajeet
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJRFZUQ0NBYjBDQVFBd0VERU9NQXdHQTFVRUF3d0ZZV3BsWlhRd2dnR2lNQTBHQ1NxR1NJYjNEUUVCQVFVQQpBNElCandBd2dnR0tBb0lCZ1FERzRGS2d1TTVjdG9UMUcwRjIxdlQzWUx4SkZXL0ZhYy9sdHFZZlFkQnk2VURaCkdsWTdNSkpyaWhmc2htanQ3U2sxS1o3YWVlRU92SGFGUWZnVWpKM3ZhNy96bGFLUDAzTXd1d1lsZ0JhY1lTQUoKU2phcS81elFwaEZnVmdNSkZ4M2ZBQThXVmtMbVBkbE1TU1k0VUlnN1B3ZVgrVmhUM1RHL3RxVGlyUUZKZk9oTApEcDR2d1IyaGg1OXU2ZkFpejBQYjhiQW5Scmtod0UycDd4TDAyUDEwQ3NVNkx4cW5mQ0xodk04emNuQy8xc3NmClJ3Z1hVKzBCR0FiUXM5dGtrVE9UcXBUUHZCNUI0aDhDWHhpbnFSYjY0Y2V6cGZLWENnK0pLM3pycjVoUW56U0wKaU9aV1dSRGE0c1FtUDZ5a1ZLajNVSzhlclNFbDRWclV1TWFlOW1jN1U1Wk5aekU1azVyNVZNYXFRTmVoMGkwcQpJYktFM1NTMUdLN3k2bUE5WXNYeFVXamo1MDZ5TDY0b2pLaldMNkhVazFweVFCOWRWMGg2TVc0dUpjQStkUXFFCi9TQk9STmtJUmYyM1dyWkF6YlZsWGE2RWNFbmQ5Ym1ZL3JmRlNJU3dnTDRBanFoaHNScXVmWXA5SG5ZTlZ5WTMKRDZQc012cHowZHpWSU5PV2lDMENBd0VBQWFBQU1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQmdRQTE0Z3JDVzhBUApWTHI2djRNdStTRUhXbWFLTDZXSnFCeU1MVzErZFg2YmNIQldpblllV3Q2V1ZiODllS1IybHJscVRPcnpFZEJtCmg2ck9mODJhRmdUUjZra3ptS0c3aTNVT0M4OUgzWFFpZUNUYnU2UzhLUC9iSHU3dXF1T3VhbkNBN3ZlOFpnUnEKWUNRWXFiRmtCWnZidTYzcSt2d0VtTVpRaVAxWmg0bWx0cUZhcm5LWWY3L2ZSV21rNnYyQkRRMm9NYk5oNnNqNQpSd1FXMis3emFnNnFnZ2JmL250WW4xU0NaQWgwZEYxQ0c2T3A4bG84MUdCaEtqZERsck9QYjlmYU13UHl0eEp2CkJYbTMyOThFenVEVHFiQWk0U0R6dHEvSjVOODN6dzBHWVQ2MEd5d0ZVNmg3cm5BUmRPTWxSaFlSSVFxV0pHM0kKZkRkdGgxSEt4WXoyRDlxdURpeGJmdGs5dnJtRjdxWTl4RUhRZThsbmErT3BWYkhiM1g3ZEs4K3I2UUp0dHVUTQo3eDhGY1dWTmhFQ0RucXFJTVBOUzBYVENyeGNUMzFGenY5UHp1K1FibFRwM0EzZU9iandqaVlIZk1aUHlNeVFMCmVVOHZMM3M2Zmd5SzVUWDkrYUJGcWdibHFGN0k4YmZSeEhwK3VEdXlkMnRWWnJOZUFWSXpGdGs9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
```

# 8. Gateway & HTTPRoute 2025
Weightage: 12%

Migrate an existing web application from Ingress to Gateway API.
You must maintain HTTPS access.

First, create a Gateway named web-gateway with hostname gateway.web.k8s.local that maintains the existing TLS and listener configuration from the existing ingress resource named web.

Next, create an HTTPRoute named web-route with hostname gateway.web.k8s.local that maintains the existing routing rules from the current Ingress resource named web.

Note - A GatewayClass named nginx is installed in the cluster.

- [https://kubernetes.io/docs/concepts/services-networking/gateway/](https://kubernetes.io/docs/concepts/services-networking/gateway/)
- [https://kubernetes.io/docs/concepts/services-networking/ingress/#tls](https://kubernetes.io/docs/concepts/services-networking/ingress/#tls)

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
# Gateway API CRD ì„¤ì¹˜
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml

# í…ŒìŠ¤íŠ¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ë° ì„œë¹„ìŠ¤ ìƒì„±
kubectl create deployment web-app --image=nginx:1.21 --replicas=1
kubectl expose deployment web-app --name=web-service --port=80

# TLS ì¸ì¦ì„œ ìƒì„±
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout /tmp/tls.key -out /tmp/tls.crt \
  -subj "/CN=gateway.web.k8s.local"

kubectl create secret tls web-tls --cert=/tmp/tls.crt --key=/tmp/tls.key

# ê¸°ì¡´ Ingress ìƒì„± (ë§ˆì´ê·¸ë ˆì´ì…˜ ëŒ€ìƒ)
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web
spec:
  tls:
  - hosts:
    - gateway.web.k8s.local
    secretName: web-tls
  rules:
  - host: gateway.web.k8s.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
EOF

echo "Gateway API ë§ˆì´ê·¸ë ˆì´ì…˜ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# 1. GatewayClass ìƒì„± (nginx ì»¨íŠ¸ë¡¤ëŸ¬ìš©)
cat <<EOF | kubectl apply -f -
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: nginx
spec:
  controllerName: "nginx.org/gateway-controller"
EOF

# 2. Gateway ìƒì„±
vi web-gateway.yaml
kubectl apply -f web-gateway.yaml

# 3. HTTPRoute ìƒì„±
vi web-route.yaml
kubectl apply -f web-route.yaml

# 4. Gateway API ë¦¬ì†ŒìŠ¤ í™•ì¸
kubectl get gatewayclass
kubectl get gateway
kubectl get httproute

# 5. Gateway ìƒíƒœ í™•ì¸
kubectl describe gateway web-gateway
kubectl describe httproute web-route
```

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: nginx
spec:
  controllerName: "nginx.org/gateway-controller"
```

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-gateway
spec:
  tls:
  - hosts:
      - gateway.web.k8s.local
    secretName: web-tls
  rules:
  - host: gateway.web.k8s.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

- [https://kubernetes.io/docs/tasks/administer-cluster/certificates/](https://kubernetes.io/docs/tasks/administer-cluster/certificates/)

```bash
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls.key -out /tmp/tls.crt -subj "/CN=gateway.web.k8s.local"
kubectl create secret tls web-tls --cert=/tmp/tls.crt --key=/tmp/tls.key

kubectl create deploy web-app --image=nginx --replicas=1
kubectl expose deploy web-app --name=web-service --port 80

vi web-ingress.yaml
kubectl apply -f web-ingress.yaml

vi web-route.yaml
kubectl apply -f web-route.yaml

```

- [https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/)

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: web-gateway
spec:
  gatewayClassName: nginx
  listeners:
  - name: https
    protocol: HTTPS
    port: 443
    hostname: gateway.web.k8s.local
    tls:
      mode: Terminate
      certificateRefs:
      - kind: Secret
        name: web-tls
```

- [https://kubernetes.io/docs/concepts/services-networking/gateway/#api-kind-httproute](https://kubernetes.io/docs/concepts/services-networking/gateway/#api-kind-httproute)

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: web-route
spec:
  parentRefs:
  - name: web-gateway
  hostnames:
  - "gateway.web.k8s.local"
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /
    backendRefs:
    - name: web-service
      port: 80
```

# 10. ì„œë¹„ìŠ¤ DNS í™•ì¸

# Q15

Create a nginx pod called dns-resolver using image nginx, expose it internally with a service called dns-resolver-service.

Check if pod and service name are resolvable from within the cluster.

Use the image: busybox:1.28 for dns lookup.

Save the result in /root/nginx.svc

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
echo "DNS í™•ì¸ í…ŒìŠ¤íŠ¸ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# 1. nginx Pod ìƒì„±
kubectl run dns-resolver --image=nginx

# 2. ì„œë¹„ìŠ¤ë¡œ ë…¸ì¶œ
kubectl expose pod dns-resolver --name=dns-resolver-service --port=80

# 3. DNS ì¡°íšŒ í…ŒìŠ¤íŠ¸ ë° ê²°ê³¼ ì €ì¥
kubectl run dns-lookup --image=busybox:1.28 --restart=Never -it --rm -- \
nslookup dns-resolver-service > /root/nginx.svc

# 4. ê²°ê³¼ í™•ì¸
cat /root/nginx.svc

# 5. ì¶”ê°€ DNS í…ŒìŠ¤íŠ¸
kubectl run dns-test --image=busybox:1.28 --restart=Never -it --rm -- sh -c "
echo 'Testing service DNS resolution:'
nslookup dns-resolver-service
echo 'Testing pod DNS resolution:'
nslookup dns-resolver-service.default.svc.cluster.local
"
```

# 9. CNI (Container Network Interface) 2025

Weightage: 10%

Install and set up a Container Network Interface (CNI) that meets these requirements:
Pick and install one of the CNI options:

The CNI you choose must satisfy following requirement:
<native network policy support>

Flannel version 0.26.1

Manifest - https://github.com/flanner-io/flanner/releases/download/v0.26.1/kube-flanner.yml

Calico version 3.28.2

Manifest - https://raw.githubusercontent.com/projectcalico/calico/v3.28.2/manifests/tigera-operator.yaml

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
# ê¸°ì¡´ CNI ì œê±° (ìˆë‹¤ë©´)
kubectl delete -f https://github.com/flannel-io/flannel/releases/download/v0.26.1/kube-flannel.yml --ignore-not-found

# ë…¸ë“œ ìƒíƒœ í™•ì¸
kubectl get nodes

echo "CNI ì„¤ì¹˜ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# 1. ìš”êµ¬ì‚¬í•­ ë¶„ì„
# Network Policy ì§€ì›ì´ í•„ìš”í•˜ë¯€ë¡œ Calico ì„ íƒ (Flannelì€ ë¯¸ì§€ì›)

# 2. Calico ì„¤ì¹˜ (create ì‚¬ìš© - ì¤‘ìš”!)
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.2/manifests/tigera-operator.yaml

# 3. Calico ì„¤ì¹˜ ìƒíƒœ í™•ì¸
kubectl get pods -n calico-system
kubectl get pods -n tigera-operator

# 4. ë…¸ë“œ ìƒíƒœ í™•ì¸ (Readyê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸°)
kubectl get nodes

# 5. Network Policy í…ŒìŠ¤íŠ¸
kubectl run test-pod1 --image=nginx:1.21 --labels="app=test1"
kubectl run test-pod2 --image=nginx:1.21 --labels="app=test2"

# 6. ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
kubectl exec test-pod1 -- ping -c 2 $(kubectl get pod test-pod2 -o jsonpath='{.status.podIP}')

# 7. Network Policy ìƒì„± (í…ŒìŠ¤íŠ¸)
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-netpol
spec:
  podSelector:
    matchLabels:
      app: test2
  policyTypes:
  - Ingress
  ingress: []  # ëª¨ë“  íŠ¸ë˜í”½ ì°¨ë‹¨
EOF

# 8. ì°¨ë‹¨ëœ ì—°ê²° í…ŒìŠ¤íŠ¸ (ì‹¤íŒ¨í•´ì•¼ í•¨)
kubectl exec test-pod1 -- ping -c 2 $(kubectl get pod test-pod2 -o jsonpath='{.status.podIP}') || echo "Traffic blocked by NetworkPolicy (expected)"

# 9. ì •ë¦¬
kubectl delete networkpolicy test-netpol
kubectl delete pod test-pod1 test-pod2
```

# 10. WordPress Application Some Pods are Nou Up and running 2025
Weightage: 9%

You manage a WordPress application. Some pods are not up and running.
Adjust all Pod resource requests as follows:

Divide node resources evenly across all 3 pods.

Give each Pod a fair share of CPU and memory.

Add enough overhead to keep the node stable.

Note - Use exact same requests for both containers and init containers.
Scale down the WordPress deployment to 0 replicas while updating the resource requests.

After updates, confirm:

WordPress keeps 3 replicas.

All Pods are running and ready.

```bash
vi wordpress-deploy.yaml
kubectl apply -f wordpress-deploy.yaml
kubectl scale deploy wordpress --replicas=0

kubectl describe node controlplane
kubectl patch deployment wordpress -p '{"spec": {"template": {"spec": {"initContainers": [{"name": "init-wordpress", "resources": {"requests": {"cpu": "100m", "memory": "100Mi"}}}]}}}}'
kubectl patch deployment wordpress -p '{"spec": {"template": {"spec": {"containers": [{"name": "wordpress", "resources": {"requests": {"cpu": "100m", "memory": "100Mi"}}}]}}}}'
kubectl scale deploy wordpress --replicas=3
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
spec:
  replicas: 3
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      initContainers:
      - name: init-wordpress
        image: busybox
        command: ["sh", "-c", "echo Initializing && sleep 5"]
      containers:
      - name: wordpress
        image: wordpress:latest
        ports:
        - containerPort: 80

```

# 10. ì£¼ì–´ì§„ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ NetworkPolicy ë§Œë“¤ê¸°

# Q18
Weightage: 11%

Create a Network Policy named "appychip" in default namespace
There should be two types, ingress and egress.

The ingress should block traffic from an IP range of your choice except some other IP range.
Should also have namespace and pod selector.
Ports for ingress policy should be 6379

For Egress, it should allow traffic to an IP range of your choice on 5978 port.

- [https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/)

```bash
vi q18.yaml
kubectl apply -f q18.yaml
```

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: appychip
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978

```

# 11. Debian package Installation 2025

Weightage: 10%

Complete these tasks to prepare the system for Kubernetes:
Set up cri-dockerd:

Install the Debian package:
~/cri-dockerd_0.3.9.3-0.ubuntu-focal_amd64.deb

Start the cri-dockerd service.

Enable and start the systemd service for cri-dockerd.

Configure these system parameters:

Set net.bridge.bridge-nf-call-iptables to 1.

Set net.ipv6.conf.all.forwarding to 1.

Set net.ipv4.ip_forward to 1.

- [https://kubernetes.io/docs/setup/production-environment/container-runtimes/](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
# cri-dockerd íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ (ì‹œë®¬ë ˆì´ì…˜)
curl -LO https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.9/cri-dockerd_0.3.9.3-0.ubuntu-focal_amd64.deb

echo "ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# 1. ì‹œìŠ¤í…œ ë§¤ê°œë³€ìˆ˜ êµ¬ì„±
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.ipv6.conf.all.forwarding = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

# 2. ì¬ë¶€íŒ… ì—†ì´ ë§¤ê°œë³€ìˆ˜ ì ìš©
sudo sysctl --system

# 3. ë§¤ê°œë³€ìˆ˜ í™•ì¸
sysctl net.ipv4.ip_forward
sysctl net.ipv6.conf.all.forwarding
sysctl net.bridge.bridge-nf-call-iptables

# 4. cri-dockerd íŒ¨í‚¤ì§€ ì„¤ì¹˜
sudo apt update
sudo dpkg -i ~/cri-dockerd_0.3.9.3-0.ubuntu-focal_amd64.deb

# 5. ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
sudo systemctl status cri-docker.service
sudo systemctl status cri-docker.socket

# 6. ì„œë¹„ìŠ¤ ì‹œì‘ ë° í™œì„±í™”
sudo systemctl start cri-docker.service
sudo systemctl enable cri-docker.service

# 7. ìµœì¢… ìƒíƒœ í™•ì¸
sudo systemctl status cri-docker.service
systemctl list-units | grep cri

echo "ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ"
```

# 11. Kubernetes Context - 1

# Q19
Weightage: 7%

You have access to multiple clusters from your main terminal through kubectl contexts.
Write all those context names into
/opt/course/1/contexts

Next write a command to display the current context into
/opt/course/1/context_default_kubectl.sh,
the command should use kubectl.

Finally write a second command doing the same thing into
/opt/course/1/context_default_no_kubectl.sh,
but without the use of kubectl.

- [https://kubernetes.io/ko/docs/concepts/configuration/organize-cluster-access-kubeconfig/](https://kubernetes.io/ko/docs/concepts/configuration/organize-cluster-access-kubeconfig/)

```bash
mkdir -p /opt/course/1

kubectl config get-contexts -o name > /opt/course/1/contexts
cat /opt/course/1/contexts

echo kubectl config current-context > /opt/course/1/context_default_kubectl.sh
chmod +x /opt/course/1/context_default_kubectl.sh
cat /opt/course/1/context_default_kubectl.sh
bash /opt/course/1/context_default_kubectl.sh

cat ~/.kube/config | grep current
echo 'cat ~/.kube/config | grep current' > /opt/course/1/context_default_no_kubectl.sh
bash /opt/course/1/context_default_no_kubectl.sh
```

# 12. ëª¨ë“  Custom CRD ë‚˜ì—´ ë° custom-crd.txt ì €ì¥
Weightage: 7%

List all custom crd from cert manager and store it in custom-crd.txt.

Get the subject field from the cert manager and store it in cert-manager-subject.txt

## ğŸ› ï¸ í™˜ê²½ ì„¤ì • (KillerCodaì—ì„œ ì‹¤í–‰)

```bash
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.crds.yaml
echo "CRD ê´€ë¦¬ í…ŒìŠ¤íŠ¸ í™˜ê²½ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤."
```

## âœ… ì†”ë£¨ì…˜

```bash
# 1. cert-manager CRD ì„¤ì¹˜
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.crds.yaml

# 2. cert-manager ê´€ë ¨ CRD ëª©ë¡ ì €ì¥
kubectl get crd | grep cert > custom-crd.txt

# 3. ê²°ê³¼ í™•ì¸
cat custom-crd.txt

# 4. certificates CRDì˜ subject í•„ë“œ ì •ë³´ ì¶”ì¶œ
kubectl get crd certificates.cert-manager.io -o jsonpath='{.spec.versions[*].schema.openAPIV3Schema.properties.spec.properties.subject}' \
> cert-manager-subject.txt

# 5. subject í•„ë“œ ì •ë³´ í™•ì¸
cat cert-manager-subject.txt

# 6. ì¶”ê°€ CRD ì •ë³´ í™•ì¸
kubectl get crd certificates.cert-manager.io -o yaml | grep -A 10 subject

echo "CRD ì •ë³´ ì¶”ì¶œ ì™„ë£Œ"
```

# 13. 2025 Network Policy YAML
There are two deployments. frontend and backend deployment.
frontend will be in frontend NS and backend will be in backend NS.
Apply the least permissive policy to have interaction between frontend and backend deployment.
Below are the 3 YAML file to apply network policy.
Choose either of them with least permission.

1 YAML - pod selector of all. type is ingress and pod selector with all
2 YAML - pod selector as well as namespace selector are present
3 YAML - pod selector, NS selector, POD CIDR


```bash
 # 2ë²ˆì§¸ ì •ì±…ì´ ê°€ì¥ ì ì ˆ. 1ë²ˆì€ ë¶€ì¡±í•˜ê³ , 2ë²ˆì€ ë³µì¡í•¨
 kubectl apply -f network-policy-2.yaml
```

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: open-backend-access
  namespace: backend
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from: []
      ports:
        - protocol: TCP
          port: 8080
```

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-to-backend
  namespace: backend
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: frontend
          podSelector:
            matchLabels:
              app: frontend
      ports:
        - protocol: TCP
          port: 8080

```

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-to-backend-with-cidr
  namespace: backend
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: frontend
          podSelector:
            matchLabels:
              app: frontend
      ports:
        - protocol: TCP
          port: 8080
    - from:
        - ipBlock:
            cidr: 192.168.1.0/24
      ports:
        - protocol: TCP
          port: 8080

```


# 14. 2025 - ConfigMap

Weightage: 10%

An nginx deployment configured via a ConfigMap containing the Nginx config file.
The task is to ensure that TLS 1.2 should not be supported or permitted.
And it should only work with TLS 1.3
Do the required steps and verify that it works with TLS 1.3

- [https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_secret_tls/](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_secret_tls/)
- [https://kubernetes.io/docs/concepts/configuration/secret/#use-case-dotfiles-in-a-secret-volume](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-dotfiles-in-a-secret-volume)
- [https://kubernetes.io/docs/concepts/configuration/secret/#use-case-dotfiles-in-a-secret-volume](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-dotfiles-in-a-secret-volume)

```bash
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt \
-subj "/CN=example.com"
kubectl create secret tls nginx-tls --cert=tls.crt --key=tls.key

vi nginx-cm.yaml
kubectl apply -f nginx-cm.yaml
kubectl create deploy nginx --image=nginx --replicas=1

vi nginx-deployment.yaml
kubectl apply -f nginx-deployment.yaml
kubectl expose deploy nginx --port=443 --target-port=443 --type=NodePort
kubectl get svc
curl -k https://localhost:31529

kubectl edit cm nginx-config
# ssl_protocols TLSv1.3; ìœ¼ë¡œ ìˆ˜ì •
curl --tlsv1.2 -k https://localhost:31529
# ConfigMap ìˆ˜ì •í–ˆë”ë¼ë„, ë°°í¬ë¥¼ ë‹¤ì‹œ í•´ì•¼ í•¨

kubectl scale deploy nginx --replicas=0
kubectl scale deploy nginx --replicas=1
# rolloutë§Œ í•´ë„ ë¨
kubectl rollout restart deploy nginx
curl --tlsv1.3 -k https://localhost:31529

```

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    events {}
    http {
        server {
            listen 443 ssl;
            server_name example.com;
            ssl_certificate /etc/nginx/certs/tls.crt;
            ssl_certificate_key /etc/nginx/certs/tls.key;
            ssl_protocols TLSv1.2 TLSv1.3; # Initial config supports both
            location / {
                root /usr/share/nginx/html;
                index index.html;
            }
        }
    }

```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 443
          volumeMounts:
            - name: config
              mountPath: /etc/nginx/nginx.conf
              subPath: nginx.conf
            - name: certs
              mountPath: /etc/nginx/certs
      volumes:
        - name: config
          configMap:
            name: nginx-config
        - name: certs
          secret:
            secretName: nginx-tls

```

# 15. Nginx Ingress

Weightage: 10%

Namespace: sound-repeater

Exposing Service echoserver-service on http://example.org/echo using Service port 8080

Expose the service echo-server service in the given namespace
Create an nginx ingress for above deployment and service for the host http://www.example.org/echo over port 80

```bash
kubectl create ns sound-repeater
kubectl create deploy echoserver --image=nginx -n sound-repeater
kubectl expose deploy echoserver --name=echoserver-service -n sound-repeater --port=8080 --target-port=8080 \
--type=NodePort

vi q15.yaml
k apply -f q15.yaml
curl -o /dev/null -s -w "%{http_code}\n" http://www.example.org/echo
kubectl describe ingress nginx-ingress -n sound-repeater
kubectl describe deploy echoserver -n sound-repeater

```

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
  namespace: sound-repeater
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx-example
  rules:
  - host: www.example.org
    http:
      paths:
      - pathType: Prefix
        path: "/echo"
        backend:
          service:
            name: echoserver-service
            port:
              number: 8080

```

# 16. Kubeadm

A kubeadm provisioned cluster was migrated to a new machine. Requires configuration changes to run successfully.

Task:
We need to fix a single-node cluster that got broken during machine migration.

Identify the broken cluster components and investigate what caused them to break.

The decommissioned cluster used an external etcd server.

Next, fix the configuration of all broken cluster components.

Ensure to restart all necessary services and components for changes to take effect.

Finally, ensure the cluster, single node and all pods are Ready.

```bash
kubectl get po
journalctl -u kubelet -f
ls /etc/kubernetes/manifests/

vi /etc/kubernetes/manifests/kube-apiserver

# --etcd-server=https://128.0.0.1:2379

sudo systemctl restart kubelet
sudo systemctl status kubelet

```