# 1. HPA 2025

Question No: 1

Create a Horizontal Pod Scaler (HPA) named apache-server in the auto-scale namespace. This HPA must target existing deployment called apache-server in the auto-scale namespace.

Set the HPA to aim for 50% CPU usage per pod. Configure it to have at least 1 pod and at max 4 pod.

Also set the downscale stabilization window to 30 seconds.

- [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [https://kubernetes.io/docs/reference/kubectl/generated/kubectl_autoscale/](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_autoscale/)

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 네임스페이스 및 deployment 생성
kubectl create namespace auto-scale
kubectl create deployment apache-server --image=nginx:1.21 -n auto-scale

# 리소스 요청 설정 (HPA 동작을 위해 필요)
kubectl patch deployment apache-server -n auto-scale -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","resources":{"requests":{"cpu":"100m","memory":"128Mi"}}}]}}}}'

# Metrics Server 설치 (HPA 동작을 위해 필요)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
kubectl patch deployment metrics-server -n kube-system --type='json' -p='[{"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"}]'


echo "HPA 테스트 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# Declarative
vi hpa-2025.yaml
kubectl apply -f hpa-2025.yaml
kubectl get hpa -n auto-scale

# Imperative
kubectl autoscale deployment apache-server -n auto-scale --min=1 --max=4 --cpu-percent=50
kubectl edit hpa apache-server -n auto-scale
```

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: apache-server
  namespace: auto-scale
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: apache-server
  minReplicas: 1
  maxReplicas: 4
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 30
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```


# 2. Deploy & NodePort 2025
Question No: 2
Weightage: 10%

Reconfigure the existing deployment front-end in namespace spline-reticulator to expose port 80/tcp of the existing container nginx.

Create a new service named front-end-svc exposing the container port 80/tcp.

Configure the new service to also expose the individual pod via a NodePort.

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 네임스페이스 및 deployment 생성
kubectl create namespace spline-reticulator
kubectl create deployment front-end --image=nginx:1.21 -n spline-reticulator

echo "Deployment & NodePort 테스트 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# Deployment 편집하여 containerPort 추가
kubectl patch deployment front-end -n spline-reticulator -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","ports":[{"containerPort":80,"protocol":"TCP"}]}]}}}}'

# 서비스 생성
kubectl expose deployment front-end --name=front-end-svc --port=80 --target-port=80 -n spline-reticulator

# NodePort로 변경
kubectl patch service front-end-svc -n spline-reticulator -p '{"spec": {"type": "NodePort"}}'

# 결과 확인
kubectl get service front-end-svc -n spline-reticulator
kubectl get endpoints front-end-svc -n spline-reticulator
```

```yaml
spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        **ports:
        - containerPort: 80
          protocol: TCP**
```

# 3. Patch Deploy, Add SideCar, Volume Mount 2025
Update the existing deployment synergy-leverager, adding a co-located container named sidecar using the busybox:stable image to the existing pod.

The new co-located container has to run the following command:

```bash
/bin/sh -c "tail -n+1 -f /var/log/synergy-leverager.log"
Use a volume mounted at /var/log to make the log file synergy-leverager.log available to the co-located container.
```

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 기본 deployment 생성
kubectl create deployment synergy-leverager --image=nginx:1.21

echo "Sidecar 테스트 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# 기존 deployment 삭제 후 sidecar가 포함된 새 deployment 생성
kubectl delete deployment synergy-leverager

# YAML 파일 생성
kubectl create deployment synergy-leverager --image=nginx --dry-run=client -o yaml > synergy-leverager.yaml
vi synergy-leverager.yaml
kubectl apply -f synergy-leverager.yaml
kubectl rollout restart deployment synergy-leverager
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: synergy-leverager
  name: synergy-leverager
spec:
  replicas: 1
  selector:
    matchLabels:
      app: synergy-leverager
  template:
    metadata:
      labels:
        app: synergy-leverager
    spec:
      containers:
        - image: nginx
          name: nginx
          command: ['sh', '-c', 'while true; do echo "logging" >> /var/log/synergy-leverager.log; sleep 1; done']
          volumeMounts:
            - name: data
              mountPath: /var/log
        - name: sidecar
          image: busybox:stable
          command: ['/bin/sh', '-c', 'touch /var/log/synergy-leverager.log; tail -n+1 -f /var/log/synergy-leverager.log']
          volumeMounts:
            - name: data
              mountPath: /var/log
      volumes:
        - name: data
          emptyDir: {}
```

# 4. JSONPATH 쿼리

# Q9

Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file
all-nodes-os-info.txt at root location.

Note: The osImage are under the nodeInfo section under status of each node.

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 테스트용 Pod 생성 (JSONPath 연습용)
kubectl run test-pod1 --image=nginx:1.21
kubectl run test-pod2 --image=busybox:1.35 -- sleep 3600
kubectl run test-pod3 --image=redis:6-alpine

echo "JSONPath 테스트 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# 노드 OS 정보 추출
kubectl get nodes -o=jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.nodeInfo.osImage}{"\n"}{end}' \
> /root/all-nodes-os-info.txt

# 결과 확인
cat /root/all-nodes-os-info.txt

# 추가 JSONPath 예제들
kubectl get pods -o jsonpath='{.items[*].status.podIP}' > /tmp/pod-ips.txt
kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.phase}{"\n"}{end}' > /tmp/pod-status.txt
```

# 5. PriorityClass 2025
Perform the following tasks:

Create a new PriorityClass named high-priority for user workloads with a value that is one less than the highest existing user-defined priority class value.

Patch the existing Deployment busybox-logger running in the priority namespace to use the high-priority priority class.
Ensure that the busybox-logger Deployment rolls out successfully with the new priority class set.

Note: It is expected that pods from other Deployments running in the priority namespace are evicted.

- [https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_priorityclass/](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_priorityclass/)
- [https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_patch/)
- [https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority)

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 네임스페이스 생성
kubectl create namespace priority

# 기존 우선순위 클래스 생성 (시뮬레이션)
kubectl create priorityclass existing-priority --value=1000 --description="Existing user priority class"

# 테스트용 deployment 생성
kubectl create deployment busybox-logger --image=busybox:1.35 --namespace=priority --replicas=2
kubectl patch deployment busybox-logger -n priority -p '{"spec":{"template":{"spec":{"containers":[{"name":"busybox","command":["sh","-c","while true; do echo Logging...; sleep 5; done"]}]}}}}'

# 다른 deployment도 생성 (eviction 테스트용)
kubectl create deployment other-app --image=nginx:1.21 --namespace=priority --replicas=1

echo "PriorityClass 테스트 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# 기존 우선순위 클래스 확인
kubectl get priorityclass

# 새 PriorityClass 생성 (기존 값보다 1 작게)
kubectl create priorityclass high-priority --value=999 --description="High priority class for critical workloads"

# Deployment에 우선순위 클래스 적용
kubectl patch deployment busybox-logger -p '{"spec":{"template":{"spec": {"priorityClassName": "high-priority"}}}}' --namespace=priority

# 롤아웃 상태 확인
kubectl rollout status deployment/busybox-logger -n priority

# 결과 확인
kubectl get deployment -n priority busybox-logger -o yaml | grep -A 2 priorityClassName
kubectl get pods -n priority -o custom-columns=NAME:.metadata.name,PRIORITY:.spec.priorityClassName,STATUS:.status.phase
```

# 6. StorageClass 2025

Create a StorageClass named local-path and set it as the default StorageClass.

Requirements:
- Provisioner: rancher.io/local-path
- Volume Binding Mode: WaitForFirstConsumer
- Set as default StorageClass

- [https://kubernetes.io/docs/concepts/storage/storage-classes/](https://kubernetes.io/docs/concepts/storage/storage-classes/)

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 기존 StorageClass 확인
kubectl get storageclass

echo "StorageClass 테스트 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# 기존 기본 StorageClass 제거 (있다면)
kubectl patch storageclass standard -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}' || true

# YAML 파일 생성
vi local-path.yaml
kubectl apply -f local-path.yaml

# StorageClass 확인
kubectl get storageclass local-path -o yaml
kubectl describe storageclass local-path
```

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-path
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: rancher.io/local-path
volumeBindingMode: WaitForFirstConsumer
```

# 7. ArgoCD 2025

Weightage: 7%

Install ArgoCD in the cluster by performing the following tasks:
Add the official Argo CD Helm repository with the name argo.

Generate a template of the ArgoCD Helm Chart version 7.7.3 for the argocd namespace and save it to ~/argo-helm.yaml.
Configure the chart to not install CRDs.

Note - The Argo CD CRDs have already been pre-installed in the cluster.

- [https://helm.sh/docs](https://helm.sh/docs)

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# ArgoCD CRD 사전 설치 (시뮬레이션)
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/crds/application-crd.yaml
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/crds/applicationset-crd.yaml

# CRD 설치 확인
kubectl get crd | grep argoproj

echo "ArgoCD 설치 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# Helm repository 추가
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

# Repository 확인
helm repo list | grep argo

# Chart 버전 확인
helm search repo argo/argo-cd --versions | head -5

# 템플릿 생성 (CRD 설치 비활성화)
helm template argocd argo/argo-cd --version 7.7.3 -n argocd \
  --set crds.install=false > ~/argo-helm.yaml

# 생성된 템플릿 확인
ls -la ~/argo-helm.yaml
head -20 ~/argo-helm.yaml

# CRD가 포함되지 않았는지 확인
grep -i "CustomResourceDefinition" ~/argo-helm.yaml || echo "No CRDs found (expected)"
```

# 8. Deploy Recovery & Data Consistency
A MariaDB Deployment in the mariadb namespace has been deleted by mistake. Your task is to restore the Deployment ensuring data persistence. Follow the steps:

Create a PersistentVolumeClaim (PVC) named mariadb in the mariadb namespace with the following specifications:
Storage 250Mi

Note - You must use the existing retained PersistentVolume (PV). There is only one existing PersistentVolume.
Edit the MariaDB Deployment file located at ~/mariadb-deployment.yaml to use the PVC you created in the previous step.
Apply the updated Deployment file to the cluster.

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 네임스페이스 생성
kubectl create namespace mariadb

# 기존 PV 시뮬레이션 (retained PV 생성)
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mariadb-pv
spec:
  capacity:
    storage: 300Mi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: standard
  hostPath:
    path: /tmp/mariadb-data
EOF

# MariaDB Deployment 파일 생성 (~/mariadb-deployment.yaml)
cat <<EOF > ~/mariadb-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mariadb
  namespace: mariadb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mariadb
  template:
    metadata:
      labels:
        app: mariadb
    spec:
      containers:
        - name: mariadb
          image: mariadb:latest
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: "password"
          ports:
            - containerPort: 3306
          # volumeMounts 섹션을 추가해야 함
EOF

echo "MariaDB 복구 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# PVC 생성
vi mariadb-pvc.yaml
kubectl apply -f mariadb-pvc.yaml

# PV, PVC 상태 확인
kubectl get pv,pvc -n mariadb

# MariaDB Deployment 파일 편집 (PVC 사용하도록 수정)
vi ~/mariadb-deployment.yaml

# Deployment 적용
kubectl apply -f ~/mariadb-deployment.yaml

# 배포 상태 확인
kubectl get deployment -n mariadb
kubectl get pods -n mariadb
```

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mariadb-pv
spec:
  capacity:
    storage: 300Mi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: standard
  hostPath:
    path: /tmp/mariadb-data
```

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mariadb
  namespace: mariadb
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 250Mi
  storageClassName: standard
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mariadb
  namespace: mariadb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mariadb
  template:
    metadata:
      labels:
        app: mariadb
    spec:
      containers:
        - name: mariadb
          image: mariadb:latest
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: "password"
          ports:
            - containerPort: 3306
          volumeMounts:
            - name: mariadb-storage
              mountPath: /var/lib/mysql
      volumes:
        - name: mariadb-storage
          persistentVolumeClaim:
            claimName: mariadb

```

# 7. 사용자 및 CSR 생성 - 사용자에게 권한 부여

Create a new user “ajeet”. Grant him access to the cluster.
User “ajeet” should have permission to create, list, get, update and delete pods.
The private key exists at location:

/root/ajeet/.key and csr at /root/ajeet.csr

- [https://kubernetes.io/docs/tasks/tls/certificate-issue-client-csr/](https://kubernetes.io/docs/tasks/tls/certificate-issue-client-csr/)
- [https://kubernetes.io/ko/docs/reference/kubectl/cheatsheet/](https://kubernetes.io/ko/docs/reference/kubectl/cheatsheet/)

```bash
openssl genrsa -out ajeet.key 3072
openssl req -new -key ajeet.key -out ajeet.csr -subj "/CN=ajeet"
cat ajeet.csr | base64 | tr -d "\n"

vi csr.yaml
kubectl apply -f csr.yaml
kubectl get csr
kubectl certificate approve ajeet

kubectl create clusterrole ajeet-role --verb=create,list,get,update,delete --resource=pods
kubectl create clusterrolebinding ajeet-role-binding --clusterrole=ajeet-role  --user=ajeet

kubectl get csr ajeet -o jsonpath='{.status.certificate}'| base64 -d > ajeet.crt
kubectl config set-credentials ajeet --client-key=ajeet.key --client-certificate=ajeet.crt --embed-certs=true
kubectl config set-context ajeet --cluster=kubernetes --user=ajeet
kubectl --context ajeet auth whoami

kubectl config use-context ajeet
kubectl config view --minify
kubectl --context=ajeet auth can-i get pods
kubectl --context=ajeet get pods
```

```yaml
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: ajeet
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJRFZUQ0NBYjBDQVFBd0VERU9NQXdHQTFVRUF3d0ZZV3BsWlhRd2dnR2lNQTBHQ1NxR1NJYjNEUUVCQVFVQQpBNElCandBd2dnR0tBb0lCZ1FERzRGS2d1TTVjdG9UMUcwRjIxdlQzWUx4SkZXL0ZhYy9sdHFZZlFkQnk2VURaCkdsWTdNSkpyaWhmc2htanQ3U2sxS1o3YWVlRU92SGFGUWZnVWpKM3ZhNy96bGFLUDAzTXd1d1lsZ0JhY1lTQUoKU2phcS81elFwaEZnVmdNSkZ4M2ZBQThXVmtMbVBkbE1TU1k0VUlnN1B3ZVgrVmhUM1RHL3RxVGlyUUZKZk9oTApEcDR2d1IyaGg1OXU2ZkFpejBQYjhiQW5Scmtod0UycDd4TDAyUDEwQ3NVNkx4cW5mQ0xodk04emNuQy8xc3NmClJ3Z1hVKzBCR0FiUXM5dGtrVE9UcXBUUHZCNUI0aDhDWHhpbnFSYjY0Y2V6cGZLWENnK0pLM3pycjVoUW56U0wKaU9aV1dSRGE0c1FtUDZ5a1ZLajNVSzhlclNFbDRWclV1TWFlOW1jN1U1Wk5aekU1azVyNVZNYXFRTmVoMGkwcQpJYktFM1NTMUdLN3k2bUE5WXNYeFVXamo1MDZ5TDY0b2pLaldMNkhVazFweVFCOWRWMGg2TVc0dUpjQStkUXFFCi9TQk9STmtJUmYyM1dyWkF6YlZsWGE2RWNFbmQ5Ym1ZL3JmRlNJU3dnTDRBanFoaHNScXVmWXA5SG5ZTlZ5WTMKRDZQc012cHowZHpWSU5PV2lDMENBd0VBQWFBQU1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQmdRQTE0Z3JDVzhBUApWTHI2djRNdStTRUhXbWFLTDZXSnFCeU1MVzErZFg2YmNIQldpblllV3Q2V1ZiODllS1IybHJscVRPcnpFZEJtCmg2ck9mODJhRmdUUjZra3ptS0c3aTNVT0M4OUgzWFFpZUNUYnU2UzhLUC9iSHU3dXF1T3VhbkNBN3ZlOFpnUnEKWUNRWXFiRmtCWnZidTYzcSt2d0VtTVpRaVAxWmg0bWx0cUZhcm5LWWY3L2ZSV21rNnYyQkRRMm9NYk5oNnNqNQpSd1FXMis3emFnNnFnZ2JmL250WW4xU0NaQWgwZEYxQ0c2T3A4bG84MUdCaEtqZERsck9QYjlmYU13UHl0eEp2CkJYbTMyOThFenVEVHFiQWk0U0R6dHEvSjVOODN6dzBHWVQ2MEd5d0ZVNmg3cm5BUmRPTWxSaFlSSVFxV0pHM0kKZkRkdGgxSEt4WXoyRDlxdURpeGJmdGs5dnJtRjdxWTl4RUhRZThsbmErT3BWYkhiM1g3ZEs4K3I2UUp0dHVUTQo3eDhGY1dWTmhFQ0RucXFJTVBOUzBYVENyeGNUMzFGenY5UHp1K1FibFRwM0EzZU9iandqaVlIZk1aUHlNeVFMCmVVOHZMM3M2Zmd5SzVUWDkrYUJGcWdibHFGN0k4YmZSeEhwK3VEdXlkMnRWWnJOZUFWSXpGdGs9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
```

# 8. Gateway & HTTPRoute 2025
Weightage: 12%

Migrate an existing web application from Ingress to Gateway API.
You must maintain HTTPS access.

First, create a Gateway named web-gateway with hostname gateway.web.k8s.local that maintains the existing TLS and listener configuration from the existing ingress resource named web.

Next, create an HTTPRoute named web-route with hostname gateway.web.k8s.local that maintains the existing routing rules from the current Ingress resource named web.

Note - A GatewayClass named nginx is installed in the cluster.

- [https://kubernetes.io/docs/concepts/services-networking/gateway/](https://kubernetes.io/docs/concepts/services-networking/gateway/)
- [https://kubernetes.io/docs/concepts/services-networking/ingress/#tls](https://kubernetes.io/docs/concepts/services-networking/ingress/#tls)

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# Gateway API CRD 설치
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.1.0/standard-install.yaml

# 테스트 애플리케이션 및 서비스 생성
kubectl create deployment web-app --image=nginx:1.21 --replicas=1
kubectl expose deployment web-app --name=web-service --port=80

# TLS 인증서 생성
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout /tmp/tls.key -out /tmp/tls.crt \
  -subj "/CN=gateway.web.k8s.local"

kubectl create secret tls web-tls --cert=/tmp/tls.crt --key=/tmp/tls.key

# 기존 Ingress 생성 (마이그레이션 대상)
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web
spec:
  tls:
  - hosts:
    - gateway.web.k8s.local
    secretName: web-tls
  rules:
  - host: gateway.web.k8s.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
EOF

echo "Gateway API 마이그레이션 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# 1. GatewayClass 생성 (nginx 컨트롤러용)
cat <<EOF | kubectl apply -f -
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: nginx
spec:
  controllerName: "nginx.org/gateway-controller"
EOF

# 2. Gateway 생성
vi web-gateway.yaml
kubectl apply -f web-gateway.yaml

# 3. HTTPRoute 생성
vi web-route.yaml
kubectl apply -f web-route.yaml

# 4. Gateway API 리소스 확인
kubectl get gatewayclass
kubectl get gateway
kubectl get httproute

# 5. Gateway 상태 확인
kubectl describe gateway web-gateway
kubectl describe httproute web-route
```

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: nginx
spec:
  controllerName: "nginx.org/gateway-controller"
```

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-gateway
spec:
  tls:
  - hosts:
      - gateway.web.k8s.local
    secretName: web-tls
  rules:
  - host: gateway.web.k8s.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
```

- [https://kubernetes.io/docs/tasks/administer-cluster/certificates/](https://kubernetes.io/docs/tasks/administer-cluster/certificates/)

```bash
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls.key -out /tmp/tls.crt -subj "/CN=gateway.web.k8s.local"
kubectl create secret tls web-tls --cert=/tmp/tls.crt --key=/tmp/tls.key

kubectl create deploy web-app --image=nginx --replicas=1
kubectl expose deploy web-app --name=web-service --port 80

vi web-ingress.yaml
kubectl apply -f web-ingress.yaml

vi web-route.yaml
kubectl apply -f web-route.yaml

```

- [https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/)

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: web-gateway
spec:
  gatewayClassName: nginx
  listeners:
  - name: https
    protocol: HTTPS
    port: 443
    hostname: gateway.web.k8s.local
    tls:
      mode: Terminate
      certificateRefs:
      - kind: Secret
        name: web-tls
```

- [https://kubernetes.io/docs/concepts/services-networking/gateway/#api-kind-httproute](https://kubernetes.io/docs/concepts/services-networking/gateway/#api-kind-httproute)

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: web-route
spec:
  parentRefs:
  - name: web-gateway
  hostnames:
  - "gateway.web.k8s.local"
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /
    backendRefs:
    - name: web-service
      port: 80
```

# 10. 서비스 DNS 확인

# Q15

Create a nginx pod called dns-resolver using image nginx, expose it internally with a service called dns-resolver-service.

Check if pod and service name are resolvable from within the cluster.

Use the image: busybox:1.28 for dns lookup.

Save the result in /root/nginx.svc

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
echo "DNS 확인 테스트 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# 1. nginx Pod 생성
kubectl run dns-resolver --image=nginx

# 2. 서비스로 노출
kubectl expose pod dns-resolver --name=dns-resolver-service --port=80

# 3. DNS 조회 테스트 및 결과 저장
kubectl run dns-lookup --image=busybox:1.28 --restart=Never -it --rm -- \
nslookup dns-resolver-service > /root/nginx.svc

# 4. 결과 확인
cat /root/nginx.svc

# 5. 추가 DNS 테스트
kubectl run dns-test --image=busybox:1.28 --restart=Never -it --rm -- sh -c "
echo 'Testing service DNS resolution:'
nslookup dns-resolver-service
echo 'Testing pod DNS resolution:'
nslookup dns-resolver-service.default.svc.cluster.local
"
```

# 9. CNI (Container Network Interface) 2025

Weightage: 10%

Install and set up a Container Network Interface (CNI) that meets these requirements:
Pick and install one of the CNI options:

The CNI you choose must satisfy following requirement:
<native network policy support>

Flannel version 0.26.1

Manifest - https://github.com/flanner-io/flanner/releases/download/v0.26.1/kube-flanner.yml

Calico version 3.28.2

Manifest - https://raw.githubusercontent.com/projectcalico/calico/v3.28.2/manifests/tigera-operator.yaml

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 기존 CNI 제거 (있다면)
kubectl delete -f https://github.com/flannel-io/flannel/releases/download/v0.26.1/kube-flannel.yml --ignore-not-found

# 노드 상태 확인
kubectl get nodes

echo "CNI 설치 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# 1. 요구사항 분석
# Network Policy 지원이 필요하므로 Calico 선택 (Flannel은 미지원)

# 2. Calico 설치 (create 사용 - 중요!)
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.2/manifests/tigera-operator.yaml

# 3. Calico 설치 상태 확인
kubectl get pods -n calico-system
kubectl get pods -n tigera-operator

# 4. 노드 상태 확인 (Ready가 될 때까지 대기)
kubectl get nodes

# 5. Network Policy 테스트
kubectl run test-pod1 --image=nginx:1.21 --labels="app=test1"
kubectl run test-pod2 --image=nginx:1.21 --labels="app=test2"

# 6. 기본 연결 테스트
kubectl exec test-pod1 -- ping -c 2 $(kubectl get pod test-pod2 -o jsonpath='{.status.podIP}')

# 7. Network Policy 생성 (테스트)
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-netpol
spec:
  podSelector:
    matchLabels:
      app: test2
  policyTypes:
  - Ingress
  ingress: []  # 모든 트래픽 차단
EOF

# 8. 차단된 연결 테스트 (실패해야 함)
kubectl exec test-pod1 -- ping -c 2 $(kubectl get pod test-pod2 -o jsonpath='{.status.podIP}') || echo "Traffic blocked by NetworkPolicy (expected)"

# 9. 정리
kubectl delete networkpolicy test-netpol
kubectl delete pod test-pod1 test-pod2
```

# 10. WordPress Application Some Pods are Nou Up and running 2025
Weightage: 9%

You manage a WordPress application. Some pods are not up and running.
Adjust all Pod resource requests as follows:

Divide node resources evenly across all 3 pods.

Give each Pod a fair share of CPU and memory.

Add enough overhead to keep the node stable.

Note - Use exact same requests for both containers and init containers.
Scale down the WordPress deployment to 0 replicas while updating the resource requests.

After updates, confirm:

WordPress keeps 3 replicas.

All Pods are running and ready.

```bash
vi wordpress-deploy.yaml
kubectl apply -f wordpress-deploy.yaml
kubectl scale deploy wordpress --replicas=0

kubectl describe node controlplane
kubectl patch deployment wordpress -p '{"spec": {"template": {"spec": {"initContainers": [{"name": "init-wordpress", "resources": {"requests": {"cpu": "100m", "memory": "100Mi"}}}]}}}}'
kubectl patch deployment wordpress -p '{"spec": {"template": {"spec": {"containers": [{"name": "wordpress", "resources": {"requests": {"cpu": "100m", "memory": "100Mi"}}}]}}}}'
kubectl scale deploy wordpress --replicas=3
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
spec:
  replicas: 3
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      initContainers:
      - name: init-wordpress
        image: busybox
        command: ["sh", "-c", "echo Initializing && sleep 5"]
      containers:
      - name: wordpress
        image: wordpress:latest
        ports:
        - containerPort: 80

```

# 10. 주어진 시나리오를 기반으로 NetworkPolicy 만들기

# Q18
Weightage: 11%

Create a Network Policy named "appychip" in default namespace
There should be two types, ingress and egress.

The ingress should block traffic from an IP range of your choice except some other IP range.
Should also have namespace and pod selector.
Ports for ingress policy should be 6379

For Egress, it should allow traffic to an IP range of your choice on 5978 port.

- [https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/)

```bash
vi q18.yaml
kubectl apply -f q18.yaml
```

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: appychip
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978

```

# 11. Debian package Installation 2025

Weightage: 10%

Complete these tasks to prepare the system for Kubernetes:
Set up cri-dockerd:

Install the Debian package:
~/cri-dockerd_0.3.9.3-0.ubuntu-focal_amd64.deb

Start the cri-dockerd service.

Enable and start the systemd service for cri-dockerd.

Configure these system parameters:

Set net.bridge.bridge-nf-call-iptables to 1.

Set net.ipv6.conf.all.forwarding to 1.

Set net.ipv4.ip_forward to 1.

- [https://kubernetes.io/docs/setup/production-environment/container-runtimes/](https://kubernetes.io/docs/setup/production-environment/container-runtimes/)

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# cri-dockerd 패키지 다운로드 (시뮬레이션)
curl -LO https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.9/cri-dockerd_0.3.9.3-0.ubuntu-focal_amd64.deb

echo "시스템 패키지 설치 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# 1. 시스템 매개변수 구성
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.ipv6.conf.all.forwarding = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

# 2. 재부팅 없이 매개변수 적용
sudo sysctl --system

# 3. 매개변수 확인
sysctl net.ipv4.ip_forward
sysctl net.ipv6.conf.all.forwarding
sysctl net.bridge.bridge-nf-call-iptables

# 4. cri-dockerd 패키지 설치
sudo apt update
sudo dpkg -i ~/cri-dockerd_0.3.9.3-0.ubuntu-focal_amd64.deb

# 5. 서비스 상태 확인
sudo systemctl status cri-docker.service
sudo systemctl status cri-docker.socket

# 6. 서비스 시작 및 활성화
sudo systemctl start cri-docker.service
sudo systemctl enable cri-docker.service

# 7. 최종 상태 확인
sudo systemctl status cri-docker.service
systemctl list-units | grep cri

echo "시스템 패키지 설치 완료"
```

# 11. Kubernetes Context - 1

# Q19
Weightage: 7%

You have access to multiple clusters from your main terminal through kubectl contexts.
Write all those context names into
/opt/course/1/contexts

Next write a command to display the current context into
/opt/course/1/context_default_kubectl.sh,
the command should use kubectl.

Finally write a second command doing the same thing into
/opt/course/1/context_default_no_kubectl.sh,
but without the use of kubectl.

- [https://kubernetes.io/ko/docs/concepts/configuration/organize-cluster-access-kubeconfig/](https://kubernetes.io/ko/docs/concepts/configuration/organize-cluster-access-kubeconfig/)

```bash
mkdir -p /opt/course/1

kubectl config get-contexts -o name > /opt/course/1/contexts
cat /opt/course/1/contexts

echo kubectl config current-context > /opt/course/1/context_default_kubectl.sh
chmod +x /opt/course/1/context_default_kubectl.sh
cat /opt/course/1/context_default_kubectl.sh
bash /opt/course/1/context_default_kubectl.sh

cat ~/.kube/config | grep current
echo 'cat ~/.kube/config | grep current' > /opt/course/1/context_default_no_kubectl.sh
bash /opt/course/1/context_default_no_kubectl.sh
```

# 12. 모든 Custom CRD 나열 및 custom-crd.txt 저장
Weightage: 7%

List all custom crd from cert manager and store it in custom-crd.txt.

Get the subject field from the cert manager and store it in cert-manager-subject.txt

## 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.crds.yaml
echo "CRD 관리 테스트 환경이 설정되었습니다."
```

## ✅ 솔루션

```bash
# 1. cert-manager CRD 설치
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.crds.yaml

# 2. cert-manager 관련 CRD 목록 저장
kubectl get crd | grep cert > custom-crd.txt

# 3. 결과 확인
cat custom-crd.txt

# 4. certificates CRD의 subject 필드 정보 추출
kubectl get crd certificates.cert-manager.io -o jsonpath='{.spec.versions[*].schema.openAPIV3Schema.properties.spec.properties.subject}' \
> cert-manager-subject.txt

# 5. subject 필드 정보 확인
cat cert-manager-subject.txt

# 6. 추가 CRD 정보 확인
kubectl get crd certificates.cert-manager.io -o yaml | grep -A 10 subject

echo "CRD 정보 추출 완료"
```

# 13. 2025 Network Policy YAML
There are two deployments. frontend and backend deployment.
frontend will be in frontend NS and backend will be in backend NS.
Apply the least permissive policy to have interaction between frontend and backend deployment.
Below are the 3 YAML file to apply network policy.
Choose either of them with least permission.

1 YAML - pod selector of all. type is ingress and pod selector with all
2 YAML - pod selector as well as namespace selector are present
3 YAML - pod selector, NS selector, POD CIDR


```bash
 # 2번째 정책이 가장 적절. 1번은 부족하고, 2번은 복잡함
 kubectl apply -f network-policy-2.yaml
```

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: open-backend-access
  namespace: backend
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from: []
      ports:
        - protocol: TCP
          port: 8080
```

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-to-backend
  namespace: backend
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: frontend
          podSelector:
            matchLabels:
              app: frontend
      ports:
        - protocol: TCP
          port: 8080

```

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-to-backend-with-cidr
  namespace: backend
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: frontend
          podSelector:
            matchLabels:
              app: frontend
      ports:
        - protocol: TCP
          port: 8080
    - from:
        - ipBlock:
            cidr: 192.168.1.0/24
      ports:
        - protocol: TCP
          port: 8080

```


# 14. 2025 - ConfigMap

Weightage: 10%

An nginx deployment configured via a ConfigMap containing the Nginx config file.
The task is to ensure that TLS 1.2 should not be supported or permitted.
And it should only work with TLS 1.3
Do the required steps and verify that it works with TLS 1.3

- [https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_secret_tls/](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_secret_tls/)
- [https://kubernetes.io/docs/concepts/configuration/secret/#use-case-dotfiles-in-a-secret-volume](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-dotfiles-in-a-secret-volume)
- [https://kubernetes.io/docs/concepts/configuration/secret/#use-case-dotfiles-in-a-secret-volume](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-dotfiles-in-a-secret-volume)

```bash
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt \
-subj "/CN=example.com"
kubectl create secret tls nginx-tls --cert=tls.crt --key=tls.key

vi nginx-cm.yaml
kubectl apply -f nginx-cm.yaml
kubectl create deploy nginx --image=nginx --replicas=1

vi nginx-deployment.yaml
kubectl apply -f nginx-deployment.yaml
kubectl expose deploy nginx --port=443 --target-port=443 --type=NodePort
kubectl get svc
curl -k https://localhost:31529

kubectl edit cm nginx-config
# ssl_protocols TLSv1.3; 으로 수정
curl --tlsv1.2 -k https://localhost:31529
# ConfigMap 수정했더라도, 배포를 다시 해야 함

kubectl scale deploy nginx --replicas=0
kubectl scale deploy nginx --replicas=1
# rollout만 해도 됨
kubectl rollout restart deploy nginx
curl --tlsv1.3 -k https://localhost:31529

```

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    events {}
    http {
        server {
            listen 443 ssl;
            server_name example.com;
            ssl_certificate /etc/nginx/certs/tls.crt;
            ssl_certificate_key /etc/nginx/certs/tls.key;
            ssl_protocols TLSv1.2 TLSv1.3; # Initial config supports both
            location / {
                root /usr/share/nginx/html;
                index index.html;
            }
        }
    }

```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 443
          volumeMounts:
            - name: config
              mountPath: /etc/nginx/nginx.conf
              subPath: nginx.conf
            - name: certs
              mountPath: /etc/nginx/certs
      volumes:
        - name: config
          configMap:
            name: nginx-config
        - name: certs
          secret:
            secretName: nginx-tls

```

# 15. Nginx Ingress

Weightage: 10%

Namespace: sound-repeater

Exposing Service echoserver-service on http://example.org/echo using Service port 8080

Expose the service echo-server service in the given namespace
Create an nginx ingress for above deployment and service for the host http://www.example.org/echo over port 80

```bash
kubectl create ns sound-repeater
kubectl create deploy echoserver --image=nginx -n sound-repeater
kubectl expose deploy echoserver --name=echoserver-service -n sound-repeater --port=8080 --target-port=8080 \
--type=NodePort

vi q15.yaml
k apply -f q15.yaml
curl -o /dev/null -s -w "%{http_code}\n" http://www.example.org/echo
kubectl describe ingress nginx-ingress -n sound-repeater
kubectl describe deploy echoserver -n sound-repeater

```

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress
  namespace: sound-repeater
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx-example
  rules:
  - host: www.example.org
    http:
      paths:
      - pathType: Prefix
        path: "/echo"
        backend:
          service:
            name: echoserver-service
            port:
              number: 8080

```

# 16. Kubeadm

A kubeadm provisioned cluster was migrated to a new machine. Requires configuration changes to run successfully.

Task:
We need to fix a single-node cluster that got broken during machine migration.

Identify the broken cluster components and investigate what caused them to break.

The decommissioned cluster used an external etcd server.

Next, fix the configuration of all broken cluster components.

Ensure to restart all necessary services and components for changes to take effect.

Finally, ensure the cluster, single node and all pods are Ready.

```bash
kubectl get po
journalctl -u kubelet -f
ls /etc/kubernetes/manifests/

vi /etc/kubernetes/manifests/kube-apiserver

# --etcd-server=https://128.0.0.1:2379

sudo systemctl restart kubelet
sudo systemctl status kubelet

```