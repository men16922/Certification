[TOC]

# ì‹¤ì „ ë¬¸ì œ í’€ì´

### â“1. Retrieve Error Messages from a Container Log

- Cluster: kubectl config use-context **hk8s**

---

In the `customera` namespace, check the log for the nginx container in the `custom-app` Pod.

Save the lines which contain the text â€œerrorâ€ to the file /data/CKA/errors.txt.

---

- â—Answer

    ```bash
    kubectl config use-context hk8s
    
    **kubectl get pods  -n customera**
    kubectl logs custom-app -n customera | grep -i error > /data/CKA/errors.txt
    cat /data/CKA/errors.txt
    ```


### â“2. Node Troubleshooting

- Cluster: kubectl config use-context **hk8s**

---

A Kubernetes worker node, named hk8s-worker2 is in state NotReady.
Investigate why this is the case, and perform any appropriate steps to bring the node to a Ready state, ensuring that any changes are made permanent.

---

- â—Answer

    ```bash
    #
    **console$ ssh hk8s-master
    kubectl get nodes**
    NAME           STATUS     ROLES           AGE   VERSION
    hk8s-master    Ready      control-plane   70d   v1.28.0
    hk8s-worker1   Ready      <none>          70d   v1.28.0
    hk8s-worker2   **NotReady**   <none>          70d   v1.28.0
    
    **ssh hk8s-worker2** 
    sudo -i
    # containerd runtime ë™ì‘ ìƒíƒœ í™•ì¸
    **systemctl status containerd**
    
    # kubelet ë™ì‘ ìƒíƒœ í™•ì¸
    **systemctl status kubelet**
    **systemctl enable --now kubelet**  # enable : ë¶€íŒ…ì‹œ ìë™ìœ¼ë¡œ ë°ëª¬ ì‹¤í–‰ë˜ë„ë¡ ì„¤ì •. now ì˜µì…˜ ì¶”ê°€ë¡œ ì¦‰ì‹œë¡œ ì„œë¹„ìŠ¤ ë™ì‘
    **systemctl status kubelet**
    exit
    exit
    
    **console$ kubectl get nodes**
    NAME           STATUS     ROLES           AGE   VERSION
    hk8s-master    Ready      control-plane   70d   v1.28.0
    hk8s-worker1   Ready      <none>          70d   v1.28.0
    hk8s-worker2   Ready      <none>          70d   v1.28.0
    ```

    <aside>
    ğŸ“Œ Node Troubleshooting ë¬¸ì œ í•´ê²°ì„ í•˜ë©´ hk8s-worker1, hk8s-worker2ê°€ ëª¨ë‘ Ready ìƒíƒœê°€ ë©ë‹ˆë‹¤. ì´í›„ ë‹¤ìŒ ì‘ì—… ì§„í–‰í•´ì£¼ì„¸ìš”.
    1. hk8s-master, hk8s-worker1, hk8s-worker2 ì‹œìŠ¤í…œì„ ëª¨ë‘ ì¢…ë£Œí•˜ì„¸ìš”. 
    2. hk8s-worker1 ì„œë²„ì˜ ë©”ëª¨ë¦¬ë¥¼ 2Gë¡œ ë³€ê²½
    3. hk8s-master, hk8s-worker1, hk8s-worker2 ì‹œìŠ¤í…œì„ ëª¨ë‘ ë¶€íŒ…

    </aside>


### â“3. Count the Number of Nodes That Are Ready to Run Normal Workloads

- Cluster: kubectl config use-context **hk8s**

---

Determine how many nodes in the cluster are ready to run **normal workloads** (i.e., workloads that **do not have any special tolerations**).

Output this number to the file  /var/CKA2022/count.txt

---

- â—Answer

    ```bash
    # í´ëŸ¬ìŠ¤í„°ì—ì„œ normal workloadë¥¼ ì‹¤í–‰í•  ready ìƒíƒœì¸node ìˆ˜ë¥¼ ì¹´ìš´íŠ¸í•´ì„œ ì €ì¥
    # ì´ ë¬¸ì œëŠ” ì¼ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰í• ìˆ˜ ìˆëŠ” Nodeë¥¼ ì¹´ìš´íŠ¸ í•˜ë¼ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.
    # normal workloadì˜ ì˜ë¯¸ Podë¥¼ ì´ìš©í•´ ì¼ë°˜ì ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë™ì‘(íŠ¹ì • tolerationë“¤ì´ ì—†ëŠ” íŒŒë“œë“¤ì´ ì‹¤í–‰ë˜ëŠ”..)í•˜ëŠ” nodeì¤‘ ready ìƒíƒœì¸ ë…¸ë“œ ìˆ˜ ì¹´ìš´íŠ¸
    
    # 1. ë¨¼ì „ í•´ë‹¹ hk8s í´ëŸ¬ìŠ¤í„°ì— ì ‘ì†í•œí›„ ready ìƒíƒœì˜ ë…¸ë“œë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    $ kubectl use-context hk8s
    $ kubectl get nodes | grep -i -w ready
    hk8s-master    Ready    control-plane   14h   v1.28.0
    hk8s-woker1    Ready    <none>          14h   v1.28.0
    hk8s-woker2    Ready    <none>          14h   v1.28.0
    
    # 2. ê° ë…¸ë“œì˜ taintë¥¼ í™•ì¸í•´ì„œ pod tolerationsë¥¼ ìš”êµ¬í•˜ëŠ” í•­ëª©ì˜ ì„¤ì •ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    # master ì‹œìŠ¤í…œì€ NoScheduleì„ í†µí•´ normal workload ì‹¤í–‰ì´ ì œí•œë©ë‹ˆë‹¤. ë§ˆìŠ¤í„°ì—ì„œëŠ” ì¼ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ íŒŒë“œê°€ ìŠ¤ì¼€ì¥´ë§ ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.
    $ kubectl describe nodes hk8s-master | grep -i taint
    Taints:             node-role.kubernetes.io/control-plane:NoSchedule
    
    # worker1ë²ˆê³¼ worker2ë²ˆ ë…¸ë“œì—ëŠ” Taintê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‹ˆ normal workloadê°€ ì‹¤í–‰ê°€ëŠ¥í•©ë‹ˆë‹¤. ê²°êµ­ ì´ ë‘ê°œê°€ ë‹µì´ ë©ë‹ˆë‹¤.
    $ kubectl describe nodes hk8s-woker1 | grep -i taint
    Taints:             <none>
    
    $ kubectl describe nodes hk8s-woker1 | grep -i taint
    Taints:             <none>
    
    # 3. echo ëª…ë ¹ì„ ì´ìš©í•´ì„œ 2ë¥¼ /var/CKA2022/count.txt ì €ì¥í•©ë‹ˆë‹¤.
    echo "2" > /var/CKA2022/count.txt
    ```


### **â“4. Management Node**

- Cluster: kubectl config use-context **k8s**

---

**Set the node named k8s-worker1 as unavailable and reschedule all the pods running on it.**

---

- â—Answer

    ```bash
    kubectl drain k8s-worker1 --ignore-daemonsets  --force
    kubectl get nodes
    kubectl get pods
    ```


### **â“5. ETCD backup & restore**

<aside>
ğŸ“Œ ETCD ë°±ì—… ë° ë¦¬ìŠ¤í† ì–´ ë¬¸ì œ ìœ í˜•ì´ ë°”ë€Œì–´ì„œ ë°”ë€ ë¬¸ì œë¡œ ì¶œì œí–ˆìŠµë‹ˆë‹¤. í•™ìŠµ ì˜ìƒì„ í†µí•´ í•™ìŠµí•˜ì‹œë©´ ë¬´ë‚œíˆ í’€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë³€ê²½ ì‚¬í•­ : í•™ìŠµ ì˜ìƒì—ì„œëŠ” k8s-masterë¡œ ì ‘ì†í•´ì„œ ë°±ì—…ì„ í•˜ê³ , ë¦¬ìŠ¤í† ì–´ë¥¼ ì§„í–‰í•˜ì˜€ìœ¼ë‚˜, ìµœê·¼ ìœ í˜•ì—ì„œëŠ” 
- ë°±ì—… : console  etcdctl ëª…ë ¹ì„ ì‹¤í–‰í•´ì„œ master ì˜ etcd ë°ì´í„°ë¥¼ ìŠ¤ëƒ…ìƒ· í•´ì„œ ì½˜ì†”ì˜ íŠ¹ì • ë””ë ‰í† ë¦¬ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
- ë¦¬ì†ŒíŠ¸ì–´ : ë¦¬ìŠ¤í† ì–´ëŠ” ì£¼ì–´ì§„ snapshot.db íŒŒì¼ì„ masterfë¡œ ì „ë‹¬(ë˜ëŠ” ë§ˆìŠ¤í„°ì— ì €ì¥ë˜ì–´ ìˆìŒ)í•˜ê³ ,  master ë¡œ ssh ì›ê²© ë¡œê·¸ì¸ í›„ etcdctl  ëª…ë ¹ìœ¼ë¡œ restoreí•©ë‹ˆë‹¤. ë¦¬ìŠ¤í† ì–´ í›„  etcd.yaml ë„ ë‹¹ì—°íˆ ìˆ˜ì •í•˜ê³  íŒŒë“œ ë¦¬ìŠ¤íƒ€ìŠ¤ ë˜ì—ˆëŠ”ì§€ë„ í™•ì¸í•©ë‹ˆë‹¤.

ë¬¸ì œ í•´ê²° ê³¼ì •ì„ ì •ë¦¬í•˜ë©´ :
**1. consoleì— etcdctlì´ ì„¤ì¹˜ë˜ì–´ìˆìŠµë‹ˆë‹¤. consoleì—ì„œ k8s-masterì˜ etcdë¥¼ snapshot saveí•©ë‹ˆë‹¤.
ì¸ì¦ì„œ íŒŒì¼ ëª¨ë‘ consoleì— ì¡´ì¬í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ /home/ubuntu/key ë””ë ‰í† ë¦¬ì— ì¸ì¦ì„œ ëª¨ë‘ ì €ì¥ë¨
ë°±ì—… ëª…ë ¹ì—ì„œ endpointë§Œ ë°”ê¿”ì£¼ë©´ ë©ë‹ˆë‹¤.**

    **sudo ETCDCTL_API=3 etcdctl  --endpoints=https://10.0.2.10:2379 \
        --cacert=/data/cka/ca.crt   --cert=/data/cka/server.crt  --key=/data/cka/server.key \
         snapshot save /data/previous.db
2. restore ë¬¸ì œëŠ” ë°±ì—…ì„ ì‹¤í–‰í•œ í´ëŸ¬ìŠ¤í„°ê°€ ì•„ë‹Œ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì— snapshot íŒŒì¼ì„ ì €ì¥í•´ì„œ ë³µì›í•´ì•¼ í•©ë‹ˆë‹¤.
   scp /data/cka/etcd-snapshot-previous.db  k8s-master:~
   ssh k8s-master
   sudo ETCDCTL_API=3  etcdctl --data-dir=/var/lib/etcd-restore snapshot restore** etcd-snapshot-previous.db
   sudo vi /etc/kubernetes/manifests/etcd.yaml

**3. docker  ëŒ€ì‹  crictl   ëª…ë ¹ìœ¼ë¡œ etcd  ë™ì‘ë˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.**
sudo crictl ps    # ì‹¤í–‰í•˜ë©´ etcd id í‘œì‹œë©ë‹ˆë‹¤. ê·¸ê±° ê°€ì§€ê³  logs í™•ì¸í•˜ë©´ ë©ë‹ˆë‹¤. ì•„ë‹ˆë©´ ë” ì‰½ê²Œ consoleì—ì„œ 2-3ë¶„ í›„ì— kubectl ëª…ë ¹ì„ ì‹¤í–‰í•´ë³´ì‹œë©´ ë©ë‹ˆë‹¤.
sudo crictl  logs etcd'sID
exit

</aside>

### **â“consoleì—ì„œ  k8s masterì˜ etcd backup & restore**

- ì‚¬ì „ í™˜ê²½ êµ¬ì„±

    ```bash
    # consoleì—ì„œ k8s-masterì— ì €ì¥ëœ ì¸ì¦ì„œì™€ ë°±ì—… íŒŒì¼ì„ /data/cka í´ë”ì— ì €ì¥í•œ í›„ ê¸°ì¶œë¬¸ì œë¥¼ í’€ì–´ë³´ì„¸ìš”.
    # consleì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    sudo -i
    mkdir -p /data/cka
    cd /data/cka/
    scp k8s-master:/etc/kubernetes/pki/etcd/ca.crt .
    scp k8s-master:/etc/kubernetes/pki/etcd/server.crt .
    scp root@k8s-master:/etc/kubernetes/pki/etcd/server.key .
    scp root@k8s-master:/data/etcd-snapshot-previous.db .
    ```

- ì‘ì—… í´ëŸ¬ìŠ¤í„° :  **k8s**
  First, create a snapshot of the existing etcd instance running at `https://<k8s-master's IP>:2379`, saving the snapshot to `/data/etcd-snapshot.db`.
  Next, restore an existing, previous snapshot located at `/data/etcd-snapshot-previous.db`.

The following TLS certificates/key are supplied for connecting to the server with etcdctl:
CA certificate: `/data/cka/ca.crt`
Client certificate: `/data/cka/server.crt`
Client key: `/data/cka/server.key`
- **ë‹µì•ˆ**

    ```jsx
    # consoleì—ì„œ k8s masterì˜ etcd ë°±ì—…í•´ì„œ consoleì˜ ë””ë ‰í† ë¦¬ì— snapshot íŒŒì¼ë¡œ ì €ì¥
    **# snapshot save**
    sudo ETCDCTL_API=3 etcdctl --endpoints=https://MasterIP:2379 \
        --cacert=/data/cka/ca.crt \
        --cert=/data/cka/server.crt \
        --key=/data/cka/server.key \
        snapshot save /data/etcd-snapshot.db
    
    **# RESTORE**
    # consoleì— etcd-snapshot-previous.db íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
    scp /data/etcd-snapshot-previous.db cluster_master'sIP:~ 
    ssh cluster_master'sIP
    **sudo ETCDCTL_API=3  etcdctl --data-dir=/var/lib/etcd-restore snapshot restore** etcd-snapshot-previous.db
    ls /var/lib/etcd-restore
    
    ## etcd yamlíŒŒì¼ ìˆ˜ì •
    sudo vi /etc/kubernetes/manifests/etcd.yaml
    â€¦
     - hostPath:
          path: /var/lib/etcd-restore
          type: DirectoryOrCreate
        name: etcd-data
    
    # docker ëª…ë ¹ìœ¼ë¡œ etcdê°€ restart ë˜ì—ˆëŠ”ì§€ í™•ì¸
    #sudo docker ps -a | grep etcd
    # ctr -n k8s.io containers list | grep etcd
    crictl ps
    sudo crictl  logs etcd'sID
    exit
    
    ## consoleì—ì„œ ì¿ ë²„ë„¤í‹°ìŠ¤ ë™ì‘ ìƒíƒœ í™•ì¸
    kubectl get nodes
    ```


---

- ì°¸ê³ : ì˜ìƒ í•™ìŠµê³¼ ë™ì¼í•œ ì˜ˆì „ ìœ í˜•ì˜ ë¬¸ì œ í’€ì´ - ì‘ì—… í´ëŸ¬ìŠ¤í„°(**k8s)**
    
  ---

  First, create a snapshot of the existing etcd instance running at `https://127.0.0.1:2379`, saving the snapshot to `/data/etcd-snapshot.db`.
  Next, restore an existing, previous snapshot located at `/data/etcd-snapshot-previous.db`.

  The following TLS certificates/key are supplied for connecting to the server with etcdctl:
  CA certificate: `/etc/kubernetes/pki/etcd/ca.crt`
  Client certificate: `/etc/kubernetes/pki/etcd/server.crt`
  Client key: `/etc/kubernetes/pki/etcd/server.key`
    
  ---

    - â—Answer

        ```bash
        ssh master
        sudo -i
        sudo ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
           --cacert=/etc/kubernetes/pki/etcd/ca.crt \
           --cert=/etc/kubernetes/pki/etcd/server.crt \
           --key=/etc/kubernetes/pki/etcd/server.key \
           snapshot save /data/etcd-snapshot.db
        
        sudo ETCDCTL_API=3  etcdctl --data-dir=/var/lib/etcd-previous snapshot restore /data/etcd-snapshot-previous.db
        sudo tree /var/lib/etcd-previous/
        
        sudo vi /etc/kubernetes/manifests/etcd.yaml
        ...
          - hostPath:
              path: /var/lib/etcd-previous
              type: DirectoryOrCreate
            name: etcd-data
        
        # cri-docker ë¡œ runtime ìš´ì˜ì‹œ 
        sudo docker ps -a | grep etcd; exit
        
        # containerd ë¡œ ì‹¤í–‰ì‹œ
        sudo ctr -n k8s.io container ls | grep etcd
        
        exit
        exit
        ```


### **â“6. Cluster Upgrade - only Master**

<aside>
ğŸ“Œ í´ëŸ¬ìŠ¤í„° ì—…ê·¸ë ˆì´ë“œëŠ” ë²„ì „ì´ ê³„ì† ë°”ë€ë‹ˆë‹¤. ì‹œí—˜ ë²„ì „ì´ ë°”ë€Œë‹ˆ ì–´ì©”ìˆ˜ ì—†ê² ì£ ? ë¨¼ì € í•™ìŠµ ì˜ìƒì—ì„œ ì—…ê·¸ë ˆì´ë“œë¥¼ ì–´ë–¤ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ëŠ”ì§€, ë˜ dos ë¥¼ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ ë³´ì‹œë©´ ì‚¬ì‹¤ ë²„ì „ì— ì˜í–¥ë°›ì§€ ì•Šê³  ì—…ê·¸ë ˆì´ë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.  docsì˜ ìˆœì„œëŒ€ë¡œ ì§„í–‰í•˜ì„¸ìš”. ë²„ì „ì˜ ìˆ«ìë§Œ ë‹¤ë¥´ê²Œ ë„£ìœ¼ë©´ ì—…ê·¸ë ˆì´ë“œ ë©ë‹ˆë‹¤. 
ê¸°ì¶œë¬¸ì œ í’€ì´ì‹œ ì°¸ì¡° : 2024.03ì›” ì—…ë°ì´íŠ¸í•œ ì‹¤ìŠµ í™˜ê²½ êµ¬ì„± ì™„ë£Œëœ í´ëŸ¬ìŠ¤í„°ì—ëŠ” 1.28.0 ë²„ì „ì´ ì„¤ì¹˜ë˜ì–´ ìˆê³ , ì—…ê·¸ë ˆì´ë“œëŠ” 1.28.2 ë¡œ í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

</aside>

- ì‘ì—… í´ëŸ¬ìŠ¤í„° : kubectl config use-context **k8s**

---

upgrade system : hk8s-master
Given an existing Kubernetes cluster running version `1.28.0`,
upgrade all of the Kubernetes control plane and node components on the master node only to version `1.28.2`.
Be sure to `drain` the `master` node before upgrading it and `uncordon` it after the upgrade.

---

- ë‹µ : 1.28.0 â†’ 1.28.2ë¡œ Ubuntu ê¸°ë°˜ k8s  upgrade

  ### Control-planeì˜ k8s(kubeadm,kubelet,kubectl) ì—…ê·¸ë ˆì´ë“œ

    - Control-plane  upgrade
    - **k8s clusterì˜ Control-planeì„ ë²„ì „ 1.28.0 ì„œ 1.28.2 ë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ì‹œì˜¤.**
    - [https://kubernetes.io/ko/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#ì—…ê·¸ë ˆì´ë“œí• -ë²„ì „-ê²°ì •](https://kubernetes.io/ko/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#%EC%97%85%EA%B7%B8%EB%A0%88%EC%9D%B4%EB%93%9C%ED%95%A0-%EB%B2%84%EC%A0%84-%EA%B2%B0%EC%A0%95)

    ```bash
    ## k8s upgrade
    1. ì—…ê·¸ë ˆì´ë“œ í•  ì‹œìŠ¤í…œ ì ‘ì†
    **ssh k8s-master
    sudo -i
    kubectl get nodes**
    NAME          STATUS   ROLES           AGE   VERSION
    k8s-master    Ready    control-plane   20d   v1.28.0
    k8s-worker1   Ready    <none>          20d   v1.28.0
    k8s-worker2   Ready    <none>          20d   v1.28.0
    
    2. ì—…ê·¸ë ˆì´ë“œí•  ë²„ì „ ê²°ì •ë° í™•ì¸. 1.28.8-00 ë²„ì „ì„ ê²€ìƒ‰
    **apt update
    apt-cache madison kubeadm** 
    
    # ìƒìœ„ ëª‡ê°œ ë¼ì¸ë§Œ í™•ì¸
    **apt-cache madison kubeadm | head**
       kubeadm |  1.28.2-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
       kubeadm |  1.28.1-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
       kubeadm |  1.28.0-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
    	...
    
    [ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ ë…¸ë“œ ì—…ê·¸ë ˆì´ë“œ]
    https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#upgrading-control-plane-nodes
    # 1.28.0-00ì—ì„œ 1.28.2-00ë¥¼ ìµœì‹  íŒ¨ì¹˜ ë²„ì „ìœ¼ë¡œ ë°”ê¾¼ë‹¤.
    1. kubeadm ì—…ê·¸ë ˆì´ë“œ :  kubeadm-1.28.2-00
    **apt-mark unhold kubeadm
    apt-get update
    apt-get install -y kubeadm=1.28.2-00
    apt-mark hold kubeadm
    
    kubeadm version**
    **kubeadm version: &version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.2", GitCommit:"89a4ea3e1e4ddd7f7572286090359983e0387b2f", GitTreeState:"clean", BuildDate:"2023-09-13T09:34:32Z", GoVersion:"go1.20.8", Compiler:"gc", Platform:"linux/amd64"}**
    
    4. master componants ë¥¼ ì—…ê·¸ë ˆì´ë“œ
    **kubeadm upgrade plan**
    ...
    Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
    COMPONENT   CURRENT       TARGET
    kubelet     3 x v1.28.0   v1.28.8
    
    Upgrade to the latest version in the v1.28 series:
    
    COMPONENT                 CURRENT   TARGET
    kube-apiserver            v1.28.7   v1.28.8
    kube-controller-manager   v1.28.7   v1.28.8
    kube-scheduler            v1.28.7   v1.28.8
    kube-proxy                v1.28.7   v1.28.8
    CoreDNS                   v1.10.1   v1.10.1
    etcd                      3.5.9-0   3.5.9-0
    
    You can now apply the upgrade by executing the following command:
    
    	kubeadm upgrade apply v1.28.8
    
    Note: Before you can perform this upgrade, you have to update kubeadm to v1.28.8._____________________________________________________________________
    
    _____________________________________________________________________
    
    # update ê°€ëŠ¥í•œ ë§ˆìŠ¤í„° ì»´í¬ë„ˆíŠ¸ í™•ì¸í•˜ê³  upgrade ì‹¤í–‰
    **kubeadm upgrade apply v1.28.2 -y**
    [upgrade/config] Making sure the configuration is correct:
    [upgrade/config] Reading configuration from the cluster...
    [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
    [preflight] Running pre-flight checks.
    [upgrade] Running cluster health checks
    [upgrade/version] You have chosen to change the cluster version to "v1.28.2"
    [upgrade/versions] Cluster version: v1.28.7
    [upgrade/versions] kubeadm version: v1.28.2
    [upgrade/prepull] Pulling images required for setting up a Kubernetes cluster
    [upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection
    [upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'
    W0316 16:40:55.105464   80167 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
    [upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.28.2" (timeout: 5m0s)...
    [upgrade/etcd] Upgrading to TLS for etcd
    [upgrade/staticpods] Preparing for "etcd" upgrade
    [upgrade/staticpods] Current and new manifests of etcd are equal, skipping upgrade
    [upgrade/etcd] Waiting for etcd to become available
    [upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests522589174"
    [upgrade/staticpods] Preparing for "kube-apiserver" upgrade
    [upgrade/staticpods] Renewing apiserver certificate
    [upgrade/staticpods] Renewing apiserver-kubelet-client certificate
    [upgrade/staticpods] Renewing front-proxy-client certificate
    [upgrade/staticpods] Renewing apiserver-etcd-client certificate
    [upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-03-16-16-40-55/kube-apiserver.yaml"
    [upgrade/staticpods] Waiting for the kubelet to restart the component
    [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
    [apiclient] Found 1 Pods for label selector component=kube-apiserver
    [upgrade/staticpods] Component "kube-apiserver" upgraded successfully!
    [upgrade/staticpods] Preparing for "kube-controller-manager" upgrade
    [upgrade/staticpods] Renewing controller-manager.conf certificate
    [upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-03-16-16-40-55/kube-controller-manager.yaml"
    [upgrade/staticpods] Waiting for the kubelet to restart the component
    [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
    [apiclient] Found 1 Pods for label selector component=kube-controller-manager
    [upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!
    [upgrade/staticpods] Preparing for "kube-scheduler" upgrade
    [upgrade/staticpods] Renewing scheduler.conf certificate
    [upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-03-16-16-40-55/kube-scheduler.yaml"
    [upgrade/staticpods] Waiting for the kubelet to restart the component
    [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
    [apiclient] Found 1 Pods for label selector component=kube-scheduler
    [upgrade/staticpods] Component "kube-scheduler" upgraded successfully!
    [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
    [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
    [upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config1418164023/config.yaml
    [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
    [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
    [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
    [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
    [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
    [addons] Applied essential addon: CoreDNS
    [addons] Applied essential addon: kube-proxy
    
    [upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.28.2". Enjoy!
    
    [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
    
    5. ë…¸ë“œ ë“œë ˆì¸ : consoleì´ë‚˜ control-plane(master)ì—ì„œ ì‹¤í–‰
    **kubectl drain k8s-master --ignore-daemonsets** 
    # ì‚­ì œì‹œ coredns delete pending ë°œìƒì‹œ ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ 
    # kubectl delete deployment coredns -n kube-system
    # kubectl delete pod --force -n kube-system coredns-xxx-XXX
    # ëª…ë ¹ìœ¼ë¡œ ìˆ˜ë™ ì‚­ì œ ì§€ì› í•„ìš” 
    
    **kubectl get nodes**
    NAME          STATUS                     ROLES           AGE   VERSION
    k8s-master    Ready,SchedulingDisabled   control-plane   20d   v1.28.0
    k8s-worker1   Ready                      <none>          20d   v1.28.0
    k8s-worker2   Ready                      <none>          20d   v1.28.0
    
    6. kubeletê³¼ kubectl ì—…ê·¸ë ˆì´ë“œ
    **apt-mark unhold kubelet kubectl 
    apt-get update
    apt-get install -y kubelet=1.28.2-00 kubectl=1.28.2-00**
    **apt-mark hold kubelet kubectl**
    
    **systemctl daemon-reload
    systemctl restart kubelet**
    
    7. ë…¸ë“œ uncordon
    **kubectl uncordon k8s-master**
    
    kubectl get nodes
    NAME          STATUS   ROLES           AGE   VERSION
    **k8s-master    Ready    control-plane   20d   v1.28.2**
    k8s-worker1   Ready    <none>          20d   v1.28.0
    k8s-worker2   Ready    <none>          20d   v1.28.0
    
    **exit
    exit**
    ```


---

- â—OLD Version : Answer

    ```bash
    # kubeadm ì—…ê·¸ë ˆì´ë“œ
    sudo yum install -y kubeadm-1.25.3-0 --disableexcludes=kubernetes
    kubeadm version
    
    # node components ì—…ê·¸ë ˆì´ë“œ
    sudo kubeadm upgrade plan v1.25.3
    sudo kubeadm upgrade apply v1.25.3
    
    # ë…¸ë“œ ë“œë ˆì¸
    kubectl drain hk8s-m --ignore-daemonsets 
    
    # kubeletê³¼ kubectl ì—…ê·¸ë ˆì´ë“œ
    sudo yum install -y kubelet-1.25.3-0 kubectl-1.25.3-0 --disableexcludes=kubernetes
    sudo systemctl daemon-reload
    sudo systemctl restart kubelet
    
    # ë…¸ë“œ uncordon
    sudo kubectl uncordon hk8s-m
    ```


### **â“7.** Authentication and Authorization

- Cluster : **k8s**

---

Context You have been asked to create a new ClusterRole for a deployment pipeline and bind it to a specific ServiceAccount scoped to a specific namespace.
**Task:**
- Create a new ClusterRole named `deployment-clusterrole`, which only allows to **create** the following resource types: **Deployment StatefulSet DaemonSet**
- Create a new ServiceAccount named `cicd-token` in the existing namespace `app-team1`.
- Bind the new ClusterRole `deployment-clusterrole` to the new ServiceAccount `cicd-token`, limited to the namespace `app-team1`.

---

- â—Answer

    ```bash
    kubectl config use-context k8s
    
    kubectl create **clusterrole** deployment-clusterrole --verb=create --resource=deployment,statefulset,daemonset
    kubectl get clusterrole deployment-clusterrole
    
    kubectl create **serviceaccount** cicd-token --namespace=app-team1
    kubectl get serviceaccounts --namespace app-team1
    
    kubectl create **clusterrolebinding** **deployment-clusterrolebinding** --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cide-token --namespace=app-team1 
    kubectl describe clusterrolebindings deployment-clusterrolebinding
    ```


### **â“8.** Pod ìƒì„±í•˜ê¸°

- ì‘ì—… í´ëŸ¬ìŠ¤í„° : kubectl config use-context **k8s**

---

Create a new namespace and create a pod in the namespace

TASK:

- namespace name: cka-exam
- pod Name: pod-01
- image: busybox
- environment Variable:  CERT = "CKA-cert"
- command: /bin/sh
- args: -c "while true; do echo $(CERT); sleep 10;done"

---

- â—Answer

    ```bash
    kubectl create namespace cka-exam
    kubectl get namespaces cka-exam 
    
    kubectl run pod-01 --image=busybox  --dry-run=client -o yaml > pod-01.yaml 
    vi pod-01.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-01
      namespace: cka-exam
    spec:
      containers:
      - env:
        - name: CERT
          value: CKA-cert
        image: busybox
        name: pod-01
        command: ["/bin/sh"]
        args: ["-c", "while true; do echo $(CERT); sleep 10;done"]
    
    kubectl apply -f pod-01.yaml
    kubectl get pod -n cka-exam
    ```


### **â“9.** multi-container Pod ìƒì„±

- cluster :  kubectl config use-context **hk8s**
- Create a pod with 4 containers running : nginx, redis and memcached
    - pod name: eshop-frontend
    - image: nginx
    - image: redis
    - image: memcached

- â—Answer

    ```bash
    kubectl run eshop-frontend --image=nginx  --dry-run=client -o yaml > multi.yaml
    
    vi multi.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: eshop-frontend
    spec:
      containers:
      - image: nginx
        name: nginx
      - image: redis
        name: redis
      - image: memcached
        name: memcached
    
    kubectl apply -f multi.yaml
    ```


### **â“10.** Side-car Container Pod ì‹¤í–‰

- ì‘ì—… í´ëŸ¬ìŠ¤í„° :  kubectl config use-context **k8s**
- An existing Pod needs to be integrated into the `Kubernetes built-in logging architecture` (e.g. `kubectl logs`).
- Adding a streaming sidecar container is a good and common way to accomplish this requirement.
- Task:
    - Add a sidecar container named `sidecar`, using the busybox Image, to the existing Pod `eshop-cart-app`.
    - The new `sidecar` container has to run the following `command: /bin/sh -c "tail -n+1 -f /var/log/cart-app.log"`
    - Use a Volume, mounted at `/var/log`, to make the log file `cart-app.log` available to the `sidecar` container.
    - Don't modify the `cart-app`.
- â—Answer

    ```bash
    kubectl get pod eshop-cart-app -o yaml > sidecar.yaml
    cat sidecar.yaml 
    apiVersion: v1
    kind: Pod
    metadata:
      name: eshop-cart-app
    spec:
      containers:
      - image: busybox
        name: cart-app
        command: ['/bin/sh', '-c', 'i=1;while :;do  echo -e "$i: Price: $((RANDOM % 10000 + 1))" >> /var/log**/cart-app.log**; i=$((i+1)); sleep 2; done']
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      volumes:
      - emptyDir: {}
        name: varlog
    
    **vi sidecar.yaml**
    apiVersion: v1
    kind: Pod
    metadata:
      name: eshop-cart-app
    spec:
      containers:
      - image: busybox
        name: cart-app
        command: ['/bin/sh', '-c', 'i=1;while :;do  echo -e "$i: Price: $((RANDOM % 10000 + 1))" >> /var/log/cart-app.log; i=$((i+1)); sleep 2; done']
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      **- name: price
        image: busybox
        command: [/bin/sh, -c, "tail -n+1 -f /var/log/cart-app.log"]
        volumeMounts:
        - name: varlog
          mountPath: /var/log**
      volumes:
      - emptyDir: {}
        name: varlog
    
    **kubectl delete pod eshop-cart-app
    kubectl apply -f sidecar.yaml**
    ```


### â“11. Pod Scale-out

- Cluster: kubectl config use-contex **k8s**

---

Expand the number of running Pods in "**eshop-order**" to 5.

- namespace: devops
- deployment: eshop-order
- replicas: **5**

---

- â—Answer

    ```bash
    kubectl get deployment -n devops
    kubectl get deployment eshop-order -n devops
    kubectl **scale** deployment eshop-order --replicas=5 --namespace devops
    ```


### **â“12. Rolling Update**

- Cluster: kubectl config use-context **k8s**

---

**Create** a deployment as follows:

- TASK:
    - name: nginx-app
    - Using container nginx with version 1.11.10-alpine
    - The deployment should contain 3 replicas
- Next, deploy the application with new version **1.11.13-alpine**, by performing a **rolling update**
- Finally, rollback that update to the **previous version** 1.11.10-alpine

---

- â—Answer

    ```bash
    # create
    kubectl create deployment  nginx-app --image=nginx:1.11.10-alpine --replicas=3 --dry-run=client -o yaml > deplyment.yaml
    cat deplyment.yaml 
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-app
      name: nginx-app
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: nginx-app
      strategy: {}
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: nginx-app
        spec:
          containers:
          - image: nginx:1.11.10-alpine
            name: nginx
            resources: {}
    status: {}
    
    kubectl apply -f deplyment.yaml --record
    
    **# rolling update**
    kubectl set image deployment nginx-app **nginx=nginx:1.11.13-alpine** **--record  # --record ì˜µì…˜ì€ ìƒëµ ê°€ëŠ¥í•©ë‹ˆë‹¤. docsë¥¼ ë³´ë©´ì„œ ì§„í–‰í•˜ì„¸ìš”.**
    kubectl rollout history deployment **nginx-app
    
    # roll-back**
    kubectl rollout undo deployment nginx-app
    kubectl rollout history deployment **nginx-app**
    ```


### **â“13.** Network Policy with Namespace

- ì‘ì—… í´ëŸ¬ìŠ¤í„° : kubectl config use-context **k8s**

---

**Creat**e a new **NetworkPolicy** named allow-port-from-namespace in the existing **namespace devops**.

Ensure that the new NetworkPolicy allows Pods in namespace migops(using label **team=migops**) to connect to port 80 of Pods in namespace devops.

Further ensure that the new NetworkPolicy: does not allow access to Pods, which don't listen on port 80 does not allow access from Pods, which are not in namespace migops

---

- â—Answer

    ```bash
    **kubectl get deployment -n devops**
    devops            Active   45d   kubernetes.io/metadata.name=devops,**team=devops**
    migops            Active   45d   kubernetes.io/metadata.name=migops,**team=migops**
    presales          Active   45d   kubernetes.io/metadata.name=presales,team=presales
    
    **vi policy.yaml**
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: allow-port-from-namespace
      **namespace: devops**
    spec:
      **podSelector: {}**   
      policyTypes:
      - Ingress
      ingress:
      - from:
        - namespaceSelector:
            matchLabels:
              team: migops
        ports:
        - protocol: TCP
          port:80
    
    **kubectl apply -f policy.yaml**
    ```


### **â“14. Create a persistent volume**

- Cluster: kubectl config use-context **k8s**

---

**Create a persistent volume** with name app-config, of capacity 1Gi and access mode ReadWriteMany.
The type of volume is hostPath and its location is /var/app-config.

---

- â—Answer

    ```bash
    vi pv.yaml
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: app-config
    spec:
      capacity:
        storage: 1Gi
      accessModes:
      - ReadWriteMany
      hostPath:
        path: /var/app-config
    
    kubectl apply -f pv.yaml
    kubectl get pv
    ```


### **â“15. Deploy and Service**

- ì‘ì—… í´ëŸ¬ìŠ¤í„° : kubectl config use-context **k8s**

---

Reconfigure the **existing** deployment `front-end` and add a **port specification named** `http` exposing port `80/tcp` of the existing container nginx.
Create a new service named `front-end-svc` exposing the container port http.
Configure the new service to also expose the individual Pods via a `NodePort` on the nodes on which they are scheduled

---

- â—**Answer**

    ```bash
    kubectl get deploy front-end -o yaml > **front-end.yaml**
    
    cat > front-end.yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: front-end
    spec:
      selector:
        matchLabels:
          app: front-end
      replicas: 2
      template:
        metadata:
          labels:
            app: front-end
        spec:
          containers:
          - name: http
            image: nginx
            ports:
            - name: http 
              containerPort: 80
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: front-end-svc
    spec:
      type: NodePort
      ports:
      - port: 80
        protocol: TCP
        targetPort: http
      selector:
        app: front-end
    
    kubectl delete deploy front-end
    kubectl apply -f front-end.yaml
    kubectl get deployments.apps,svc
    ```


### **â“16.** DNS Lookup

- ì‘ì—… í´ëŸ¬ìŠ¤í„° : kubectl config use-context **k8s**

---

**Create a nginx pod** called nginx-resolver using image nginx, expose it internally with a service called nginx-resolver-service.
Test that you are able to look up the service and pod names from within the cluster. Use the image: busybox:1.28 for dns lookup.
- Record results in /var/CKA2022/nginx.svc and  /var/CKA2022/nginx.pod
- Pod: nginx-resolver created
- Service DNS Resolution recorded correctly
- Pod DNS resolution recorded correctly

---

- â—**Answer**

    ```bash
    #**Create a nginx pod**
    **kubectl run nginx-resolver --image=nginx --port=80
    kubectl expose pod nginx-resolver --name nginx-resolver-service --port=80 
    kubectl get svc nginx-resolver-service** 
    	nginx-resolver-service   10.104.150.11
    # Test:Service DNS Resolution recorded correctly -> /var/CKA2022/nginx.svc
    **kubectl run test-nslookup --image=busybox:1.28 -it --restart=Never --rm -- nslookup  10.104.150.11 
    kubectl run test-nslookup --image=busybox:1.28 -it --restart=Never --rm -- nslookup  nginx-resolver-service > /var/CKA2022/nginx.svc**
    
    # Test: Pod DNS resolution recorded correctly -> /var/CKA2022/nginx.pod
    kubectl get pod ngnix-resolver -o wide
          nginx-resolver   **10.244.1.55**
    kubectl run test-nslookup --image=busybox:1.28 -it --restart=Never --rm -- nslookup  **10-244-1-55.default.pod.cluster.local** > **/var/CKA2022/nginx.pod**
    
    cat /var/CKA2022/nginx.svc
    cat /var/CKA2022/nginx.pod
    ```


### **â“17.** Application with PVC

- Cluster: kubectl config use-context **k8s**

---

Create a new PersistentVolumeClaim:

Name: pv-volume

Class: csi-hostpath-sc

Capacity: 10Mi

Create a new Pod which mounts the PersistentVolumeClaim as a volume:

Name: web-server

Image: nginx

Mount path: /usr/share/nginx/html

Configure the new Pod to have ReadWriteOnce access on the volume.

---

- â—Answer

    ```bash
    vi pvc.yaml
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: pv-volume
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 10Mi
      storageClassName: csi-hostpath-sc
    
    kubectl apply -f pvc.yaml
    kubectl get pv,pvc
    
    vi pod-with-pvc.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: web-server
    spec:
      containers:
        - name: web-server
          image: nginx
          volumeMounts:
          - mountPath: "/usr/share/nginx/html"
            name: pv-volume
      volumes:
        - name: pv-volume
          persistentVolumeClaim:
            claimName: pv-volume
    
    kubectl **apply -f pod-with-pvc**
    kubectl get pod
    ```