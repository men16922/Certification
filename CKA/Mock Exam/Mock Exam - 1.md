[TOC]

# Q1
Create a Pod `mc-pod` in the `mc-namespace` namespace with three containers. The first container should be named `mc-pod-1`, run the `nginx:1-alpine` image, and set an environment variable `NODE_NAME` to the node name. The second container should be named `mc-pod-2`, run the `busybox:1` image, and continuously log the output of the `date` command to the file `/var/log/shared/date.log` every second. The third container should have the name `mc-pod-3`, run the image `busybox:1`, and print the contents of the `date.log` file generated by the second container to stdout. Use a shared, non-persistent volume.

```shell
kubectl run mc-pod --image=nginx:1-alpine --dry-run=client -o yaml > q1.yaml
vi q1.yaml
kubectl apply -f q1.yaml
kubectl get po -n mc-namespace

```
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mc-pod
  namespace: mc-namespace
spec:
  containers:
    - image: nginx:1-alpine
      name: mc-pod-1
      env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
    - image: busybox:1
      name: mc-pod-2
      command: ['sh', '-c', 'while true; do date >> /var/log/shared/date.log; sleep 1; done']
      volumeMounts:
        - name: shared-volume
          mountPath: /var/log/shared
    - image: busybox:1
      name: mc-pod-3
      command: ['sh', '-c', 'tail -f /var/log/shared/date.log']
      volumeMounts:
        - name: shared-volume
          mountPath: /var/log/shared
  volumes:
    - name: shared-volume
      emptyDir: {}
```
# Q2
This question needs to be solved on node node01. To access the node using SSH, use the credentials below:

username: bob
password: caleston123

As an administrator, you need to prepare node01 to install kubernetes. One of the steps is installing a container runtime. Install the cri-docker_0.3.16.3-0.debian.deb package located in /root and ensure that the cri-docker service is running and enabled to start on boot.

```shell
ssh bob@node01
sudo dpkg -i /root/cri-docker_0.3.16.3-0.debian.deb
sudo systemctl enable cri-docker
sudo systemctl start cri-docker
sudo systemctl status cri-docker
```

# Q3
On controlplane node, identify all CRDs related to VerticalPodAutoscaler and save their names into the file /root/vpa-crds.txt.

```shell
kubectl get crds | grep verticalpodautoscaler | awk '{print $1}' > /root/vpa-crds.txt
```

# Q4
Create a service messaging-service to expose the messaging application within the cluster on port 6379.

Use imperative commands.
Service: messaging-service

Port: 6379

Type: ClusterIp

Use the right labels

```shell
kubectl expose pod messaging --name=messaging-service --port 6379
```

# Q5
Create a deployment named hr-web-app using the image kodekloud/webapp-color with 2 replicas.


Name: hr-web-app

Image: kodekloud/webapp-color

Replicas: 2

```shell
kubectl create deploy hr-web-app --image=kodekloud/webapp-color --replicas=2
```

# Q6
A new application orange is deployed. There is something wrong with it. Identify and fix the issue.

```shell
kubectl get po orange -o yaml > q6.yaml
vi q6.yaml
kubectl delete po orange
kubectl apply -f q6.yaml
k describe po orange
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: orange
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    name: orange-container
  initContainers:
  - command:
    - sh
    - -c
    - sleep 2;
    image: busybox
    name: init-myservice
```
# Q7
Expose the hr-web-app created in the previous task as a service named hr-web-app-service, accessible on port 30082 on the nodes of the cluster.


The web application listens on port 8080.

Name: hr-web-app-service

Type: NodePort

Endpoints: 2

Port: 8080

NodePort: 30082

```shell
kubectl expose deploy hr-web-app --name=hr-web-app-service --port 8080 --dry-run=client -o yaml > q7.yaml
vi q7.yaml
kubectl apply -f q7.yaml
kubectl get endpoints hr-web-app-service
```

```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hr-web-app
  name: hr-web-app-service
spec:
  type: NodePort
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
    nodePort: 30082
  selector:
    app: hr-web-app
```

# Q8
Create a Persistent Volume with the given specification: -

Volume name: pv-analytics

Storage: 100Mi

Access mode: ReadWriteMany

Host path: /pv/data-analytics


Is the volume name set?

Is the storage capacity set?

Is the accessMode set?

Is the hostPath set?

```shell
vi q8.yaml
kubectl apply -f q8.yaml
```
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-analytics
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: "/pv/data-analytics"
```

# Q9
Create a Horizontal Pod Autoscaler (HPA) with name webapp-hpa for the deployment named kkapp-deploy in the default namespace with the webapp-hpa.yaml file located under the root folder.
Ensure that the HPA scales the deployment based on CPU utilization, maintaining an average CPU usage of 50% across all pods.
Configure the HPA to cautiously scale down pods by setting a stabilization window of 300 seconds to prevent rapid fluctuations in pod count.

Note: The kkapp-deploy deployment is created for backend; you can check in the terminal.


Is the HPA webapp-hpa deployed?

Is the deployment configured for metrics CPU Utilization?

Is the stabilization window set to 300 seconds?

```shell
vi webapp-hpa.yaml
kubectl apply -f webapp-hpa.yaml
```

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kkapp-deploy
  minReplicas: 2
  maxReplicas: 10
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

# Q10
Deploy a Vertical Pod Autoscaler (VPA) with name analytics-vpa for the deployment named analytics-deployment in the default namespace.
The VPA should automatically adjust the CPU and memory requests of the pods to optimize resource utilization. Ensure that the VPA operates in Auto mode, allowing it to evict and recreate pods with updated resource requests as needed.


Is the VPA analytics-vpa created for deployment analytics-deployment?

Is the updatePolicy set to Auto mode for deployment analytics-deployment?

```shell
vi analytics-vpa.yaml
kubectl apply -f analytics-vpa.yaml
```

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: analytics-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment
    name:       analytics-deployment
  updatePolicy:
    updateMode: "Auto"
```

# Q11
Create a Kubernetes Gateway resource with the following specifications:

Name: web-gateway
Namespace: nginx-gateway
Gateway Class Name: nginx
Listeners:
Protocol: HTTP
Port: 80
Name: http

```shell
vi q11.yaml
kubectl apply -f q11.yaml
```

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: web-gateway
  namespace: nginx-gateway
spec:
  gatewayClassName: nginx
  listeners:
  - name: http
    protocol: HTTP
    port: 80
```

# Q12
One co-worker deployed an nginx helm chart kk-mock1 in the kk-ns namespace on the cluster. A new update is pushed to the helm chart, and the team wants you to update the helm repository to fetch the new changes.


After updating the helm chart, upgrade the helm chart version to 18.1.15.



Is the deployment running?

Is the chart version upgraded?

```shell
helm ls -A
helm repo ls
helm repo update kk-mock1 -n kk-ns
helm search repo kk-mock1/nginx -n kk-ns -l | head -n30
helm upgrade kk-mock1 kk-mock1/nginx -n kk-ns --version=18.1.15
helm ls -n kk-ns
```