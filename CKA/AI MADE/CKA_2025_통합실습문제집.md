# CKA 2025 통합 실습 문제집

## 🎯 KillerCoda Playground 실습 가이드

**사용법:**
1. **KillerCoda Kubernetes Playground** 접속: https://killercoda.com/playgrounds/scenario/kubernetes
2. 각 문제의 **환경 설정** 명령어를 실행하여 문제 상황 생성
3. **문제**를 읽고 해결 시도
4. **솔루션**을 참고하여 검증5

---

## 📋 목차

### Part 1: 기본 실습 (1-10번)
- [CNI 설치](#문제-1-cni-설치)
- [Helm 템플릿 생성](#문제-2-helm-템플릿-생성)
- [HPA 구성](#문제-3-hpa-구성)
- [Sidecar 컨테이너](#문제-4-sidecar-컨테이너)
- [PriorityClass 구성](#문제-5-priorityclass-구성)
- [Pod Security Admission](#문제-6-pod-security-admission)
- [Taint와 Toleration](#문제-7-taint와-toleration)
- [NetworkPolicy 구현](#문제-8-networkpolicy-구현)
- [StorageClass 구성](#문제-9-storageclass-구성)
- [JSONPath 쿼리](#문제-10-jsonpath-쿼리)

### Part 2: 고급 실습 (11-20번)
- [Job과 CronJob 관리](#문제-11-job과-cronjob-관리)
- [DaemonSet과 노드 관리](#문제-12-daemonset과-노드-관리)
- [Multi-Container Pod](#문제-13-multi-container-pod)
- [복잡한 애플리케이션 디버깅](#문제-14-복잡한-애플리케이션-디버깅)
- [클러스터 성능 최적화](#문제-15-클러스터-성능-최적화)
- [서비스 메시 및 로드 밸런싱](#문제-16-서비스-메시-및-로드-밸런싱)
- [멀티 테넌트 보안 구성](#문제-17-멀티-테넌트-보안-구성)
- [인증서 관리 및 TLS 구성](#문제-18-인증서-관리-및-tls-구성)
- [클러스터 백업 및 재해 복구](#문제-19-클러스터-백업-및-재해-복구)
- [모니터링 및 로깅 구성](#문제-20-모니터링-및-로깅-구성)

---

## Part 1: 기본 실습

### 문제 1: CNI 설치

#### 📋 문제
NetworkPolicy 지원이 필요한 CNI를 설치하세요. Calico와 Flannel 중 적절한 것을 선택하여 설치하고 Pod 간 통신을 테스트하세요.

**요구사항:**
- NetworkPolicy 지원 필수
- Pod 간 통신 가능
- 매니페스트 파일 사용 (Helm 사용 금지)

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 기존 CNI 제거 (있다면)
kubectl delete -f https://github.com/flannel-io/flannel/releases/download/v0.26.1/kube-flannel.yml --ignore-not-found

# 노드 상태 확인 (CNI 없으면 NotReady)
kubectl get nodes

echo "CNI가 제거되었습니다. NetworkPolicy를 지원하는 CNI를 설치하세요."
```

#### ✅ 솔루션

```bash
# 1. NetworkPolicy 지원 여부 확인
# Calico: 지원 O, Flannel: 지원 X
# 따라서 Calico 선택

# 2. Calico 설치 (create 사용!)
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.2/manifests/tigera-operator.yaml

# 3. Calico 설치 확인
kubectl get pods -n calico-system
kubectl get pods -n tigera-operator

# 4. 노드 상태 확인
kubectl get nodes

# 5. 테스트 Pod 생성
kubectl run test1 --image=busybox:1.35 --restart=Never -- sleep 3600
kubectl run test2 --image=busybox:1.35 --restart=Never -- sleep 3600

# 6. Pod 간 통신 테스트
kubectl exec test1 -- ping -c 4 $(kubectl get pod test2 -o jsonpath='{.status.podIP}')

# 7. NetworkPolicy 테스트
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-netpol
spec:
  podSelector:
    matchLabels:
      run: test2
  policyTypes:
  - Ingress
  ingress: []  # 모든 트래픽 차단
EOF

# 8. 차단된 통신 테스트 (실패해야 함)
kubectl exec test1 -- ping -c 2 $(kubectl get pod test2 -o jsonpath='{.status.podIP}') || echo "Traffic blocked by NetworkPolicy (expected)"

# 9. 정리
kubectl delete networkpolicy test-netpol
kubectl delete pod test1 test2

echo "Calico CNI 설치 및 NetworkPolicy 테스트 완료"
```

---

### 문제 2: Helm 템플릿 생성

#### 📋 문제
ArgoCD Helm Chart의 템플릿을 생성하세요. CRD 설치는 비활성화하고 템플릿을 파일로 저장하세요.

**요구사항:**
- Chart 버전: 7.7.3
- Namespace: argocd
- CRD 설치 비활성화
- 템플릿을 ~/argo-helm.yaml로 저장

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# Helm 설치 확인
helm version

# ArgoCD CRD가 이미 설치되어 있다고 가정
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/crds/application-crd.yaml
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/crds/applicationset-crd.yaml

echo "ArgoCD CRD가 설치되었습니다. Helm 템플릿을 생성하세요."
```

#### ✅ 솔루션

```bash
# 1. Helm repository 추가
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

# 2. 사용 가능한 차트 버전 확인
helm search repo argo/argo-cd --versions | head -10

# 3. 템플릿 생성 (CRD 설치 비활성화)
helm template argocd argo/argo-cd --version 7.7.3 -n argocd \
  --set crds.install=false > ~/argo-helm.yaml

# 4. 생성된 템플릿 확인
ls -la ~/argo-helm.yaml
head -20 ~/argo-helm.yaml

# 5. 템플릿 내용 검증
grep -c "kind:" ~/argo-helm.yaml
grep "CustomResourceDefinition" ~/argo-helm.yaml || echo "No CRDs found (expected)"

# 6. 네임스페이스 생성 후 적용 테스트 (선택사항)
kubectl create namespace argocd
kubectl apply -f ~/argo-helm.yaml

# 7. 설치된 리소스 확인
kubectl get all -n argocd

echo "Helm 템플릿 생성 완료: ~/argo-helm.yaml"
```

---

### 문제 3: HPA 구성

#### 📋 문제
Horizontal Pod Autoscaler를 구성하여 CPU 사용량에 따라 자동으로 Pod 수를 조절하세요.

**요구사항:**
- CPU 사용률 70% 기준
- 최소 2개, 최대 10개 Pod
- 스케일 다운 안정화 시간 300초
- 부하 테스트로 동작 확인

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# HPA 테스트 네임스페이스 생성
kubectl create namespace hpa-test

# Metrics Server 설치 (HPA 동작을 위해 필요)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Metrics Server 설정 수정 (KillerCoda 환경용)
kubectl patch deployment metrics-server -n kube-system --type='json' \
  -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'

echo "HPA 테스트 환경이 설정되었습니다. Metrics Server가 준비될 때까지 잠시 기다리세요."
```

#### ✅ 솔루션

```bash
# 1. 테스트 애플리케이션 배포
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: hpa-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: web
        image: nginx:1.21
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        ports:
        - containerPort: 80
EOF

# 2. 서비스 생성
kubectl expose deployment web-app --name=web-app-service --port=80 -n hpa-test

# 3. HPA 생성
cat <<EOF | kubectl apply -f -
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
  namespace: hpa-test
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
EOF

# 4. HPA 상태 확인
kubectl get hpa -n hpa-test
kubectl describe hpa web-app-hpa -n hpa-test

# 5. 부하 테스트 실행
kubectl run load-generator --image=busybox:1.35 -n hpa-test --rm -it --restart=Never -- sh -c "
while true; do
  wget -q -O- http://web-app-service.hpa-test.svc.cluster.local
  sleep 0.01
done
" &

# 6. HPA 동작 모니터링
kubectl get hpa -n hpa-test -w &
sleep 120

# 7. 스케일링 결과 확인
kubectl get pods -n hpa-test
kubectl get hpa -n hpa-test

# 8. 부하 테스트 중지
pkill -f "wget.*web-app-service"

echo "HPA 구성 및 테스트 완료"
```

---

### 문제 4: Sidecar 컨테이너

#### 📋 문제
기존 deployment에 로그 수집용 sidecar 컨테이너를 추가하세요.

**요구사항:**
- 메인 컨테이너: nginx
- Sidecar 컨테이너: busybox (로그 파일 tail)
- 공유 볼륨을 통한 로그 파일 공유
- Sidecar에서 로그 출력 확인

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# Sidecar 테스트 네임스페이스 생성
kubectl create namespace sidecar-test

echo "Sidecar 테스트 환경이 설정되었습니다."
```

#### ✅ 솔루션

```bash
# 1. Sidecar가 포함된 deployment 생성
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-server
  namespace: sidecar-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app-server
  template:
    metadata:
      labels:
        app: app-server
    spec:
      containers:
      - name: app
        image: nginx:1.21
        ports:
        - containerPort: 80
        command:
        - sh
        - -c
        - |
          # 로그 파일 생성 및 nginx 시작
          touch /var/log/app.log
          while true; do
            echo "\$(date): Application log entry" >> /var/log/app.log
            sleep 5
          done &
          nginx -g 'daemon off;'
        volumeMounts:
        - name: log-volume
          mountPath: /var/log
      - name: log-sidecar
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          # 로그 파일이 생성될 때까지 대기
          while [ ! -f /var/log/app.log ]; do
            echo "Waiting for log file..."
            sleep 2
          done
          echo "Starting log collection..."
          tail -f /var/log/app.log
        volumeMounts:
        - name: log-volume
          mountPath: /var/log
      volumes:
      - name: log-volume
        emptyDir: {}
EOF

# 2. Pod 상태 확인
kubectl get pods -n sidecar-test

# 3. Sidecar 컨테이너 로그 확인
kubectl logs -n sidecar-test -l app=app-server -c log-sidecar -f &
sleep 30

# 4. Pod 내부에서 로그 파일 확인
kubectl exec -n sidecar-test deployment/app-server -c app -- ls -la /var/log/
kubectl exec -n sidecar-test deployment/app-server -c app -- tail -5 /var/log/app.log

echo "SideCar 컨테이너 구성 완료"
```

---

### 문제 5: PriorityClass 구성

#### 📋 문제
높은 우선순위를 가진 PriorityClass를 생성하고, 기존 deployment에 적용하여 리소스 부족 시 우선 스케줄링되도록 설정하세요.

**요구사항:**
- PriorityClass 이름: high-priority
- 우선순위 값: 1000
- 기존 deployment에 적용
- 우선순위 동작 확인

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 네임스페이스 생성
kubectl create namespace priority-test

# 테스트용 deployment 생성
kubectl create deployment test-app --image=nginx:1.21 --replicas=2 -n priority-test

echo "우선순위 테스트 환경이 설정되었습니다."
```

#### ✅ 솔루션

```bash
# 1. 높은 우선순위 클래스 생성
kubectl create priorityclass high-priority --value=1000 --description="High priority class for critical workloads"

# 2. PriorityClass 확인
kubectl describe priorityclass high-priority

# 3. Deployment에 우선순위 적용
kubectl patch deployment test-app -n priority-test -p '{"spec":{"template":{"spec":{"priorityClassName":"high-priority"}}}}'

# 4. 우선순위 적용 확인
kubectl get pods -n priority-test -o custom-columns=NAME:.metadata.name,PRIORITY:.spec.priorityClassName,STATUS:.status.phase

# 5. Pod 상세 정보에서 우선순위 확인
kubectl describe pod -n priority-test -l app=test-app | grep -A 5 "Priority"

echo "PriorityClass 구성 완료"
```

---

### 문제 6: Pod Security Admission

#### 📋 문제
네임스페이스에 Pod Security Standards를 적용하여 보안 정책을 강화하세요.

**요구사항:**
- restricted 정책을 enforce 모드로 적용
- 정책 위반 Pod 생성 시도
- 정책을 준수하는 보안 Pod 생성

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 보안 테스트 네임스페이스 생성
kubectl create namespace security-test

echo "보안 테스트 환경이 설정되었습니다."
```

#### ✅ 솔루션

```bash
# 1. Pod Security Standards 적용
kubectl label namespace security-test \
  pod-security.kubernetes.io/enforce=restricted \
  pod-security.kubernetes.io/enforce-version=latest \
  pod-security.kubernetes.io/audit=restricted \
  pod-security.kubernetes.io/audit-version=latest \
  pod-security.kubernetes.io/warn=restricted \
  pod-security.kubernetes.io/warn-version=latest

# 2. 네임스페이스 라벨 확인
kubectl get namespace security-test --show-labels

# 3. 정책 위반 Pod 생성 시도 (실패해야 함)
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: privileged-pod
  namespace: security-test
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    securityContext:
      privileged: true  # 이것은 거부되어야 함
EOF

# 4. 정책을 준수하는 보안 Pod 생성
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
  namespace: security-test
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: nginx
    image: nginx:1.21
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      runAsNonRoot: true
      runAsUser: 1000
      capabilities:
        drop:
        - ALL
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi
    volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: var-cache
      mountPath: /var/cache/nginx
    - name: var-run
      mountPath: /var/run
  volumes:
  - name: tmp
    emptyDir: {}
  - name: var-cache
    emptyDir: {}
  - name: var-run
    emptyDir: {}
EOF

# 5. Pod 상태 확인
kubectl get pods -n security-test

# 6. 보안 컨텍스트 확인
kubectl describe pod secure-pod -n security-test | grep -A 20 "Security Context"

echo "Pod Security Admission 구성 완료"
```

---

### 문제 7: Taint와 Toleration

#### 📋 문제
노드에 taint를 설정하고, 특정 Pod만 해당 노드에 스케줄될 수 있도록 toleration을 구성하세요.

**요구사항:**
- 노드에 maintenance taint 설정
- Toleration이 있는 Pod와 없는 Pod 생성
- 스케줄링 동작 확인

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 테스트 네임스페이스 생성
kubectl create namespace taint-test

# 현재 노드 상태 확인
kubectl get nodes
kubectl describe nodes | grep -i taint

echo "Taint와 Toleration 테스트 환경이 설정되었습니다."
```

#### ✅ 솔루션

```bash
# 1. 노드에 taint 설정
NODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')
kubectl taint node $NODE_NAME maintenance=true:NoSchedule

# 2. Taint 설정 확인
kubectl describe node $NODE_NAME | grep -i taint

# 3. Toleration 없는 Pod 생성 (스케줄링 실패해야 함)
kubectl run no-toleration-pod --image=nginx:1.21 -n taint-test

# 4. Pod 상태 확인 (Pending 상태여야 함)
kubectl get pods -n taint-test
kubectl describe pod no-toleration-pod -n taint-test | grep -A 5 Events

# 5. Toleration이 있는 Pod 생성
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: toleration-pod
  namespace: taint-test
spec:
  tolerations:
  - key: "maintenance"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  containers:
  - name: nginx
    image: nginx:1.21
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
EOF

# 6. Toleration Pod 상태 확인 (Running 상태여야 함)
kubectl get pods -n taint-test -o wide

# 7. Taint 제거
kubectl taint node $NODE_NAME maintenance-

# 8. 이전에 Pending이던 Pod 상태 확인
kubectl get pods -n taint-test -o wide

echo "Taint와 Toleration 구성 완료"
```

---

### 문제 8: NetworkPolicy 구현

#### 📋 문제
3계층 애플리케이션을 위한 NetworkPolicy를 구현하세요.

**요구사항:**
- 웹, 앱, 데이터베이스 계층 분리
- 계층 간 통신만 허용
- 외부에서 웹 계층만 접근 가능

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
kubectl create namespace web-tier
kubectl create namespace app-tier  
kubectl create namespace db-tier

kubectl label namespace web-tier name=web-tier
kubectl label namespace app-tier name=app-tier
kubectl label namespace db-tier name=db-tier

echo "3계층 애플리케이션 환경이 설정되었습니다."
```

#### ✅ 솔루션

```bash
# 1. Web tier policy
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: web-tier-policy
  namespace: web-tier
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: app-tier
    ports:
    - protocol: TCP
      port: 8080
EOF

# 2. App tier policy  
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-tier-policy
  namespace: app-tier
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: web-tier
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: db-tier
    ports:
    - protocol: TCP
      port: 5432
EOF

# 3. DB tier policy
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-tier-policy
  namespace: db-tier
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: app-tier
    ports:
    - protocol: TCP
      port: 5432
EOF

# 4. 테스트 애플리케이션 배포
kubectl create deployment web-app --image=nginx:1.21 -n web-tier
kubectl create deployment app-server --image=nginx:1.21 -n app-tier
kubectl create deployment database --image=postgres:13 --env="POSTGRES_PASSWORD=secret" -n db-tier

# 5. NetworkPolicy 확인
kubectl get networkpolicy -A

echo "NetworkPolicy 구현 완료"
```

---

### 문제 9: StorageClass 구성

#### 📋 문제
사용자 정의 StorageClass를 생성하고 기본 StorageClass로 설정하세요.

**요구사항:**
- StorageClass 이름: fast-ssd
- 기본 StorageClass로 설정
- Volume 확장 허용
- WaitForFirstConsumer 바인딩 모드

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 기존 StorageClass 확인
kubectl get storageclass

echo "StorageClass 테스트 환경이 설정되었습니다."
```

#### ✅ 솔루션

```bash
# 1. 기존 기본 StorageClass 제거 (있다면)
kubectl patch storageclass standard -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}' || true

# 2. 새 StorageClass 생성
cat <<EOF | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: k8s.io/minikube-hostpath
parameters:
  type: "fast"
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
EOF

# 3. StorageClass 확인
kubectl get storageclass
kubectl describe storageclass fast-ssd

# 4. 테스트 PVC 생성
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
EOF

# 5. PVC 상태 확인
kubectl get pvc test-pvc

echo "StorageClass 구성 완료"
```

---

### 문제 10: JSONPath 쿼리

#### 📋 문제
JSONPath를 사용하여 클러스터 정보를 추출하고 파일로 저장하세요.

**요구사항:**
- 노드 OS 정보 추출
- Pod IP 주소 목록
- 특정 필드로 정렬
- 결과를 파일로 저장

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 테스트용 Pod 생성
kubectl run test-pod1 --image=nginx:1.21
kubectl run test-pod2 --image=busybox:1.35 -- sleep 3600
kubectl run test-pod3 --image=redis:6-alpine

echo "JSONPath 테스트 환경이 설정되었습니다."
```

#### ✅ 솔루션

```bash
# 1. 노드 OS 정보 추출
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.nodeInfo.osImage}{"\n"}{end}' > /tmp/node-os-info.txt

# 2. Pod IP 추출
kubectl get pods -o jsonpath='{.items[*].status.podIP}' > /tmp/pod-ips.txt

# 3. Pod 이름과 상태 추출
kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.phase}{"\t"}{.spec.nodeName}{"\n"}{end}' > /tmp/pod-status.txt

# 4. 특정 필드로 정렬
kubectl get pods --sort-by='.metadata.creationTimestamp' -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.metadata.creationTimestamp}{"\n"}{end}' > /tmp/pods-by-time.txt

# 5. 복잡한 JSONPath 쿼리 - 리소스 요청량
kubectl get pods -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[0].resources.requests.cpu}{"\t"}{.spec.containers[0].resources.requests.memory}{"\n"}{end}' > /tmp/pod-resources.txt

# 6. 결과 파일 확인
echo "=== Node OS Info ==="
cat /tmp/node-os-info.txt

echo "=== Pod IPs ==="
cat /tmp/pod-ips.txt

echo "=== Pod Status ==="
cat /tmp/pod-status.txt

echo "=== Pods by Creation Time ==="
cat /tmp/pods-by-time.txt

echo "=== Pod Resources ==="
cat /tmp/pod-resources.txt

# 7. 파일 목록 확인
ls -la /tmp/*.txt

echo "JSONPath 쿼리 완료"
```

---

## 🎯 Part 1 완료 체크리스트

### ✅ 완료된 주요 영역

1. **네트워킹** ✅
   - CNI 설치 (Calico)
   - NetworkPolicy 구현

2. **워크로드 관리** ✅
   - Helm 템플릿 생성
   - HPA 구성
   - Sidecar 컨테이너
   - PriorityClass 구성

3. **보안** ✅
   - Pod Security Admission
   - Taint와 Toleration

4. **스토리지** ✅
   - StorageClass 구성

5. **운영** ✅
   - JSONPath 쿼리

### 🚀 다음 단계

이제 **Part 2: 고급 실습**으로 넘어가서 더 복잡한 시나리오를 연습하세요!

---

## Part 2: 고급 실습

### 문제 11: Job과 CronJob 관리

#### 📋 문제
복잡한 배치 처리 워크플로우를 Job과 CronJob으로 구현하세요.

**요구사항:**
1. 병렬 처리 Job 생성 (5개 병렬, 총 20개 완료)
2. 일일 백업용 CronJob 설정
3. Job 실패 처리 및 재시도 정책 구현
4. Job 정리 정책 설정

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 네임스페이스 생성
kubectl create namespace batch-processing

echo "배치 처리 환경이 설정되었습니다."
```

#### ✅ 솔루션

```bash
# 1. 병렬 처리 Job 생성
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing-job
  namespace: batch-processing
spec:
  parallelism: 5
  completions: 20
  backoffLimit: 3
  activeDeadlineSeconds: 1800  # 30분
  ttlSecondsAfterFinished: 3600  # 1시간 후 정리
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: processor
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          echo "Processing batch \$HOSTNAME at \$(date)"
          # 랜덤 처리 시간 시뮬레이션
          sleep \$((RANDOM % 30 + 10))
          # 10% 확률로 실패 시뮬레이션
          if [ \$((RANDOM % 10)) -eq 0 ]; then
            echo "Processing failed for \$HOSTNAME"
            exit 1
          fi
          echo "Batch \$HOSTNAME completed successfully at \$(date)"
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
EOF

# 2. CronJob for 일일 백업 생성
cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-backup
  namespace: batch-processing
spec:
  schedule: "0 2 * * *"  # 매일 오전 2시
  timeZone: "UTC"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 300
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 1800  # 30분
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: backup
            image: alpine:3.18
            command:
            - sh
            - -c
            - |
              echo "Starting backup at \$(date)"
              mkdir -p /backup/\$(date +%Y%m%d)
              echo "Backup data \$(date)" > /backup/\$(date +%Y%m%d)/backup.txt
              # 백업 프로세스 시뮬레이션
              sleep 60
              echo "Backup completed successfully at \$(date)"
            env:
            - name: BACKUP_LOCATION
              value: "/backup"
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            emptyDir: {}
EOF

# 3. Job 상태 모니터링
kubectl get jobs -n batch-processing
kubectl describe job data-processing-job -n batch-processing

# 4. CronJob 상태 확인
kubectl get cronjobs -n batch-processing
kubectl describe cronjob daily-backup -n batch-processing

# 5. 수동으로 CronJob 실행 (테스트용)
kubectl create job manual-backup --from=cronjob/daily-backup -n batch-processing

echo "Job과 CronJob 설정이 완료되었습니다."
```

---

### 문제 12: DaemonSet과 노드 관리

#### 📋 문제
모든 노드에서 실행되는 로그 수집 DaemonSet을 구성하고 노드 관리 작업을 수행하세요.

**요구사항:**
1. 로그 수집용 DaemonSet 생성
2. Control Plane 노드에서도 실행되도록 toleration 설정
3. 노드 드레인 및 언코든 작업
4. DaemonSet 업데이트 전략 구현

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 네임스페이스 생성
kubectl create namespace monitoring

echo "모니터링 환경이 설정되었습니다."
```

#### ✅ 솔루션

```bash
# 1. 로그 수집 DaemonSet 생성
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: log-collector
  namespace: monitoring
  labels:
    app: log-collector
spec:
  selector:
    matchLabels:
      app: log-collector
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: log-collector
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: log-collector
        image: busybox:1.35
        command:
        - sh
        - -c
        - |
          while true; do
            echo "\$(date): Collecting logs from node \$(hostname)"
            # 호스트의 로그 파일들을 모니터링
            find /var/log -name "*.log" -type f 2>/dev/null | head -5
            sleep 30
          done
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
          readOnly: true
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      hostNetwork: true
      hostPID: true
EOF

# 2. DaemonSet 상태 확인
kubectl get daemonset -n monitoring
kubectl get pods -n monitoring -o wide

# 3. 노드에 테인트 추가 (드레인 시뮬레이션)
NODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')
kubectl taint node $NODE_NAME maintenance=true:NoSchedule

# 4. DaemonSet은 여전히 실행 중인지 확인 (toleration 때문에)
kubectl get pods -n monitoring -o wide

# 5. 테인트 제거 (언코든)
kubectl taint node $NODE_NAME maintenance-

# 6. DaemonSet 업데이트 테스트
kubectl patch daemonset log-collector -n monitoring -p '{"spec":{"template":{"spec":{"containers":[{"name":"log-collector","image":"busybox:1.36"}]}}}}'

# 7. 롤링 업데이트 상태 확인
kubectl rollout status daemonset/log-collector -n monitoring

echo "DaemonSet과 노드 관리 작업이 완료되었습니다."
```

---

### 문제 13: 복잡한 애플리케이션 디버깅

#### 📋 문제
다중 계층 애플리케이션에서 발생하는 복합적인 문제를 해결하세요.

**요구사항:**
1. 프론트엔드, 백엔드, 데이터베이스 연결 문제 해결
2. 서비스 디스커버리 문제 진단
3. 리소스 부족으로 인한 성능 문제 해결
4. 로그 분석을 통한 근본 원인 파악

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 네임스페이스 생성
kubectl create namespace troubleshoot-app

# 잘못된 설정으로 백엔드 생성
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: troubleshoot-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: nginx:1.21
        env:
        - name: DB_HOST
          value: "wrong-database-service"  # 잘못된 서비스 이름
        - name: DB_PORT
          value: "5432"
        resources:
          requests:
            cpu: 2000m  # 과도한 CPU 요청
            memory: 4Gi  # 과도한 메모리 요청
        ports:
        - containerPort: 80
EOF

# 잘못된 포트로 백엔드 서비스 생성
kubectl expose deployment backend --name=backend-service --port=8080 --target-port=8080 -n troubleshoot-app

# 데이터베이스 생성 (올바른 이름)
kubectl create deployment database --image=postgres:13 --env="POSTGRES_PASSWORD=secret" -n troubleshoot-app
kubectl expose deployment database --name=database-service --port=5432 -n troubleshoot-app

echo "문제가 있는 애플리케이션이 배포되었습니다. 디버깅을 시작하세요."
```

#### ✅ 솔루션

```bash
# 1. 전체 애플리케이션 상태 확인
kubectl get all -n troubleshoot-app
kubectl get events -n troubleshoot-app --sort-by='.lastTimestamp'

# 2. Pod 상태 상세 확인
kubectl describe pods -n troubleshoot-app

# 3. 백엔드 리소스 문제 해결
kubectl patch deployment backend -n troubleshoot-app -p '{"spec":{"template":{"spec":{"containers":[{"name":"backend","resources":{"requests":{"cpu":"100m","memory":"128Mi"}}}]}}}}'

# 4. 백엔드 환경변수 수정 (올바른 데이터베이스 서비스 이름)
kubectl patch deployment backend -n troubleshoot-app -p '{"spec":{"template":{"spec":{"containers":[{"name":"backend","env":[{"name":"DB_HOST","value":"database-service"},{"name":"DB_PORT","value":"5432"}]}]}}}}'

# 5. 백엔드 서비스 포트 문제 해결
kubectl patch service backend-service -n troubleshoot-app -p '{"spec":{"ports":[{"port":8080,"targetPort":80}]}}'

# 6. 서비스 연결 테스트
kubectl run test-connectivity --image=busybox:1.35 -n troubleshoot-app --rm -it --restart=Never -- sh -c "
echo 'Testing backend service...'
nslookup backend-service.troubleshoot-app.svc.cluster.local
wget -qO- --timeout=5 http://backend-service:8080 || echo 'Connection failed'
echo 'Testing database service...'
nslookup database-service.troubleshoot-app.svc.cluster.local
"

# 7. 서비스 엔드포인트 확인
kubectl get endpoints -n troubleshoot-app

# 8. 최종 상태 확인
kubectl get pods -n troubleshoot-app
kubectl get svc -n troubleshoot-app

echo "애플리케이션 디버깅이 완료되었습니다."
```

---

### 문제 14: 클러스터 성능 최적화

#### 📋 문제
클러스터의 성능 문제를 진단하고 최적화하세요.

**요구사항:**
1. 리소스 사용량이 높은 Pod 식별
2. 노드 리소스 부족 문제 해결
3. HPA 설정 최적화
4. 리소스 쿼터 및 제한 조정

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 네임스페이스 생성
kubectl create namespace performance-test

# 제한적인 리소스 쿼터 설정
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tight-quota
  namespace: performance-test
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
EOF

echo "성능 테스트 환경이 설정되었습니다."
```

#### ✅ 솔루션

```bash
# 1. 리소스 쿼터 상태 확인
kubectl describe resourcequota tight-quota -n performance-test

# 2. 리소스 쿼터 조정
kubectl patch resourcequota tight-quota -n performance-test -p '{"spec":{"hard":{"requests.cpu":"2","requests.memory":"2Gi","limits.cpu":"4","limits.memory":"4Gi"}}}'

# 3. 최적화된 애플리케이션 배포
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: optimized-app
  namespace: performance-test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: optimized-app
  template:
    metadata:
      labels:
        app: optimized-app
    spec:
      containers:
      - name: app
        image: nginx:1.21
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 400m
            memory: 512Mi
EOF

# 4. HPA 설정으로 자동 스케일링 구현
cat <<EOF | kubectl apply -f -
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: optimized-app-hpa
  namespace: performance-test
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: optimized-app
  minReplicas: 2
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
EOF

# 5. 서비스 생성 및 노출
kubectl expose deployment optimized-app --name=app-service --port=80 -n performance-test

# 6. 부하 테스트 실행
kubectl run load-generator --image=busybox:1.35 -n performance-test --rm -it --restart=Never -- sh -c "
while true; do
  wget -q -O- http://app-service.performance-test.svc.cluster.local
  sleep 0.1
done
" &

# 7. HPA 동작 모니터링
kubectl get hpa -n performance-test -w &
sleep 60

# 8. 부하 테스트 중지
pkill -f "wget.*app-service"

# 9. 최종 리소스 사용량 확인
kubectl top pods -n performance-test
kubectl describe resourcequota tight-quota -n performance-test

echo "클러스터 성능 최적화가 완료되었습니다."
```

---

### 문제 15: 서비스 메시 및 로드 밸런싱

#### 📋 문제
고급 로드 밸런싱과 서비스 메시 기능을 구현하세요.

**요구사항:**
1. 세션 어피니티가 있는 서비스 생성
2. 가중치 기반 라우팅 구현
3. 헬스 체크 및 준비성 프로브 설정
4. 서비스 간 통신 최적화

#### 🛠️ 환경 설정 (KillerCoda에서 실행)

```bash
# 네임스페이스 생성
kubectl create namespace service-mesh

echo "서비스 메시 환경이 설정되었습니다."
```

#### ✅ 솔루션

```bash
# 1. 세션 어피니티가 있는 서비스 생성
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service-v1
  namespace: service-mesh
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user-service
      version: v1
  template:
    metadata:
      labels:
        app: user-service
        version: v1
    spec:
      containers:
      - name: user-service
        image: nginx:1.21
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
EOF

# 2. 세션 어피니티 서비스 생성
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: service-mesh
spec:
  selector:
    app: user-service
  ports:
  - port: 80
    targetPort: 80
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600
EOF

# 3. 두 번째 버전의 서비스 생성 (가중치 라우팅용)
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service-v2
  namespace: service-mesh
spec:
  replicas: 1
  selector:
    matchLabels:
      app: user-service
      version: v2
  template:
    metadata:
      labels:
        app: user-service
        version: v2
    spec:
      containers:
      - name: user-service
        image: nginx:1.22  # 다른 버전
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
EOF

# 4. 헬스 체크 테스트용 클라이언트 생성
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: client-pod
  namespace: service-mesh
spec:
  containers:
  - name: client
    image: busybox:1.35
    command: ['sleep', '3600']
EOF

# 5. 서비스 상태 확인
kubectl get svc -n service-mesh
kubectl get endpoints -n service-mesh

# 6. 세션 어피니티 테스트
kubectl exec -it client-pod -n service-mesh -- sh -c "
for i in \$(seq 1 10); do
  echo \"Request \$i:\"
  wget -qO- http://user-service.service-mesh.svc.cluster.local | grep -o 'nginx/[0-9.]*'
  sleep 1
done
"

# 7. 트래픽 분산 확인 (v1과 v2 간)
kubectl exec -it client-pod -n service-mesh -- sh -c "
for i in \$(seq 1 20); do
  wget -qO- http://user-service.service-mesh.svc.cluster.local | grep -o 'nginx/[0-9.]*'
done | sort | uniq -c
"

echo "서비스 메시 및 로드 밸런싱 설정이 완료되었습니다."
```

---

## 🎯 Part 2 완료 체크리스트

### ✅ 완료된 고급 영역

1. **배치 처리** ✅
   - Job과 CronJob 관리
   - 병렬 처리 및 재시도 정책

2. **시스템 관리** ✅
   - DaemonSet과 노드 관리
   - 롤링 업데이트 전략

3. **문제 해결** ✅
   - 복잡한 애플리케이션 디버깅
   - 클러스터 성능 최적화

4. **고급 네트워킹** ✅
   - 서비스 메시 및 로드 밸런싱
   - 세션 어피니티 구성

### 🚀 다음 단계

이제 **실전 시험 패턴** 문서로 최종 점검을 진행하세요!

---

**🎉 CKA 2025 통합 실습이 완료되었습니다!**
**실제 시험에서 자신 있게 문제를 해결할 수 있을 것입니다! 🚀**